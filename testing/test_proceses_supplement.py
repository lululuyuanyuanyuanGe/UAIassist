from utilities.file_process import *
from utilities.message_process import *
import json
from pathlib import Path
from datetime import datetime
from langchain_core.messages import SystemMessage
from utilities.modelRelated import model_creation
from langchain_openai import ChatOpenAI

def _process_supplement(file_path: list[str], model: ChatOpenAI):
        """This node will process the supplement files, it will analyze the supplement files and summarize the content of the files as well as stored the summary in data.json"""
        
        # Load existing data.json
        data_json_path = Path("agents/data.json")
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            data = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        
        table_files = file_path
        document_files = file_path
        
        # Process table files
        for table_file in table_files:
            try:
                source_path = Path(table_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼åˆ†æä¸“å®¶ï¼Œç°åœ¨è¿™ä¸ªexcelè¡¨æ ¼å·²ç»è¢«è½¬æ¢æˆäº†HTMLæ ¼å¼ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»è¿™ä¸ªè¡¨æ ¼ï¼Œåˆ†æè¡¨æ ¼çš„ç»“æ„ï¼Œå¹¶æ€»ç»“è¡¨æ ¼çš„å†…å®¹ï¼Œæ‰€æœ‰çš„è¡¨å¤´ã€åˆ—åã€æ•°æ®éƒ½è¦æ€»ç»“å‡ºæ¥ã€‚

                æ–‡ä»¶å†…å®¹:
                {file_content}

                è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
                {{
                    "è¡¨æ ¼ç»“æ„": "æè¿°è¡¨æ ¼çš„æ•´ä½“ç»“æ„",
                    "è¡¨å¤´ä¿¡æ¯": ["åˆ—å1", "åˆ—å2", "åˆ—å3"],
                    "æ•°æ®æ¦‚è¦": "æ•°æ®çš„æ€»ä½“æè¿°å’Œé‡è¦ä¿¡æ¯",
                    "è¡Œæ•°ç»Ÿè®¡": "æ€»è¡Œæ•°",
                    "å…³é”®å­—æ®µ": ["é‡è¦å­—æ®µ1", "é‡è¦å­—æ®µ2"]
                }}"""
                                
                analysis_response = model.invoke([SystemMessage(content=system_prompt)])
                
                # Store in data.json
                data["è¡¨æ ¼"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(table_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¡¨æ ¼æ–‡ä»¶å‡ºé”™ {table_file}: {e}")

        # Process document files
        for document_file in document_files:
            try:
                source_path = Path(document_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æä¸“å®¶ï¼Œç°åœ¨è¿™ä¸ªæ–‡æ¡£å·²ç»è¢«è½¬æ¢æˆäº†txtæ ¼å¼ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»è¿™ä¸ªæ–‡æ¡£ï¼Œåˆ†ææ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ€»ç»“æ–‡æ¡£çš„å†…å®¹ã€‚æ–‡æ¡£å¯èƒ½åŒ…å«é‡è¦çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ³•å¾‹æ¡æ–‡ã€æ”¿ç­–è§„å®šç­‰ï¼Œä½ ä¸èƒ½é—æ¼è¿™äº›ä¿¡æ¯ã€‚
                
                æ–‡ä»¶å†…å®¹:
                {file_content}

                è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
                {{
                    "æ–‡æ¡£ç±»å‹": "åˆ¤æ–­æ–‡æ¡£çš„ç±»å‹ï¼ˆå¦‚æ”¿ç­–æ–‡ä»¶ã€æ³•å¾‹æ¡æ–‡ã€è¯´æ˜æ–‡æ¡£ç­‰ï¼‰",
                    "ä¸»è¦å†…å®¹": "æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹æ¦‚è¦",
                    "é‡è¦æ¡æ¬¾": ["é‡è¦æ¡æ¬¾1", "é‡è¦æ¡æ¬¾2"],
                    "å…³é”®ä¿¡æ¯": ["å…³é”®ä¿¡æ¯1", "å…³é”®ä¿¡æ¯2"],
                    "åº”ç”¨åœºæ™¯": "è¿™äº›ä¿¡æ¯åœ¨è¡¨æ ¼å¡«å†™ä¸­çš„ç”¨é€”"
                }}"""
                                
                analysis_response = model.invoke([SystemMessage(content=system_prompt)])

                # Update state with analysis response
                # state["process_user_input_messages"].append(analysis_response)
                
                # Store in data.json
                data["æ–‡æ¡£"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(document_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"âœ… æ–‡æ¡£æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                print(f"åˆ†æç»“æœ: {analysis_response.content}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡æ¡£æ–‡ä»¶å‡ºé”™ {document_file}: {e}")
        
        # Save updated data.json
        try:
            with open(data_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"âœ… å·²æ›´æ–° data.jsonï¼Œè¡¨æ ¼æ–‡ä»¶ {len(data['è¡¨æ ¼'])} ä¸ªï¼Œæ–‡æ¡£æ–‡ä»¶ {len(data['æ–‡æ¡£'])} ä¸ª")
        except Exception as e:
            print(f"âŒ ä¿å­˜ data.json æ—¶å‡ºé”™: {e}")
        
        # Create summary message
        summary_message = f"""ğŸ“Š è¡¥å……æ–‡ä»¶å¤„ç†å®Œæˆ:
        âœ… è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ªå·²åˆ†æå¹¶å­˜å‚¨
        âœ… æ–‡æ¡£æ–‡ä»¶: {len(document_files)} ä¸ªå·²åˆ†æå¹¶å­˜å‚¨
        ğŸ“ æ•°æ®åº“å·²æ›´æ–°ï¼Œæ€»è®¡è¡¨æ ¼ {len(data['è¡¨æ ¼'])} ä¸ªï¼Œæ–‡æ¡£ {len(data['æ–‡æ¡£'])} ä¸ª"""
        
        return 


user_input_files = input("è¯·è¾“å…¥ç”¨æˆ·è¾“å…¥çš„æ–‡ä»¶è·¯å¾„: ")
result = detect_and_process_file_paths(user_input_files)
print(result)

file_path = retrieve_file_content(result, "1")
print(file_path)

for file in file_path:
    analysis_content = Path(file).read_text(encoding='utf-8')

    model = ChatOpenAI(model="gpt-4o", temperature=0.0)
        
    _process_supplement([file], model)