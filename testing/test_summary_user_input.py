from utilities.file_process import *
from utilities.message_process import *
import json
from pathlib import Path
from datetime import datetime
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, BaseMessage
from utilities.modelRelated import model_creation
from langchain_openai import ChatOpenAI

# Example 1: Chat history with simple template
simple_template_messages = [
    HumanMessage(content="ä½ å¥½ï¼Œæˆ‘éœ€è¦å¤„ç†ä¸€ä¸ªè¡¨æ ¼"),
    AIMessage(content="æ‚¨å¥½ï¼æˆ‘æ˜¯è¡¨æ ¼å¤„ç†åŠ©æ‰‹ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦å¤„ç†ä»€ä¹ˆæ ·çš„è¡¨æ ¼ï¼Œæˆ–è€…æ‚¨å¯ä»¥ä¸Šä¼ è¡¨æ ¼æ¨¡æ¿ã€‚"),
    HumanMessage(content="æˆ‘æœ‰ä¸€ä¸ªå‘˜å·¥ä¿¡æ¯è¡¨ï¼ŒåŒ…å«å§“åã€å¹´é¾„ã€éƒ¨é—¨ã€èŒä½è¿™å‡ åˆ—"),
    AIMessage(content="æ˜ç™½äº†ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å‘˜å·¥ä¿¡æ¯è¡¨ã€‚æ‚¨æåˆ°çš„è¡¨æ ¼åŒ…å«ä»¥ä¸‹åˆ—ï¼š\n- å§“å\n- å¹´é¾„\n- éƒ¨é—¨\n- èŒä½\n\nè¿™æ˜¯ä¸€ä¸ªç®€å•æ¨¡æ¿ï¼Œåªæœ‰åˆ—æ ‡é¢˜ã€‚æ‚¨éœ€è¦æˆ‘å¸®æ‚¨åšä»€ä¹ˆå¤„ç†å—ï¼Ÿ"),
    HumanMessage(content="å¯¹çš„ï¼Œå°±æ˜¯è¿™æ ·çš„ç®€å•è¡¨æ ¼ï¼Œæˆ‘éœ€è¦å¡«å……ä¸€äº›æ•°æ®")
]

# Example 2: Chat history with complex template  
complex_template_messages = [
    HumanMessage(content="æˆ‘éœ€è¦å¤„ç†ä¸€ä¸ªå¤æ‚çš„è´¢åŠ¡æŠ¥è¡¨"),
    AIMessage(content="å¥½çš„ï¼Œè¯·æè¿°ä¸€ä¸‹æ‚¨çš„è´¢åŠ¡æŠ¥è¡¨ç»“æ„ï¼Œæˆ–è€…ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶ã€‚"),
    HumanMessage(content="è¿™ä¸ªè¡¨æ ¼æ¯”è¾ƒå¤æ‚ï¼Œæœ‰è¡Œæ ‡é¢˜å’Œåˆ—æ ‡é¢˜ã€‚åˆ—æ ‡é¢˜æ˜¯å„ä¸ªæœˆä»½ï¼š1æœˆã€2æœˆã€3æœˆç­‰ï¼Œè¡Œæ ‡é¢˜æ˜¯ä¸åŒçš„è´¹ç”¨ç±»å‹ï¼šåŠå…¬è´¹ã€å·®æ—…è´¹ã€è®¾å¤‡è´¹ç­‰ã€‚"),
    AIMessage(content="æˆ‘ç†è§£äº†ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒç»´äº¤å‰è¡¨æ ¼ï¼š\n- åˆ—æ ‡é¢˜ï¼šæœˆä»½ï¼ˆ1æœˆã€2æœˆã€3æœˆ...ï¼‰\n- è¡Œæ ‡é¢˜ï¼šè´¹ç”¨ç±»å‹ï¼ˆåŠå…¬è´¹ã€å·®æ—…è´¹ã€è®¾å¤‡è´¹...ï¼‰\n\nè¿™æ˜¯ä¸€ä¸ªå¤æ‚æ¨¡æ¿ï¼ŒåŒ…å«è¡Œæ ‡é¢˜å’Œåˆ—æ ‡é¢˜çš„äº¤å‰ç»“æ„ã€‚"),
    HumanMessage(content="æ˜¯çš„ï¼Œæ¯ä¸ªäº¤å‰ç‚¹éœ€è¦å¡«å…¥å¯¹åº”æœˆä»½çš„è´¹ç”¨é‡‘é¢"),
    AIMessage(content="æ˜ç™½äº†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„å¤æ‚è¡¨æ ¼æ¨¡æ¿ï¼Œéœ€è¦åœ¨è¡Œåˆ—äº¤å‰å¤„å¡«å…¥æ•°æ®ã€‚")
]

# Example 3: Chat history with general inquiry (no template)
general_inquiry_messages = [
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="æ‚¨å¥½ï¼æˆ‘æ˜¯è¡¨æ ¼å¤„ç†åŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨å¤„ç†å„ç§è¡¨æ ¼ç›¸å…³çš„ä»»åŠ¡ã€‚"),
    HumanMessage(content="æˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ éƒ½èƒ½åšä»€ä¹ˆ"),
    AIMessage(content="æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n1. åˆ†æè¡¨æ ¼ç»“æ„\n2. å¡«å……è¡¨æ ¼æ•°æ®\n3. å¤„ç†ç®€å•å’Œå¤æ‚æ¨¡æ¿\n4. æ•°æ®æ ¼å¼è½¬æ¢\n\næ‚¨æœ‰ä»€ä¹ˆå…·ä½“éœ€æ±‚å—ï¼Ÿ"),
    HumanMessage(content="æˆ‘å…ˆäº†è§£ä¸€ä¸‹ï¼Œç¨åå†å…·ä½“ä½¿ç”¨")
]

# Example 4: Chat history with file upload scenario
file_upload_messages = [
    HumanMessage(content="æˆ‘è¦ä¸Šä¼ ä¸€ä¸ªExcelæ–‡ä»¶"),
    AIMessage(content="å¥½çš„ï¼Œè¯·ä¸Šä¼ æ‚¨çš„Excelæ–‡ä»¶ï¼Œæˆ‘ä¼šå¸®æ‚¨åˆ†æè¡¨æ ¼ç»“æ„ã€‚"),
    HumanMessage(content="æ–‡ä»¶è·¯å¾„ï¼š/path/to/student_grades.xlsx"),
    AIMessage(content="æˆ‘æ”¶åˆ°äº†æ‚¨ä¸Šä¼ çš„æ–‡ä»¶ã€‚æ­£åœ¨åˆ†æè¡¨æ ¼ç»“æ„..."),
    HumanMessage(content="è¿™ä¸ªè¡¨æ ¼åŒ…å«å­¦ç”Ÿå§“åã€å„ç§‘æˆç»©ã€æ€»åˆ†ç­‰ä¿¡æ¯ï¼Œç»“æ„æ¯”è¾ƒç®€å•"),
    AIMessage(content="æ ¹æ®æ‚¨çš„æè¿°ï¼Œè¿™æ˜¯ä¸€ä¸ªå­¦ç”Ÿæˆç»©è¡¨ï¼Œå±äºç®€å•æ¨¡æ¿ç±»å‹ï¼Œåªæœ‰åˆ—æ ‡é¢˜æ²¡æœ‰å¤æ‚çš„è¡Œåˆ—äº¤å‰ç»“æ„ã€‚")
]

def test_summary_user_input():
    """Test function that actually invokes LLM and allows human evaluation"""
    
    # Initialize the LLM
    llm = model_creation(model_name="gpt-4o", temperature=0.2)
    
    def _summary_user_input_real(process_user_input_messages: list[BaseMessage]) -> AIMessage:
        """Real implementation that calls LLM"""
        
        # Extract content from messages
        process_user_input_messages_content = [item.content for item in process_user_input_messages]
        process_user_input_messages_content = "\n".join(f"{item.type#}: {item.content}" for item in process_user_input_messages)
        
        system_prompt = f"""ä½ çš„ä»»åŠ¡æ˜¯è´Ÿè´£æ€»ç»“ç”¨æˆ·åœ¨è¿™ä¸€è½®éƒ½æä¾›äº†å“ªäº›ä¿¡æ¯ï¼Œä½ éœ€è¦æ ¹æ®æ•´ä¸ªå¯¹è¯è®°å½•ï¼Œæ€»ç»“ç”¨æˆ·éƒ½æä¾›äº†å“ªäº›ä¿¡æ¯ï¼Œå¹¶ä¸”æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œå†³å®šä¸‹ä¸€æ­¥çš„æµç¨‹

è§„åˆ™å¦‚ä¸‹ï¼š
- å¦‚æœå‡ºç°äº†å¤æ‚æ¨¡æ¿ï¼ˆåŒæ—¶åŒ…å«è¡Œæ ‡é¢˜å’Œåˆ—æ ‡é¢˜çš„äº¤å‰è¡¨æ ¼ï¼‰ï¼Œè¿”å›"complex_template"
- å¦‚æœå‡ºç°äº†ç®€å•æ¨¡æ¿ï¼ˆåªæœ‰åˆ—æ ‡é¢˜çš„æ™®é€šè¡¨æ ¼ï¼‰ï¼Œè¿”å›"simple_template"  
- å…¶ä½™æƒ…å†µè¯·è¿”å›"previous_node"

ä½ çš„å›å¤éœ€è¦åŒ…å«å¯¹è¿™ä¸€è½®çš„æ€»ç»“ï¼Œå’ŒèŠ‚ç‚¹è·¯ç”±ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

å†å²å¯¹è¯: {process_user_input_messages_content}

è¯·è¿”å›ï¼š
{{
    "summary": "æ€»ç»“ç”¨æˆ·åœ¨è¿™ä¸€è½®éƒ½æä¾›äº†å“ªäº›ä¿¡æ¯",
    "next_node": "complex_template/simple_template/previous_node"
}}
"""
        
        try:
            messages = [SystemMessage(content=system_prompt)]
            print(f"ğŸ”„ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ€»ç»“...")
            
            response = llm.invoke(messages)
            print(f"âœ… LLMè°ƒç”¨æˆåŠŸ")
            
            return response
            
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
            
            # Fallback response when LLM fails
            fallback_response = AIMessage(content="""
            {
                "summary": "ç”±äºç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•å®Œæˆæ™ºèƒ½åˆ†æã€‚ç”¨æˆ·æœ¬è½®æä¾›äº†è¾“å…¥ä¿¡æ¯ã€‚",
                "next_node": "previous_node"
            }
            """)
            
            return fallback_response

    # Test scenarios with human evaluation
    test_scenarios = [
        ("ç®€å•æ¨¡æ¿åœºæ™¯", simple_template_messages),
        ("å¤æ‚æ¨¡æ¿åœºæ™¯", complex_template_messages), 
        ("ä¸€èˆ¬è¯¢é—®åœºæ™¯", general_inquiry_messages),
        ("æ–‡ä»¶ä¸Šä¼ åœºæ™¯", file_upload_messages)
    ]
    
    print("=" * 50)
    print("å¼€å§‹LLMæ€»ç»“åŠŸèƒ½æµ‹è¯• - éœ€è¦äººå·¥è¯„ä¼°")
    print("=" * 50)
    
    for scenario_name, messages in test_scenarios:
        print(f"\n{'='*20} {scenario_name} {'='*20}")
        
        # Show input messages
        print("\nğŸ“ è¾“å…¥çš„å¯¹è¯å†å²:")
        for i, msg in enumerate(messages, 1):
            msg_type = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AIåŠ©æ‰‹"
            print(f"  {i}. [{msg_type}]: {msg.content}")
        
        # Get LLM response
        print(f"\nğŸ¤– è°ƒç”¨LLMåˆ†æ...")
        result = _summary_user_input_real(messages)
        
        print(f"\nğŸ“‹ LLMåˆ†æç»“æœ:")
        print("-" * 40)
        print(result.content)
        print("-" * 40)
        
        # Human evaluation
        print(f"\nğŸ‘¤ è¯·è¯„ä¼°LLMçš„å›å¤è´¨é‡:")
        print("1. æ€»ç»“æ˜¯å¦å‡†ç¡®ï¼Ÿ")
        print("2. è·¯ç”±å†³ç­–æ˜¯å¦æ­£ç¡®ï¼Ÿ")
        print("3. JSONæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ")
        
        while True:
            evaluation = input("\nè¯·è¾“å…¥è¯„ä¼° (excellent/good/fair/poor) æˆ– 's' è·³è¿‡: ").lower().strip()
            if evaluation in ['excellent', 'good', 'fair', 'poor', 's']:
                break
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„è¯„ä¼°: excellent, good, fair, poor, æˆ– s")
        
        if evaluation != 's':
            print(f"âœ… äººå·¥è¯„ä¼°: {evaluation}")
            
            if evaluation in ['fair', 'poor']:
                feedback = input("è¯·æä¾›æ”¹è¿›å»ºè®®: ")
                print(f"ğŸ“ æ”¹è¿›å»ºè®®: {feedback}")
        
        print("\n" + "="*60)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ„Ÿè°¢æ‚¨çš„è¯„ä¼°ã€‚")

if __name__ == "__main__":
    test_summary_user_input()