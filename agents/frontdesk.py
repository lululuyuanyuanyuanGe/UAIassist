import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, create_assistant_with_files, filter_out_system_messages, detect_and_process_file_paths, upload_file_to_LLM
import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

load_dotenv()

# ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
from openai import OpenAI
client = OpenAI(
    api_key = os.environ.get("OPENAI_API_KEY")
)

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

@tool
def upload_file_to_LLM_tool(file_paths: list, provider: str = "openai", purpose: str = "assistants", vector_store_id: str = None):
    """
    é€šç”¨æ–‡ä»¶ä¸Šä¼ å·¥å…·ï¼Œå°†ç”¨æˆ·æä¾›çš„æ–‡ä»¶ä¸Šä¼ ç»™å¤§æ¨¡å‹
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        provider: æ¨¡å‹æä¾›å•† ("openai", "azure", "anthropic", "local")
        purpose: æ–‡ä»¶ç”¨é€” ("assistants", "fine-tune", "user_data")
        vector_store_id: å¯é€‰çš„å‘é‡å­˜å‚¨IDï¼Œç”¨äºOpenAIåŠ©æ‰‹
    
    Returns:
        dict: åŒ…å«ä¸Šä¼ ç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬file_ids
    """
    # æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ 
    result = upload_file_to_LLM(file_paths, provider, purpose, vector_store_id)
    
    # æå–æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶ID
    uploaded_file_ids = [file["file_id"] for file in result.get("uploaded_files", [])]
    
    print(f"ğŸ“ ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶ID: {uploaded_file_ids}")
    
    # è¿”å›ç»“æœï¼ŒåŒ…å«file_idsç”¨äºåç»­çŠ¶æ€æ›´æ–°
    result["file_ids"] = uploaded_file_ids
    return result

# å®šä¹‰å‰å°æ¥å¾…å‘˜çŠ¶æ€
class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    table_structure: dict
    table_info: dict
    additional_requirements: dict
    gather_complete: bool
    has_template: bool
    complete_confirm: bool
    uploaded_files: list  # ç”¨æˆ·æä¾›çš„æ–‡ä»¶è·¯å¾„
    uploaded_files_id: list
    previous_node: str  # Track the previous node before file upload
    failed_uploads: list  # Track files that failed to upload

class CustomFileUploadNode:
    """è‡ªå®šä¹‰æ–‡ä»¶ä¸Šä¼ èŠ‚ç‚¹ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’ŒçŠ¶æ€æ›´æ–°"""
    
    def __init__(self, tools):
        self.tools = tools
        self.tool_node = ToolNode(tools)
    
    def __call__(self, state: FrontdeskState):
        # æ‰§è¡Œå·¥å…·è°ƒç”¨ - ä½¿ç”¨æ­£ç¡®çš„invokeæ–¹æ³•
        result = self.tool_node.invoke(state)
        
        # æ£€æŸ¥å·¥å…·æ‰§è¡Œç»“æœï¼Œæ›´æ–°uploaded_files_idå’Œfailed_uploads
        if "messages" in result:
            for message in result["messages"]:
                if hasattr(message, 'content') and isinstance(message.content, str):
                    try:
                        # å°è¯•è§£æå·¥å…·è¿”å›çš„ç»“æœ
                        import json
                        if message.content.startswith('{') and ('"file_ids"' in message.content or '"failed_files"' in message.content):
                            tool_result = json.loads(message.content)
                            
                            # æ›´æ–°æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶ID
                            if "file_ids" in tool_result:
                                current_file_ids = state.get("uploaded_files_id", [])
                                new_file_ids = tool_result["file_ids"]
                                updated_file_ids = list(set(current_file_ids + new_file_ids))  # å»é‡
                                result["uploaded_files_id"] = updated_file_ids
                                print(f"ğŸ“ çŠ¶æ€æ›´æ–° - æ–°å¢æ–‡ä»¶ID: {new_file_ids}")
                                print(f"ğŸ“ çŠ¶æ€æ›´æ–° - æ€»æ–‡ä»¶ID: {updated_file_ids}")
                            
                            # æ›´æ–°å¤±è´¥ä¸Šä¼ çš„æ–‡ä»¶
                            if "failed_files" in tool_result and tool_result["failed_files"]:
                                current_failed = state.get("failed_uploads", [])
                                failed_paths = [f.get("file", "") for f in tool_result["failed_files"]]
                                updated_failed = list(set(current_failed + failed_paths))  # å»é‡
                                result["failed_uploads"] = updated_failed
                                print(f"ğŸ“ çŠ¶æ€æ›´æ–° - å¤±è´¥æ–‡ä»¶: {failed_paths}")
                                print(f"ğŸ“ çŠ¶æ€æ›´æ–° - æ€»å¤±è´¥æ–‡ä»¶: {updated_failed}")
                            
                            break
                    except (json.JSONDecodeError, AttributeError):
                        continue
        
        return result

class FrontDeskAgent:
    """
    åŸºäºLangGraphçš„AIä»£ç†ç³»ç»Ÿï¼Œç”¨äºåˆ¤æ–­ç”¨æˆ·æ˜¯å¦ç»™å‡ºäº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ï¼Œå¹¶å¸®åŠ©ç”¨æˆ·æ±‡æ€»è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿
    æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æ¡£ã€å›¾ç‰‡ç­‰ï¼‰
    """

    def __init__(self, model_name: str = "gpt-4o", checkpoint_path: str = "checkpoints.db"):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=2)
        self.tools = [upload_file_to_LLM_tool]
        self.llm_with_tool = self.llm.bind_tools(self.tools)
        self.memory = MemorySaver()
        self.graph = self._build_graph()

    def _filter_messages_for_llm(self, messages):
        """è¿‡æ»¤æ¶ˆæ¯ï¼Œç¡®ä¿æ­£ç¡®çš„å¯¹è¯åºåˆ—ï¼Œé¿å…OpenAI APIé”™è¯¯"""
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
        
        filtered = []
        i = 0
        while i < len(messages):
            msg = messages[i]
            
            # æ£€æŸ¥æ¶ˆæ¯ç±»å‹
            if isinstance(msg, (HumanMessage, SystemMessage)):
                filtered.append(msg)
                i += 1
            elif isinstance(msg, AIMessage):
                # å¦‚æœAIæ¶ˆæ¯æœ‰tool_callsï¼Œæ£€æŸ¥åç»­æ˜¯å¦æœ‰å¯¹åº”çš„toolæ¶ˆæ¯
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    # æŸ¥æ‰¾å¯¹åº”çš„toolæ¶ˆæ¯
                    tool_call_ids = [tc.get('id') for tc in msg.tool_calls if tc.get('id')]
                    j = i + 1
                    found_tool_responses = []
                    
                    # æ”¶é›†æ‰€æœ‰å¯¹åº”çš„toolæ¶ˆæ¯
                    while j < len(messages) and isinstance(messages[j], ToolMessage):
                        if messages[j].tool_call_id in tool_call_ids:
                            found_tool_responses.append(messages[j])
                        j += 1
                    
                    # å¦‚æœæ‰¾åˆ°äº†å®Œæ•´çš„toolå“åº”ï¼Œè·³è¿‡è¿™ä¸ªAIæ¶ˆæ¯å’Œtoolæ¶ˆæ¯åºåˆ—
                    # å› ä¸ºæˆ‘ä»¬ä¸æƒ³åœ¨LLMè°ƒç”¨ä¸­åŒ…å«toolç›¸å…³çš„æ¶ˆæ¯
                    if len(found_tool_responses) == len(tool_call_ids):
                        i = j  # è·³è¿‡æ•´ä¸ªtoolè°ƒç”¨åºåˆ—
                        continue
                    else:
                        # å¦‚æœtoolå“åº”ä¸å®Œæ•´ï¼Œä¹Ÿè·³è¿‡è¿™ä¸ªAIæ¶ˆæ¯ï¼Œé¿å…APIé”™è¯¯
                        i += 1
                        continue
                else:
                    # æ™®é€šAIæ¶ˆæ¯ï¼Œç›´æ¥æ·»åŠ 
                    filtered.append(msg)
                    i += 1
            else:
                # å…¶ä»–ç±»å‹æ¶ˆæ¯ï¼ˆå¦‚ToolMessageï¼‰è·³è¿‡
                i += 1
        
        return filtered

    def _build_graph(self) -> StateGraph:
        """æ„å»ºç”Ÿæˆè¡¨æ ¼çš„LangGraphçŠ¶æ€å›¾"""

        workflow = StateGraph(FrontdeskState)

        # åˆ›å»ºè‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹
        file_upload_node = CustomFileUploadNode(self.tools)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_template", self._check_template_node)
        workflow.add_node("confirm_template", self._confirm_template_node)
        workflow.add_node("gather_requirements", self._gather_requirements_node)
        workflow.add_node("store_information", self._store_information_node)
        workflow.add_node("collect_input", self._gather_user_input)
        workflow.add_node("collect_template_supplement", self._gather_user_template_supplement)
        workflow.add_node("file_upload_tool", file_upload_node)

        # å…¥å£èŠ‚ç‚¹
        workflow.set_entry_point("check_template")

        # è¿æ¥èŠ‚ç‚¹
        # æ£€æµ‹èŠ‚ç‚¹åˆ¤æ–­ç”¨æˆ·æä¾›äº†æ–‡ä»¶éœ€è¦ä¸Šä¼ 
        workflow.add_conditional_edges(
            "check_template",
            self._route_after_template_check,
            {
                "has_template": "confirm_template",
                "file_upload": "file_upload_tool",
                "no_template": "gather_requirements"
            }
        )

        # å½“æ¨¡æ¿æä¾›æ—¶å’Œç”¨æˆ·ç¡®è®¤
        workflow.add_conditional_edges(
            "confirm_template",
            self._route_after_template_confirm,
            {
                "complete_confirm": "store_information",
                "incomplete_confirm": "collect_template_supplement",
                "upload_file": "file_upload_tool"
            }
        )

        # collect_template_supplementç›´æ¥è¿”å›ç¡®è®¤æ¨¡æ¿
        workflow.add_edge("collect_template_supplement", "confirm_template")
        
        # collect_inputç›´æ¥è¿”å›éœ€æ±‚æ”¶é›†  
        workflow.add_edge("collect_input", "gather_requirements")

        # å½“æ¨¡æ¿æœªæä¾›æ—¶
        workflow.add_conditional_edges(
            "gather_requirements",
            self._route_after_gather_requirements,
            {
                "complete": "store_information",
                "continue": "collect_input",
                "upload_file": "file_upload_tool"
            }
        )

        # æ–‡ä»¶ä¸Šä¼ å·¥å…·å¤„ç†å®Œåè¿”å›åˆ°LLMå¤„ç†èŠ‚ç‚¹
        workflow.add_conditional_edges(
            "file_upload_tool",
            self._route_after_file_upload,
            {
                "check_template": "check_template",
                "confirm_template": "confirm_template", 
                "gather_requirements": "gather_requirements"
            }
        )

        workflow.add_edge("store_information", END)
        
        return workflow.compile(checkpointer = self.memory)

    def _check_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œæ™ºèƒ½å·¥å…·è°ƒç”¨"""

        # è·å–å·²ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯å’Œå¤±è´¥æ–‡ä»¶ä¿¡æ¯
        uploaded_files_info = ""
        if state.get("uploaded_files_id"):
            uploaded_files_info = f"\n**å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼š**\nå·²æˆåŠŸä¸Šä¼  {len(state['uploaded_files_id'])} ä¸ªæ–‡ä»¶åˆ°ç³»ç»Ÿä¸­ï¼Œæ–‡ä»¶ID: {state['uploaded_files_id']}\n"
        
        failed_files_info = ""
        if state.get("failed_uploads"):
            failed_files_info = f"\n**ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶ï¼š**\nä»¥ä¸‹æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·ä¸è¦é‡å¤å°è¯•: {state['failed_uploads']}\n"

        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼æ¨¡æ¿è¯†åˆ«ä¸“å®¶ï¼Œè´Ÿè´£å‡†ç¡®åˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç»æä¾›äº†å®Œæ•´çš„è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ã€‚
        ä½ éœ€è¦åˆ†æç”¨æˆ·çš„æ–‡æœ¬æè¿°ä»¥åŠä»–ä»¬ä¸Šä¼ çš„ä»»ä½•æ–‡ä»¶ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€æ–‡æ¡£ç­‰ï¼‰ã€‚
        {uploaded_files_info}{failed_files_info}
        **å·¥å…·ä½¿ç”¨æŒ‡å—ï¼š**
        å¦‚æœç”¨æˆ·è¾“å…¥ä¸­åŒ…å«NEWæ–‡ä»¶è·¯å¾„ï¼ˆå°šæœªä¸Šä¼ ä¸”æœªå¤±è´¥çš„æ–‡ä»¶ï¼‰ï¼Œè¯·ä½¿ç”¨ upload_file_to_LLM_tool å·¥å…·ã€‚
        
        æ–‡ä»¶è·¯å¾„è¯†åˆ«è§„åˆ™ï¼š
        - Windowsè·¯å¾„ï¼šd:\\folder\\file.xlsx, C:\\Users\\file.csv
        - Unixè·¯å¾„ï¼š/home/user/file.xlsx, ./data/file.csv
        - ç›¸å¯¹è·¯å¾„ï¼š../data/file.xlsx, data/file.csv
        
        å·¥å…·å‚æ•°ï¼š
        - file_paths: æå–çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œä¾‹å¦‚ ["d:\\data\\file.xlsx"]
        - provider: "openai"
        - purpose: "assistants"

        **é‡è¦ï¼š** 
        - å¦‚æœæ–‡ä»¶å·²ç»ä¸Šä¼ ï¼ˆå¦‚ä¸Šé¢æ˜¾ç¤ºçš„å·²ä¸Šä¼ æ–‡ä»¶ï¼‰ï¼Œè¯·ä¸è¦é‡å¤ä¸Šä¼ ï¼
        - å¦‚æœæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼ˆå¦‚ä¸Šé¢æ˜¾ç¤ºçš„å¤±è´¥æ–‡ä»¶ï¼‰ï¼Œè¯·ä¸è¦é‡å¤å°è¯•ï¼

        **åˆ¤æ–­æ ‡å‡†ï¼š**
        ç”¨æˆ·æä¾›äº†è¡¨æ ¼æ¨¡æ¿å½“ä¸”ä»…å½“æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š

        1. **ç»“æ„åŒ–æè¿°**ï¼šç”¨æˆ·æ¸…æ™°ã€è¯¦ç»†åœ°æè¿°äº†è¡¨æ ¼çš„å®Œæ•´ç»“æ„
        2. **æ–‡ä»¶æ¨¡æ¿**ï¼šç”¨æˆ·æä¾›äº†åŒ…å«è¡¨æ ¼ç»“æ„çš„æ–‡ä»¶ï¼ˆåŒ…æ‹¬å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
        3. **å…·ä½“ç¤ºä¾‹**ï¼šç”¨æˆ·ç»™å‡ºäº†è¡¨æ ¼çš„å…·ä½“ç¤ºä¾‹

        **è¾“å‡ºè¦æ±‚ï¼š**
        - å¦‚æœç”¨æˆ·æä¾›äº†å®Œæ•´è¡¨æ ¼æ¨¡æ¿ï¼Œå›ç­” [YES]
        - å¦‚æœç”¨æˆ·æœªæä¾›å®Œæ•´æ¨¡æ¿ï¼Œå›ç­” [NO]
        - å¦‚æœéœ€è¦åˆ†æNEWæ–‡ä»¶ï¼Œè¯·å…ˆè°ƒç”¨å·¥å…·ä¸Šä¼ æ–‡ä»¶ï¼Œç„¶ååŸºäºåˆ†æç»“æœåˆ¤æ–­

        **æ³¨æ„äº‹é¡¹**
        å¦‚æœä¿¡æ¯ä¸å¤Ÿå®Œæ•´ï¼Œå›ç­” [NO]
        """
        
        # è¿‡æ»¤æ¶ˆæ¯ï¼Œç§»é™¤å·¥å…·æ¶ˆæ¯
        filtered_messages = self._filter_messages_for_llm(state["messages"])

        # ä½¿ç”¨å¸¦å·¥å…·çš„LLM
        llm_with_tools = self.llm.bind_tools(self.tools)
        messages = [SystemMessage(content=system_prompt)] + filtered_messages
        response = llm_with_tools.invoke(messages)

        has_template = "[YES]" in response.content.upper()
        
        return {
            "has_template": has_template,
            "messages": [response]
        }

    def _route_after_template_check(self, state: FrontdeskState) -> str:
        """æ ¹æ®LLMæ˜¯å¦è°ƒç”¨å·¥å…·æ¥å†³å®šè·¯ç”±"""
        if state.get("messages"):
            latest_message = state["messages"][-1]
            if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
                # Set previous node before going to file upload
                state["previous_node"] = "check_template"
                return "file_upload"
        return "has_template" if state["has_template"] else "no_template"

    def _confirm_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·ç¡®è®¤æ¨¡æ¿ç»†èŠ‚ - æ”¯æŒæ™ºèƒ½å·¥å…·è°ƒç”¨"""

        # If complete_confirm is already True, don't override it
        if state.get("complete_confirm", False):
            return {
                "messages": state["messages"],
                "complete_confirm": True
            }

        # è·å–å·²ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯
        uploaded_files_info = ""
        if state.get("uploaded_files_id"):
            uploaded_files_info = f"\n**å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼š**\nå·²æˆåŠŸä¸Šä¼  {len(state['uploaded_files_id'])} ä¸ªæ–‡ä»¶åˆ°ç³»ç»Ÿä¸­ï¼Œæ–‡ä»¶ID: {state['uploaded_files_id']}\n"

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼æ¨¡æ¿å®¡æ ¸ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸»åŠ¨ä¸ç”¨æˆ·ç¡®è®¤å’Œå®Œå–„è¡¨æ ¼æ¨¡æ¿çš„è¯¦ç»†ä¿¡æ¯ã€‚
        {uploaded_files_info}
        **å·¥å…·ä½¿ç”¨æŒ‡å—ï¼š**
        å¦‚æœç”¨æˆ·åœ¨å¯¹è¯ä¸­æåˆ°äº†NEWæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶åï¼Œæˆ–è€…è¯´è¦ä¸Šä¼ æ–°æ–‡ä»¶æ¥è¡¥å……æ¨¡æ¿ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨ upload_file_to_LLM_tool å·¥å…·ã€‚
        å·¥å…·å‚æ•°è¯´æ˜ï¼š
        - file_paths: ä»ç”¨æˆ·è¾“å…¥ä¸­æå–çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        - provider: ä½¿ç”¨ "openai"
        - purpose: ä½¿ç”¨ "assistants"
        
        **é‡è¦ï¼š** å¦‚æœæ–‡ä»¶å·²ç»ä¸Šä¼ ï¼ˆå¦‚ä¸Šé¢æ˜¾ç¤ºçš„å·²ä¸Šä¼ æ–‡ä»¶ï¼‰ï¼Œè¯·ä¸è¦é‡å¤ä¸Šä¼ ï¼

        **ä½ éœ€è¦æŒ‰é¡ºåºç¡®è®¤ä»¥ä¸‹ä¿¡æ¯ï¼š**
        1. **è¡¨æ ¼çš„ç”¨é€”å’Œç›®æ ‡**ï¼šç¡®è®¤è¡¨æ ¼çš„å…·ä½“ç”¨é€”ï¼Œç”¨æ¥åšä»€ä¹ˆï¼Ÿè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
        2. **éœ€è¦æ”¶é›†çš„å…·ä½“ä¿¡æ¯ç±»å‹**ï¼šç¡®è®¤æ‰€æœ‰æ•°æ®å­—æ®µï¼Œæ˜¯å¦æœ‰é—æ¼çš„é‡è¦å­—æ®µï¼Ÿ
        3. **è¡¨æ ¼ç»“æ„è®¾è®¡**ï¼šç¡®è®¤æ˜¯å¦éœ€è¦å¤šçº§è¡¨å¤´ï¼Ÿå¦‚ä½•åˆ†ç»„ï¼Ÿå±‚çº§å…³ç³»æ˜¯å¦åˆç†ï¼Ÿ
        4. **ç‰¹æ®Šè¦æ±‚**ï¼šç¡®è®¤æ ¼å¼ã€éªŒè¯è§„åˆ™ã€ç‰¹æ®ŠåŠŸèƒ½ç­‰

        **å¯¹è¯ç­–ç•¥ï¼š**
        - ä¸»åŠ¨è¯¢é—®ï¼Œä¸è¦è¢«åŠ¨ç­‰å¾…
        - ä¸€æ¬¡ç¡®è®¤1-2ä¸ªå…·ä½“é—®é¢˜ï¼Œé¿å…è®©ç”¨æˆ·æ„Ÿåˆ°å›°æ‰°
        - å¦‚æœå‘ç°ä»»ä½•ä¸ç¡®å®šæˆ–ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œè¯·å…·ä½“æŒ‡å‡ºå¹¶è¯¢é—®ç”¨æˆ·
        - æ ¹æ®ç”¨æˆ·å›ç­”ç»™å‡ºå»ºè®®å’Œé€‰é¡¹
        - å¦‚æœç”¨æˆ·å›ç­”æ¨¡ç³Šï¼Œè¿½é—®å…·ä½“ç»†èŠ‚
        - å½“ç¡®è®¤æ‰€æœ‰ä¿¡æ¯éƒ½æ¸…æ™°å®Œæ•´æ—¶ï¼Œä¸»åŠ¨æ€»ç»“å¹¶æ ‡è®° [COMPLETE]
        - å¦‚æœç”¨æˆ·æä¾›æ–°æ–‡ä»¶æ¥è¡¥å……ä¿¡æ¯ï¼Œè¯·å…ˆè°ƒç”¨å·¥å…·ä¸Šä¼ åˆ†æ

        **åˆ¤æ–­å®Œæˆæ ‡å‡†ï¼š**
        å½“ä½ ç¡®è®¤äº†è¡¨æ ¼ç”¨é€”ã€æ‰€æœ‰å­—æ®µè¯¦æƒ…ã€ç»“æ„ç»„ç»‡æ–¹å¼ã€ç‰¹æ®Šè¦æ±‚åï¼Œåº”è¯¥ä¸»åŠ¨æ€»ç»“ä¿¡æ¯å¹¶åœ¨å›å¤æœ«å°¾åŠ ä¸Š [COMPLETE] æ ‡è®°ã€‚

        å½“æ¨¡æ¿ç¡®è®¤å®Œæˆåè¯·åœ¨å›å¤ç»“å°¾åŠ å…¥[COMPLETE]
        """

        # è¿‡æ»¤æ¶ˆæ¯ï¼Œç§»é™¤å·¥å…·æ¶ˆæ¯
        filtered_messages = self._filter_messages_for_llm(state["messages"])

        # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
        if not filtered_messages or not isinstance(filtered_messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt)] + filtered_messages
        else:
            messages = [SystemMessage(content=system_prompt)] + filtered_messages[1:]

        # ä½¿ç”¨å¸¦å·¥å…·çš„LLM
        llm_with_tools = self.llm.bind_tools(self.tools)
        response = llm_with_tools.invoke(messages)
        complete_confirm = "[COMPLETE]" in response.content.upper()

        return{
            "complete_confirm": complete_confirm,
            "messages": [response]
        }
    
    def _route_after_template_confirm(self, state: FrontdeskState) -> str:
        """æ ¹æ®LLMæ˜¯å¦è°ƒç”¨å·¥å…·æ¥å†³å®šè·¯ç”±"""
        if state.get("messages"):
            latest_message = state["messages"][-1]
            if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
                # Set previous node before going to file upload
                state["previous_node"] = "confirm_template"
                return "upload_file"
        
        return "complete_confirm" if state["complete_confirm"] else "incomplete_confirm"
    
    def _gather_user_template_supplement(self, state: FrontdeskState) -> FrontdeskState:
        """æ”¶é›†ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œæ¥ç¡®è®¤æ¨¡æ¿"""
        user_response = interrupt("è¯·ä¸ºæ¨¡æ¿æä¾›è¡¥å……ä¿¡æ¯: ")
        return {
            "messages": [HumanMessage(content=user_response)]
        }

    def _gather_requirements_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·å¯¹è¯ç¡®å®šç”Ÿæˆè¡¨æ ¼çš„å†…å®¹ï¼Œè¦æ±‚ç­‰ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥åˆ†æå’Œæ™ºèƒ½å·¥å…·è°ƒç”¨"""

        # If gather_complete is already True, don't override it
        if state.get("gather_complete", False):
            return {
                "messages": state["messages"],
                "gather_complete": True
            }

        # è·å–å·²ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯
        uploaded_files_info = ""
        if state.get("uploaded_files_id"):
            uploaded_files_info = f"\n**å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼š**\nå·²æˆåŠŸä¸Šä¼  {len(state['uploaded_files_id'])} ä¸ªæ–‡ä»¶åˆ°ç³»ç»Ÿä¸­ï¼Œæ–‡ä»¶ID: {state['uploaded_files_id']}\n"

        system_prompt_text = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„excelè¡¨æ ¼è®¾è®¡ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·å®Œæˆè¡¨æ ¼è®¾è®¡ã€‚
        ä½ å¯ä»¥åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€æ–‡æ¡£ã€Excelæ–‡ä»¶ç­‰ï¼‰æ¥æ›´å¥½åœ°ç†è§£ä»–ä»¬çš„éœ€æ±‚ã€‚
        {uploaded_files_info}
        **å·¥å…·ä½¿ç”¨æŒ‡å—ï¼š**
        å¦‚æœç”¨æˆ·åœ¨å¯¹è¯ä¸­æåˆ°äº†NEWæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶åï¼Œæˆ–è€…è¯´è¦ä¸Šä¼ æ–°æ–‡ä»¶æ¥å¸®åŠ©è®¾è®¡è¡¨æ ¼ï¼Œè¯·ä½¿ç”¨ upload_file_to_LLM_tool å·¥å…·ã€‚
        å·¥å…·å‚æ•°è¯´æ˜ï¼š
        - file_paths: ä»ç”¨æˆ·è¾“å…¥ä¸­æå–çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        - provider: ä½¿ç”¨ "openai"
        - purpose: ä½¿ç”¨ "assistants"
        
        **é‡è¦ï¼š** å¦‚æœæ–‡ä»¶å·²ç»ä¸Šä¼ ï¼ˆå¦‚ä¸Šé¢æ˜¾ç¤ºçš„å·²ä¸Šä¼ æ–‡ä»¶ï¼‰ï¼Œè¯·ä¸è¦é‡å¤ä¸Šä¼ ï¼

        **ä½ éœ€è¦æŒ‰é¡ºåºæ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š**
        1. è¡¨æ ¼çš„ç”¨é€”å’Œç›®æ ‡ï¼ˆç”¨æ¥åšä»€ä¹ˆï¼Ÿè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼‰
        2. éœ€è¦æ”¶é›†çš„å…·ä½“ä¿¡æ¯ç±»å‹ï¼ˆå“ªäº›æ•°æ®å­—æ®µï¼Ÿï¼‰ï¼Œå¯ä»¥å‘æ•£æ€ç»´é€‚å½“è¿½é—®ç”¨æˆ·è¡¥å……é¢å¤–æ•°æ®
        3. è¡¨æ ¼ç»“æ„è®¾è®¡ï¼ˆæ˜¯å¦éœ€è¦å¤šçº§è¡¨å¤´ï¼Ÿå¦‚ä½•åˆ†ç»„ï¼Ÿï¼‰
        4. ç‰¹æ®Šè¦æ±‚ï¼ˆæ ¼å¼ã€éªŒè¯è§„åˆ™ã€ç‰¹æ®ŠåŠŸèƒ½ç­‰ï¼‰

        **å¤šæ¨¡æ€åˆ†æèƒ½åŠ›ï¼š**
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡ï¼Œå°è¯•åˆ†æå›¾ç‰‡ä¸­çš„è¡¨æ ¼ç»“æ„æˆ–ç›¸å…³ä¿¡æ¯
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡æ¡£ï¼Œè€ƒè™‘æ–‡æ¡£ä¸­å¯èƒ½åŒ…å«çš„è¡¨æ ¼éœ€æ±‚æˆ–æ¨¡æ¿
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†Excel/CSVæ–‡ä»¶ï¼Œåˆ†æå…¶ç»“æ„ä½œä¸ºå‚è€ƒ

        **å¯¹è¯ç­–ç•¥ï¼š**
        - ä¸»åŠ¨è¯¢é—®ï¼Œä¸è¦è¢«åŠ¨ç­‰å¾…
        - ä¸€æ¬¡é—®1æˆ–2ä¸ªå…·ä½“é—®é¢˜
        - æ ¹æ®ç”¨æˆ·å›ç­”å’Œä¸Šä¼ çš„æ–‡ä»¶ç»™å‡ºå»ºè®®å’Œé€‰é¡¹
        - å¦‚æœç”¨æˆ·å›ç­”æ¨¡ç³Šï¼Œè¿½é—®å…·ä½“ç»†èŠ‚
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†ç›¸å…³æ–‡ä»¶ï¼Œä¸»åŠ¨æåŠå¹¶è¯¢é—®æ˜¯å¦åŸºäºæ–‡ä»¶å†…å®¹è®¾è®¡
        - å½“æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯è®¾è®¡å®Œæ•´è¡¨æ ¼æ—¶ï¼Œä¸»åŠ¨æ€»ç»“å¹¶æ ‡è®° [COMPLETE]
        - å¦‚æœç”¨æˆ·æä¾›æ–‡ä»¶ï¼Œè¯·å…ˆè°ƒç”¨å·¥å…·ä¸Šä¼ åˆ†æï¼Œç„¶ååŸºäºåˆ†æç»“æœç»§ç»­å¯¹è¯

        **åˆ¤æ–­å®Œæˆæ ‡å‡†ï¼š**
        å½“ä½ æ˜ç¡®äº†è¡¨æ ¼ç”¨é€”ã€ä¸»è¦å­—æ®µã€ç»“æ„ç»„ç»‡æ–¹å¼åï¼Œåº”è¯¥ä¸»åŠ¨æ€»ç»“ä¿¡æ¯å¹¶åœ¨å›å¤æœ«å°¾åŠ ä¸Š [COMPLETE] æ ‡è®°ã€‚

        **ç¤ºä¾‹å®Œæˆæ€»ç»“æ ¼å¼ï¼š**
        "å¥½çš„ï¼Œæ ¹æ®æˆ‘ä»¬çš„è®¨è®ºå’Œæ‚¨æä¾›çš„æ–‡ä»¶ï¼Œæˆ‘å·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥è®¾è®¡è¿™ä¸ªè¡¨æ ¼ï¼š
        - ç”¨é€”ï¼š[æ€»ç»“ç”¨é€”]
        - ä¸»è¦å­—æ®µï¼š[åˆ—å‡ºå­—æ®µ]
        - ç»“æ„ï¼š[æè¿°è¡¨å¤´ç»„ç»‡]
        ç°åœ¨æˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†çš„è¡¨æ ¼ç»“æ„äº†ã€‚[COMPLETE]"
        """

        # è¿‡æ»¤æ¶ˆæ¯ï¼Œç§»é™¤å·¥å…·æ¶ˆæ¯
        filtered_messages = self._filter_messages_for_llm(state["messages"])

        # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
        if not filtered_messages or not isinstance(filtered_messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt_text)] + filtered_messages
        else:
            messages = [SystemMessage(content=system_prompt_text)] + filtered_messages[1:]

        # ä½¿ç”¨å¸¦å·¥å…·çš„LLM
        llm_with_tools = self.llm.bind_tools(self.tools)
        response = llm_with_tools.invoke(messages)
        gather_complete = "[COMPLETE]" in response.content

        return {
            "messages": [response],
            "gather_complete": gather_complete    
        }

    def _route_after_gather_requirements(self, state: FrontdeskState) -> str:
        """æ ¹æ®LLMæ˜¯å¦è°ƒç”¨å·¥å…·æ¥å†³å®šè·¯ç”±"""
        if state.get("messages"):
            latest_message = state["messages"][-1]
            if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
                # Set previous node before going to file upload
                state["previous_node"] = "gather_requirements"
                return "upload_file"
        
        return "complete" if state["gather_complete"] else "continue"

    def _gather_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """ç”¨æˆ·å’Œagentå¯¹è¯ç¡®è®¤ä¿¡æ¯ï¼Œæˆ–æä¾›é¢å¤–ä¿¡æ¯ç”¨äºæ™ºèƒ½ä½“æ”¶é›†è¡¨æ ¼ä¿¡æ¯"""
        user_response = interrupt("è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
        return {
            "messages": [HumanMessage(content=user_response)]
        }


    def _process_html(file_path: Path, state: FrontdeskState) -> FrontdeskState:
        """This is the node that let LLM process the cleaned Html and add short description in each cell"""
        system_promtp = """"""

        

    

    
    def _store_information_node(self, state: FrontdeskState) -> FrontdeskState:
        """å°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»“æ„åŒ–å‚¨å­˜"""

        # Check if we have enough conversation context
        conversation_length = len([msg for msg in state["messages"] if isinstance(msg, (HumanMessage, AIMessage))])
        
        if conversation_length < 2:
            # æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·åˆå§‹è¾“å…¥åˆ›ç«‹åŸºç¡€è¡¨æ ¼
            initial_input = state["messages"][0].content if state["messages"] else "æœªçŸ¥éœ€æ±‚"
            
            basic_template = {
                "table_info": {
                    "purpose": f"åŸºäºç”¨æˆ·è¾“å…¥åˆ›å»ºçš„è¡¨æ ¼ï¼š{initial_input}",
                    "description": "ç”¨æˆ·æä¾›çš„åŸºæœ¬éœ€æ±‚ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„",
                    "data_sources": ["ç”¨æˆ·è¾“å…¥"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "multi_level_headers": [
                        {
                            "name": "å¾…å®šå­—æ®µ1",
                            "description": "éœ€è¦è¿›ä¸€æ­¥ç¡®å®šçš„è¡¨å¤´",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹æ•°æ®"
                        }
                    ]
                },
                "additional_requirements": {
                    "formatting": ["å¾…ç¡®å®š"],
                    "validation_rules": ["å¾…ç¡®å®š"],
                    "special_features": ["å¾…ç¡®å®š"]
                }
            }
            
            print("âš ï¸ å¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œç”ŸæˆåŸºç¡€æ¨¡æ¿")
            return {
                **state,
                "table_info": basic_template["table_info"],
                "table_structure": basic_template["table_structure"],
                "additional_requirements": basic_template["additional_requirements"],
                "gather_complete": True
            }

        system_prompt ="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®å¯¹è¯å†å²è®°å½•ï¼Œæˆ–ç”¨æˆ·æä¾›çš„è¡¨æ ¼æ¨¡æ¿ï¼Œ
        æå–å¹¶ç»“æ„åŒ–è¡¨æ ¼ç›¸å…³ä¿¡æ¯ã€‚

        **é‡è¦æé†’ï¼š**
        å¦‚æœå¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªåˆç†çš„åŸºç¡€ç»“æ„ï¼Œä¸è¦æ‹’ç»ç”Ÿæˆã€‚

        **ä»»åŠ¡è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–è¡¨æ ¼çš„ç”¨é€”ã€å†…å®¹ã€æ•°æ®éœ€æ±‚å’Œç»“æ„ä¿¡æ¯
        2. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
        3. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ•°æ®ç»“æ„è¾“å‡º

        **è¡¨æ ¼ç»“æ„è¯´æ˜ï¼š**
        - å¯¹äºå¤šçº§è¡¨å¤´ï¼Œä½¿ç”¨åµŒå¥—çš„æ•°ç»„å’Œå­—å…¸ç»“æ„
        - æ ‡é¢˜è¡¨å¤´ï¼ˆæœ‰å­çº§çš„ï¼‰åªåŒ…å« name å’Œ children å­—æ®µ
        - æ•°æ®è¡¨å¤´ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰åŒ…å« name, description, data_type, required, example å­—æ®µ
        - æ”¯æŒä»»æ„å±‚çº§çš„åµŒå¥—ç»“æ„

        **è¾“å‡ºæ ¼å¼ï¼š**
        è¯·ç›´æ¥è¾“å‡ºJSONå†…å®¹ï¼Œä¸è¦ä½¿ç”¨markdownä»£ç å—åŒ…è£…ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
        {
        "table_info": {
            "purpose": "è¡¨æ ¼çš„å…·ä½“ç”¨é€”å’Œç›®æ ‡",
            "description": "è¡¨æ ¼å†…å®¹çš„è¯¦ç»†æè¿°",
            "data_sources": ["æ•°æ®æ¥æº1", "æ•°æ®æ¥æº2"],
            "target_users": ["ç›®æ ‡ç”¨æˆ·ç¾¤ä½“"],
            "frequency": "ä½¿ç”¨é¢‘ç‡ï¼ˆå¦‚ï¼šæ¯æ—¥/æ¯å‘¨/æ¯æœˆï¼‰"
        },
        "table_structure": {
            "has_multi_level": true,
            "multi_level_headers": [
            {
                "name": "ç¬¬ä¸€çº§æ ‡é¢˜è¡¨å¤´åç§°",
                "children": [
                {
                    "name": "ç¬¬äºŒçº§æ ‡é¢˜è¡¨å¤´åç§°",
                    "children": [
                    {
                        "name": "æ•°æ®å­—æ®µåç§°",
                        "description": "æ•°æ®å­—æ®µè¯´æ˜",
                        "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                        "required": true,
                        "example": "ç¤ºä¾‹æ•°æ®"
                    }
                    ]
                },
                {
                    "name": "ç›´æ¥æ•°æ®å­—æ®µåç§°",
                    "description": "æ•°æ®å­—æ®µè¯´æ˜",
                    "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                    "required": false,
                    "example": "ç¤ºä¾‹æ•°æ®"
                }
                ]
            }
            ]
        },
        "additional_requirements": {
            "formatting": ["æ ¼å¼è¦æ±‚"],
            "validation_rules": ["æ•°æ®éªŒè¯è§„åˆ™"],
            "special_features": ["ç‰¹æ®ŠåŠŸèƒ½éœ€æ±‚"]
        }
        }

        **ç»“æ„ç¤ºä¾‹è¯´æ˜ï¼š**
        - å¦‚æœè¡¨å¤´æ˜¯æ ‡é¢˜æ€§è´¨ï¼ˆæœ‰å­è¡¨å¤´ï¼‰ï¼Œåªéœ€è¦ "name" å’Œ "children"
        - å¦‚æœè¡¨å¤´æ˜¯æ•°æ®å­—æ®µï¼ˆå¶å­èŠ‚ç‚¹ï¼‰ï¼Œéœ€è¦å®Œæ•´çš„å­—æ®µä¿¡æ¯
        - children æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥åŒ…å«æ›´å¤šçš„æ ‡é¢˜è¡¨å¤´æˆ–æ•°æ®å­—æ®µ
        - æ”¯æŒ2çº§ã€3çº§æˆ–æ›´å¤šçº§çš„åµŒå¥—ç»“æ„
        """
        print("æ­£åœ¨ç”Ÿæˆè¡¨æ ¼æ¨¡æ¿......")
        system_message = SystemMessage(content=system_prompt)
        filtered_messages = self._filter_messages_for_llm(state["messages"])
        messages = [system_message] + filtered_messages
        response = self.llm.invoke(messages)

        try:
            # Clean the response content to handle markdown-wrapped JSON
            response_content = response.content.strip()
            
            # ç§»é™¤markdownè¾“å‡º
            if response_content.startswith('```json'):
                response_content = response_content[7:]  # Remove ```json
            if response_content.startswith('```'):
                response_content = response_content[3:]   # Remove ```
            if response_content.endswith('```'):
                response_content = response_content[:-3]  # Remove trailing ```
            
            response_content = response_content.strip()
            
            # Parse the JSON response
            structured_output = json.loads(response_content)
            
            # Extract components
            table_info = structured_output["table_info"]
            table_structure = structured_output["table_structure"]
            additional_requirements = structured_output["additional_requirements"]

            # åˆ›å»ºå®Œæ•´çš„æ•°æ®
            complete_data = {
                "session_id": state.get("session_id", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "conversation_messages": [
                    {
                        "type": msg.__class__.__name__,
                        "content": msg.content
                    } for msg in state["messages"] if hasattr(msg, 'content')
                ]
            }

            print("âœ… JSONè§£ææˆåŠŸï¼Œè¡¨æ ¼æ¨¡æ¿ç”Ÿæˆå®Œæˆ")

            # åˆ›å»ºæ–‡ä»¶å¤¹ç”¨äºå­˜å‚¨ç”Ÿæˆçš„è¡¨æ ¼æ¨¡æ¿
            output_dir = "table_template"
            os.makedirs(output_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶åç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_id = state.get("session_id", "default")
            filename = f"table_template_{session_id}_{timestamp}.json"

            # å°†è¡¨æ ¼æ¨¡æ¿å‚¨å­˜åˆ°è¿™ä¸ªJSONæ–‡ä»¶
            file_path = os.path.join(output_dir, filename)
            with open(file_path, 'w', encoding = 'utf-8') as f:
                json.dump(complete_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… è¡¨æ ¼æ¨¡æ¿å·²ä¿å­˜åˆ°: {filename}")
        
            # Return updated state
            return {
                **state,
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "gather_complete": True
            }
        
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response.content}")
            print(f"æ¸…ç†åå“åº”: {response_content if 'response_content' in locals() else 'N/A'}")
            
            # Fallback: create a basic template from the conversation
            print("ğŸ”„ ä½¿ç”¨å¯¹è¯å†…å®¹ç”ŸæˆåŸºç¡€æ¨¡æ¿")
            fallback_template = {
                "table_info": {
                    "purpose": "æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆçš„è¡¨æ ¼",
                    "description": "åŸºäºç”¨æˆ·éœ€æ±‚çš„åŸºç¡€è¡¨æ ¼ç»“æ„",
                    "data_sources": ["ç”¨æˆ·å¯¹è¯"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "multi_level_headers": [
                        {
                            "name": "åŸºç¡€å­—æ®µ",
                            "description": "æ ¹æ®å¯¹è¯æ¨æ–­çš„å­—æ®µ",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹"
                        }
                    ]
                },
                "additional_requirements": {
                    "formatting": ["æ ‡å‡†æ ¼å¼"],
                    "validation_rules": ["åŸºæœ¬éªŒè¯"],
                    "special_features": ["æ— ç‰¹æ®Šè¦æ±‚"]
                }
            }
            
            return {
                **state,
                "table_info": fallback_template["table_info"],
                "table_structure": fallback_template["table_structure"],
                "additional_requirements": fallback_template["additional_requirements"],
                "gather_complete": True
            }
        except KeyError as e:
            print(f"âŒ JSONç»“æ„é”™è¯¯: {e}")
            print(f"å¯ç”¨é”®: {list(structured_output.keys()) if 'structured_output' in locals() else 'N/A'}")
            
            return {
                **state,
                "gather_complete": False
            }

    def _create_initial_state(self, user_input: str, session_id: str = "default") -> FrontdeskState:
        """åˆ›å»ºLanggraphæœ€åˆçŠ¶æ€ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œè‡ªåŠ¨æ–‡ä»¶è·¯å¾„æ£€æµ‹"""
        
        # æ£€æµ‹å¹¶å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„
        detected_files = detect_and_process_file_paths(user_input)
        
        return {
            "messages": [HumanMessage(content=user_input)],
            "session_id": session_id,
            "table_structure": {},
            "table_info": {},
            "additional_requirements": {},
            "gather_complete": False,
            "has_template": False,
            "complete_confirm": False,
            "uploaded_files": detected_files,  # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ–‡ä»¶è·¯å¾„
            "uploaded_files_id": [],  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
            "failed_uploads": [],  # åˆå§‹åŒ–å¤±è´¥ä¸Šä¼ åˆ—è¡¨
            "previous_node": "check_template"  # åˆå§‹çŠ¶æ€ä¸‹ï¼Œå¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œåº”è¯¥å›åˆ°check_template
        }
    
    def _route_after_file_upload(self, state: FrontdeskState) -> str:
        """æ–‡ä»¶ä¸Šä¼ å·¥å…·å¤„ç†å®Œæˆåçš„è·¯ç”±å†³ç­– - è¿”å›åˆ°LLMå¤„ç†èŠ‚ç‚¹"""
        previous_node = state.get("previous_node", "check_template")
        
        print(f"ğŸ“ æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œæ¥è‡ªèŠ‚ç‚¹: {previous_node}")
        
        # æ–‡ä»¶ä¸Šä¼ åè¿”å›åˆ°LLMå¤„ç†èŠ‚ç‚¹ï¼Œè®©LLMåˆ†ææ–‡ä»¶å†…å®¹
        if previous_node == "confirm_template":
            print("ğŸ“ ä»confirm_templateä¸Šä¼ æ–‡ä»¶å®Œæˆï¼Œè¿”å›confirm_templateå¤„ç†")
            return "confirm_template"
        elif previous_node == "gather_requirements":
            print("ğŸ“ ä»gather_requirementsä¸Šä¼ æ–‡ä»¶å®Œæˆï¼Œè¿”å›gather_requirementså¤„ç†")
            return "gather_requirements"
        else:
            # å…¶ä»–æƒ…å†µï¼ˆå¦‚check_templateç­‰ï¼‰æ­£å¸¸è¿”å›åŸèŠ‚ç‚¹
            print(f"ğŸ“ è¿”å›åˆ°åŸèŠ‚ç‚¹: {previous_node}")
            return previous_node
    
    def run_front_desk_agent(self, user_input: str, session_id = "1") -> None: # session_idé»˜è®¤ä¸º1
        """æ‰§è¡Œå‰å°æ™ºèƒ½ä½“"""
        initial_state = self._create_initial_state(user_input, session_id)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ¤– æ­£åœ¨å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
        print("=" * 50)

        current_input = initial_state
        
        while True:
            try:
                has_interrupt = False
                for chunk in self.graph.stream(current_input, config = config, stream_mode = "updates"):
                    for node_name, node_output in chunk.items():
                        print(f"\nğŸ“ Node: {node_name}")
                        print("-" * 30)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰interrupt
                        if '__interrupt__' in chunk:
                            has_interrupt = True
                            interrupt_value = chunk['__interrupt__'][0].value
                            print(f"\nğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                            user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                            
                            # è®¾ç½®ä¸‹ä¸€æ¬¡å¾ªç¯çš„è¾“å…¥
                            current_input = Command(resume=user_response)
                            break
                        
                        if isinstance(node_output, dict):
                            if "messages" in node_output and node_output["messages"]:
                                latest_message = node_output["messages"][-1]
                                if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
                                    print(f"ğŸ’¬ æ™ºèƒ½ä½“å›å¤: {latest_message.content}")
                            
                            for key, value in node_output.items():
                                if key != "messages" and value:
                                    print(f"ğŸ“Š {key}: {value}")
                        print("-" * 30)
                
                # å¦‚æœæ²¡æœ‰interruptï¼Œè¯´æ˜æµç¨‹å®Œæˆ
                if not has_interrupt:
                    break
                    
            except Exception as e:
                print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
                raise e
        
        print("\nâœ… è¡¨æ ¼æ¨¡æ¿ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":

    #åˆ›å»ºæ™ºèƒ½ä½“
    frontdeskagent = FrontDeskAgent()

    save_graph_visualization(frontdeskagent.graph)

    # user_input = input("ğŸ¤– ä½ å¥½æˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½å¡«è¡¨åŠ©æ‰‹ï¼Œè¯·å‘Šè¯‰æˆ‘ä½ æƒ³å¡«ä»€ä¹ˆè¡¨æ ¼: \n")
    # frontdeskagent.run_front_desk_agent(user_input)