import sys
from pathlib import Path
import io
import contextlib

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated

from utils.file_process import (read_txt_file, 
                                process_excel_files_for_integration,
                                process_excel_files_for_merge)
from utils.modelRelated import invoke_model
from utils.html_generator import (
    extract_empty_row_html_code_based,
    extract_headers_html_code_based,
    extract_footer_html_code_based,
    transform_data_to_html_code_based,
    combine_html_parts
)

import os
import pandas as pd
from bs4 import BeautifulSoup
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv


from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.constants import Send
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


# import other agents
from agents.processUserInput import ProcessUserInputAgent

load_dotenv()

class FilloutTableState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    data_file_path: list[str]
    supplement_files_summary: str
    template_file: str
    fill_CSV_2_template_code: str
    combined_data: str
    filled_row: str
    template_completion_code_execution_successful: bool
    CSV2Teplate_template_completion_code_execution_successful: bool
    retry: int
    combined_data_array: list[str]
    headers_mapping: str
    largest_file_row_num: int
    combined_html: str
    # Use lambda reducers for concurrent updates
    empty_row_html: Annotated[str, lambda old, new: new if new else old]
    headers_html: Annotated[str, lambda old, new: new if new else old]
    footer_html: Annotated[str, lambda old, new: new if new else old]
    CSV_data: Annotated[list[str], lambda old, new: new if new else old]
    modify_after_first_fillout: bool
    village_name: str
    strategy_for_data_combination: str

class FilloutTableAgent:
    def __init__(self):
        self.graph = self._build_graph()
        



    def _build_graph(self):
        """Build the LangGraph workflow for filling out tables"""
        graph = StateGraph(FilloutTableState)
        
        # Add nodes
        graph.add_node("determine_strategy_for_data_combination", self._determine_strategy_for_data_combination)
        graph.add_node("combine_data_for_multitable_integration", self._combine_data_for_multitable_integration)
        graph.add_node("combine_data_for_multitable_merge", self._combine_data_for_multitable_merge)
        graph.add_node("generate_CSV_based_on_combined_data", self._generate_CSV_based_on_combined_data)
        graph.add_node("transform_data_to_html", self._transform_data_to_html_code_based)  # Use code-based function
        graph.add_node("extract_empty_row_html", self._extract_empty_row_html_code_based)
        graph.add_node("extract_headers_html", self._extract_headers_html_code_based)
        graph.add_node("extract_footer_html", self._extract_footer_html_code_based)
        graph.add_node("combine_html_tables", self._combine_html_tables)
        graph.add_node("shield_for_transform_data_to_html", self._shield_for_transform_data_to_html)
        
        # Define the workflow
        graph.add_edge(START, "determine_strategy_for_data_combination")
        graph.add_conditional_edges("determine_strategy_for_data_combination", self._route_after_determine_strategy_for_data_combination)
        graph.add_conditional_edges("combine_data_for_multitable_integration", self._route_after_chunking_data)
        graph.add_conditional_edges("combine_data_for_multitable_merge", self._route_after_chunking_data)
        graph.add_edge("extract_empty_row_html", "shield_for_transform_data_to_html")
        graph.add_edge("extract_headers_html", "shield_for_transform_data_to_html")
        graph.add_edge("extract_footer_html", "shield_for_transform_data_to_html")
        graph.add_edge("generate_CSV_based_on_combined_data", "shield_for_transform_data_to_html")
        graph.add_edge("shield_for_transform_data_to_html", "transform_data_to_html")
        graph.add_edge("transform_data_to_html", "combine_html_tables")
        graph.add_edge("combine_html_tables", END)
        

        
        # Compile the graph
        return graph.compile()

    
    def create_initialize_state(self, session_id: str,
                                 template_file: str = None,
                                 data_file_path: list[str] = None,
                                 headers_mapping: dict[str, str] = None,
                                 supplement_files_summary: str = "",
                                 modify_after_first_fillout: bool = False,
                                 village_name: str = "") -> FilloutTableState:
        """This node will initialize the state of the graph"""
        return {
            "messages": [],
            "session_id": session_id,
            "data_file_path": data_file_path, # excel files(xls) that has raw data
            "template_file": template_file, # txt file of template file in html format
            "fill_CSV_2_template_code": "",
            "combined_data": "",
            "filled_row": "",
            "template_completion_code_execution_successful": False,
            "CSV2Teplate_template_completion_code_execution_successful": False,
            "retry": 0,
            "combined_data_array": [],
            "headers_mapping": headers_mapping,
            "CSV_data": [],
            "largest_file_row_num": 66,
            "supplement_files_summary": supplement_files_summary,
            "empty_row_html": "",
            "headers_html": "",
            "footer_html": "",
            "combined_html": "",
            "modify_after_first_fillout": False,
            "village_name": village_name,
            "strategy_for_data_combination": ""
        }
    def _determine_strategy_for_data_combination(self, state: FilloutTableState) -> FilloutTableState:
        """æ ¹æ®æˆ‘ä»¬è¦å¡«å†™çš„è¡¨æ ¼æ¥å†³å®šæ•°æ®æ•´åˆçš„ç­–ç•¥"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æ•´åˆç­–ç•¥åˆ†æä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘
åˆ†æç»™å®šçš„è¡¨æ ¼ç»“æ„æ˜ å°„ï¼Œç¡®å®šæ•°æ®æ•´åˆç­–ç•¥ã€‚

ã€ç­–ç•¥ç±»å‹ã€‘
æœ‰ä¸”ä»…æœ‰ä¸¤ç§ç­–ç•¥ï¼š

1. **å¤šè¡¨æ•´åˆ** - ç‰¹å¾ï¼š
   - å­˜åœ¨ä¸€ä¸ªä¸»è¦æ•°æ®æºï¼ˆé€šå¸¸æ˜¯æœ€å¤§çš„è¡¨æ ¼æˆ–åŒ…å«æœ€å¤šæ ¸å¿ƒä¿¡æ¯çš„è¡¨æ ¼ï¼‰
   - å…¶ä»–è¡¨æ ¼ä½œä¸ºè¡¥å……æ•°æ®æºï¼Œç”¨äºå¡«å……ç¼ºå¤±å­—æ®µ
   - å„å­—æ®µçš„æ•°æ®æ¥æºç›¸å¯¹ç‹¬ç«‹ï¼Œä¸å­˜åœ¨è·¨è¡¨æ ¼çš„å­—æ®µåˆå¹¶
   - ç¤ºä¾‹ï¼šè¡¨æ ¼ç»“æ„ä¸­å­—æ®µæ¥æºæ ¼å¼ä¸º"è¡¨æ ¼A:å­—æ®µX"ã€"è¡¨æ ¼B:å­—æ®µY"ç­‰

2. **å¤šè¡¨åˆå¹¶** - ç‰¹å¾ï¼š
   - å¤šä¸ªè¡¨æ ¼åœ°ä½ç›¸ç­‰ï¼Œéœ€è¦å°†å®ƒä»¬çš„æ•°æ®è¡Œåˆå¹¶åˆ°ä¸€å¼ è¡¨
   - åŒä¸€å­—æ®µå¯èƒ½æ¥è‡ªå¤šä¸ªè¡¨æ ¼çš„ç›¸åŒå­—æ®µ
   - å­—æ®µæ¥æºæ ¼å¼åŒ…å«"/"åˆ†éš”ç¬¦ï¼Œå¦‚"è¡¨æ ¼A:å­—æ®µX/è¡¨æ ¼B:å­—æ®µX"
   - æœ€ç»ˆè¡¨æ ¼çš„è¡Œæ•°ç­‰äºæ‰€æœ‰æºè¡¨æ ¼çš„è¡Œæ•°ä¹‹å’Œ

ã€åˆ†ææ­¥éª¤ã€‘
1. æ£€æŸ¥å­—æ®µæ¥æºæ ¼å¼ï¼šæ˜¯å¦åŒ…å«"/"åˆ†éš”ç¬¦
2. åˆ¤æ–­æ•°æ®æºå…³ç³»ï¼šæ˜¯ä¸»ä»å…³ç³»è¿˜æ˜¯å¹³ç­‰åˆå¹¶å…³ç³»
3. ç¡®å®šæœ€ç»ˆç­–ç•¥

ã€è¾“å‡ºè¦æ±‚ã€‘
ä»…è¾“å‡ºä»¥ä¸‹ä¸¤ä¸ªé€‰é¡¹ä¹‹ä¸€ï¼Œä¸å¾—åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
- å¤šè¡¨æ•´åˆ
- å¤šè¡¨åˆå¹¶

ã€ç¤ºä¾‹ã€‘
è¾“å…¥ï¼š"è¡¨å¤´1": ["è¡¨æ ¼1:å­—æ®µA"]ï¼Œ"è¡¨å¤´2": ["è¡¨æ ¼2:å­—æ®µB"] â†’ è¾“å‡ºï¼šå¤šè¡¨æ•´åˆ
è¾“å…¥ï¼š"è¡¨å¤´1": ["è¡¨æ ¼1:å­—æ®µA/è¡¨æ ¼2:å­—æ®µA"] â†’ è¾“å‡ºï¼šå¤šè¡¨åˆå¹¶
        """
        table_structure = state["headers_mapping"]
        response = invoke_model(model_name = "deepseek-ai/DeepSeek-V3", 
                                messages = [SystemMessage(content = system_prompt), HumanMessage(content = table_structure)])
        return {
            "strategy_for_data_combination": response
        }

    def _route_after_determine_strategy_for_data_combination(self, state: FilloutTableState) -> str:
        if state["strategy_for_data_combination"] == "å¤šè¡¨æ•´åˆ":
            return "combine_data_for_multitable_integration"
        elif state["strategy_for_data_combination"] == "å¤šè¡¨åˆå¹¶":
            return "combine_data_for_multitable_merge"
        else:
            return "combine_data_for_multitable_integration"  # default fallback
    
    def _combine_data_for_multitable_integration(self, state: FilloutTableState) -> FilloutTableState:
        """å°†å¤šä¸ªè¡¨æ ¼æ•´åˆ"""
        # return
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _combine_data_split_into_chunks")
        print("=" * 50)
        if not state["modify_after_first_fillout"]:
            try:
                # Get Excel file paths from state
                excel_file_paths = []
                print(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(state["data_file_path"])} ä¸ªæ•°æ®æ–‡ä»¶")
                
                # Convert data files to Excel paths if they're not already
                for file_path in state["data_file_path"]:
                    print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
                    if file_path.endswith('.txt'):
                        # Try to find corresponding Excel file
                        excel_path = file_path.replace('.txt', '.xlsx')
                        if Path(excel_path).exists():
                            excel_file_paths.append(excel_path)
                            print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                        else:
                            # Try .xls extension
                            excel_path = file_path.replace('.txt', '.xls')
                            if Path(excel_path).exists():
                                excel_file_paths.append(excel_path)
                                print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                            else:
                                print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {file_path}")
                    elif file_path.endswith(('.xlsx', '.xls', '.xlsm')):
                        excel_file_paths.append(file_path)
                        print(f"âœ… ç›´æ¥ä½¿ç”¨Excelæ–‡ä»¶: {file_path}")
                
                if not excel_file_paths:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Excelæ–‡ä»¶")
                    print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
                    print("=" * 50)
                    return {"combined_data_array": []}
                
                print(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(excel_file_paths)} ä¸ªExcelæ–‡ä»¶è¿›è¡Œåˆ†å—")
                

                print("ğŸ”„ æ­£åœ¨è°ƒç”¨process_excel_files_with_chunkingå‡½æ•°...")
                print("state['headers_mapping']çš„ç±»å‹: ", type(state["headers_mapping"]))
                chunked_result = process_excel_files_for_integration(excel_file_paths=excel_file_paths, 
                                                                session_id=state["session_id"],
                                                                chunk_nums=15, largest_file=None,  # Let function auto-detect
                                                                data_json_path="agents/data.json",
                                                                village_name=state["village_name"])
                
                # Extract chunks and row count from the result
                chunked_data = chunked_result["combined_chunks"]
                largest_file_row_count = chunked_result["largest_file_row_count"]
                
                
                for chunk in chunked_data:
                    print(f"==================ğŸ” æ•°æ®å— ==================:")
                    print(chunk)

                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunked_data)} ä¸ªæ•°æ®å—")
                print(f"ğŸ“Š æœ€å¤§æ–‡ä»¶è¡Œæ•°: {largest_file_row_count}")
                print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ")
                print("=" * 50)
                
                return {
                    "combined_data_array": chunked_data,
                    "largest_file_row_num": largest_file_row_count
                }
                
            except Exception as e:
                print(f"âŒ _combine_data_split_into_chunks æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
                print("=" * 50)
                return {
                    "combined_data_array": []
                }
        else:
            return state
    
    def _combine_data_for_multitable_merge(self, state: FilloutTableState) -> FilloutTableState:
        """å°†å¤šä¸ªè¡¨æ ¼åˆå¹¶èµ·æ¥ - æ‰€æœ‰data_file_pathä¸­çš„æ–‡ä»¶éƒ½ä½œä¸ºæ ¸å¿ƒæ•°æ®è¿›è¡Œåˆå¹¶"""
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _combine_data_for_multitable_merge")
        print("=" * 50)
        if not state["modify_after_first_fillout"]:
            try:
                # Get Excel file paths from state
                excel_file_paths = []
                print(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(state["data_file_path"])} ä¸ªæ•°æ®æ–‡ä»¶ï¼ˆå…¨éƒ¨ä½œä¸ºæ ¸å¿ƒæ•°æ®ï¼‰")
                
                # Convert data files to Excel paths if they're not already
                for file_path in state["data_file_path"]:
                    print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
                    if file_path.endswith('.txt'):
                        # Try to find corresponding Excel file
                        excel_path = file_path.replace('.txt', '.xlsx')
                        if Path(excel_path).exists():
                            excel_file_paths.append(excel_path)
                            print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                        else:
                            # Try .xls extension
                            excel_path = file_path.replace('.txt', '.xls')
                            if Path(excel_path).exists():
                                excel_file_paths.append(excel_path)
                                print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                            else:
                                print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {file_path}")
                    elif file_path.endswith(('.xlsx', '.xls', '.xlsm')):
                        excel_file_paths.append(file_path)
                        print(f"âœ… ç›´æ¥ä½¿ç”¨Excelæ–‡ä»¶: {file_path}")
                
                if not excel_file_paths:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Excelæ–‡ä»¶")
                    print("âœ… _combine_data_for_multitable_merge æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
                    print("=" * 50)
                    return {"combined_data_array": []}
                
                print(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(excel_file_paths)} ä¸ªExcelæ–‡ä»¶è¿›è¡Œåˆå¹¶ï¼ˆå…¨éƒ¨ä½œä¸ºæ ¸å¿ƒæ•°æ®ï¼‰")
                
                # For multitable merge, we treat all files as core data and combine them together
                # Rather than chunking based on one largest file, we merge all files row by row
                combined_data_result = process_excel_files_for_merge(
                    excel_file_paths=excel_file_paths,
                    session_id=state["session_id"],
                    village_name=state["village_name"],
                    chunk_nums=15
                )
                
                # Extract chunks and row count from the result
                chunked_data = combined_data_result["combined_chunks"]
                total_row_count = combined_data_result["total_row_count"]
                
                for chunk in chunked_data:
                    print(f"==================ğŸ” åˆå¹¶æ•°æ®å— ==================:")
                    print(chunk)

                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunked_data)} ä¸ªåˆå¹¶æ•°æ®å—")
                print(f"ğŸ“Š æ€»è¡Œæ•°: {total_row_count}")
                print("âœ… _combine_data_for_multitable_merge æ‰§è¡Œå®Œæˆ")
                print("=" * 50)
                
                return {
                    "combined_data_array": chunked_data,
                    "largest_file_row_num": total_row_count
                }
                
            except Exception as e:
                print(f"âŒ _combine_data_for_multitable_merge æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                print("âœ… _combine_data_for_multitable_merge æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
                print("=" * 50)
                return {
                    "combined_data_array": []
                }
        else:
            return state
    
    
    def _route_after_chunking_data(self, state: FilloutTableState) -> str:
        """å¹¶è¡Œæ‰§è¡Œæ¨¡æ¿ä»£ç çš„ç”Ÿæˆå’ŒCSVæ•°æ®çš„åˆæˆ"""
        print("\nğŸ”€ å¼€å§‹æ‰§è¡Œ: _route_after_combine_data_split_into_chunks")
        print("=" * 50)
        if not state["modify_after_first_fillout"]:
            print("ğŸ”„ åˆ›å»ºå¹¶è¡Œä»»åŠ¡...")
            sends = []
            sends.append(Send("generate_CSV_based_on_combined_data", state))
            sends.append(Send("extract_empty_row_html", state))
            sends.append(Send("extract_headers_html", state))
            sends.append(Send("extract_footer_html", state)) 
            print("âœ… åˆ›å»ºäº†4ä¸ªå¹¶è¡Œä»»åŠ¡:")
            print("   - generate_CSV_based_on_combined_data")
            print("   - extract_empty_row_html")
            print("   - extract_headers_html")
            print("   - extract_footer_html")
        
            
            print("âœ… _route_after_combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return sends

    
    def _generate_CSV_based_on_combined_data(self, state: FilloutTableState) -> FilloutTableState:
        """æ ¹æ®æ•´åˆçš„æ•°æ®ï¼Œæ˜ å°„å…³ç³»ï¼Œæ¨¡æ¿ç”Ÿæˆæ–°çš„æ•°æ®"""
        if not state["modify_after_first_fillout"]:
            return state
            print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _generate_CSV_based_on_combined_data")
            print("=" * 50)
            
    #         system_prompt = f"""
    # ä½ æ˜¯ä¸€åä¸“ä¸šä¸”ä¸¥è°¨çš„ç»“æ„åŒ–æ•°æ®å¡«æŠ¥ä¸“å®¶ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œè®¡ç®—èƒ½åŠ›ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®åŸå§‹æ•°æ®å’Œæ¨¡æ¿æ˜ å°„è§„åˆ™ï¼Œå°†æ•°æ®å‡†ç¡®è½¬æ¢ä¸ºç›®æ ‡ CSV æ ¼å¼ï¼Œè¾“å‡ºç»“æ„åŒ–ã€å¹²å‡€çš„æ•°æ®è¡Œã€‚

    # ã€è¾“å…¥å†…å®¹ã€‘
    # 1. æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼ˆJSON æ ¼å¼ï¼‰ï¼šæè¿°ç›®æ ‡è¡¨æ ¼æ¯ä¸€åˆ—çš„æ¥æºã€è®¡ç®—é€»è¾‘æˆ–æ¨ç†è§„åˆ™ï¼›
    # 2. åŸå§‹æ•°æ®é›†ï¼šåŒ…æ‹¬è¡¨å¤´ç»“æ„çš„ JSON å’Œ CSV æ•°æ®å—ï¼Œå…¶ä¸­æ¯æ¡æ•°æ®è¡Œå‰ä¸€è¡Œæ ‡æ³¨äº†å­—æ®µåç§°ï¼Œç”¨äºè¾…åŠ©å­—æ®µåŒ¹é…ã€‚

    # ã€ä»»åŠ¡æµç¨‹ã€‘
    # 1. è¯·ä½ é€å­—æ®µåˆ†ææ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼Œæ˜ç¡®è¯¥å­—æ®µçš„æ¥æºæˆ–æ¨ç†é€»è¾‘ï¼›
    # 2. è‹¥å­—æ®µæ¥è‡ªåŸå§‹æ•°æ®ï¼Œè¯·å…ˆå®šä½æ¥æºå­—æ®µå¹¶æ ¡éªŒå…¶æ ¼å¼ï¼›
    # 3. è‹¥å­—æ®µéœ€æ¨ç†ï¼ˆå¦‚æ—¥æœŸæ ¼å¼è½¬æ¢ã€å¹´é¾„è®¡ç®—ã€é€»è¾‘åˆ¤æ–­ç­‰ï¼‰ï¼Œè¯·å…ˆåœ¨è„‘ä¸­é€æ­¥æ¨å¯¼ï¼Œç¡®ä¿æ€è·¯æ¸…æ™°ï¼›
    # 4. è‹¥å­—æ®µéœ€è®¡ç®—ï¼Œè¯·å…ˆæ˜ç¡®æ‰€éœ€å…¬å¼å¹¶é€æ­¥è®¡ç®—å‡ºç»“æœï¼›
    # 5. åœ¨å®Œæˆæ‰€æœ‰å­—æ®µæ¨ç†åï¼Œå†å°†ç»“æœæŒ‰ç…§å­—æ®µé¡ºåºåˆå¹¶ä¸ºä¸€è¡Œ CSV æ•°æ®ï¼›
    # 6. åœ¨æ¯æ¬¡è¾“å‡ºå‰ï¼Œè¯·å…ˆ**åœ¨è„‘ä¸­é€é¡¹éªŒè¯å­—æ®µæ˜¯å¦åˆç†ã€æ ¼å¼æ˜¯å¦è§„èŒƒ**ã€‚

    # ğŸ’¡ è¯·ä½ åƒä¸€ä½äººç±»ä¸“å®¶ä¸€æ ·ï¼Œ**ä¸€æ­¥ä¸€æ­¥æ€è€ƒå†åšå†³å®š**ï¼Œä¸è¦è·³è¿‡ä»»ä½•é€»è¾‘è¿‡ç¨‹ã€‚

    # ã€è¾“å‡ºè¦æ±‚ã€‘
    # - ä»…è¾“å‡ºçº¯å‡€çš„ CSV æ•°æ®è¡Œï¼Œä¸åŒ…å«è¡¨å¤´ã€æ³¨é‡Šæˆ–ä»»ä½•å¤šä½™å†…å®¹ï¼›
    # - ä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”å­—æ®µï¼›
    # - æ¯è¡Œæ•°æ®å­—æ®µé¡ºåºå¿…é¡»ä¸æ¨¡æ¿è¡¨å¤´æ˜ å°„å®Œå…¨ä¸€è‡´ï¼›
    # - ä¸¥ç¦é—æ¼å­—æ®µã€é‡å¤å­—æ®µã€å¤šè¾“å‡ºç©ºå€¼æˆ–ç©ºè¡Œï¼›
    # - è¾“å‡ºä¸­ä¸å¾—å‡ºç° Markdown åŒ…è£¹ï¼ˆå¦‚ ```ï¼‰æˆ–é¢å¤–è¯´æ˜æ–‡å­—ã€‚

    # æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼š
    # {state["headers_mapping"]}
    # """ 
            system_prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šä¸”ä¸¥è°¨çš„ç»“æ„åŒ–æ•°æ®å¡«æŠ¥ä¸“å®¶ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œè®¡ç®—èƒ½åŠ›ã€‚

è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥è§£å†³è¿™ä¸ªæ•°æ®è½¬æ¢å’Œåˆæˆé—®é¢˜ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
æ ¹æ®å¤šä¸ªæ•°æ®æºï¼Œä»¥æ ¸å¿ƒæ•°æ®æºä¸ºä¸»è¦ä¾æ®ï¼Œç»“åˆå‚è€ƒæ•°æ®æºè¿›è¡Œè¡¥å……éªŒè¯ï¼Œå°†æ•°æ®å‡†ç¡®è½¬æ¢ä¸ºç›®æ ‡ CSV æ ¼å¼ã€‚

ã€æ•°æ®æºè¯´æ˜ã€‘
1. æ ¸å¿ƒæ•°æ®æºï¼šæ ‡è®°ä¸º"=== æ ¸å¿ƒæ•°æ®æºï¼šxxx ==="çš„æ•°æ®ï¼Œä¸»è¦ä½œç”¨æ˜¯ç¡®å®šè¦ç”Ÿæˆçš„æ•°æ®è¡Œæ•°å’Œæä¾›æ•°æ®åˆ‡åˆ†çš„åŸºç¡€ç»“æ„
2. å‚è€ƒæ•°æ®æºï¼šæ ‡è®°ä¸º"=== å‚è€ƒæ•°æ®æºï¼šxxx ==="çš„æ•°æ®ï¼Œæä¾›ç”¨äºå¡«å……ç›®æ ‡è¡¨å¤´å­—æ®µçš„å…·ä½“ä¿¡æ¯
3. è¡¥å……ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼šæ ‡è®°ä¸º"=== è¡¥å……ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ ==="çš„å†…å®¹ï¼Œç”¨äºç†è§£ä¸šåŠ¡èƒŒæ™¯å’Œå¡«å……è§„åˆ™

ã€æ ¸å¿ƒå·¥ä½œåŸåˆ™ã€‘
â­ ä¸¥æ ¼æŒ‰ç…§è¡¨å¤´æ˜ å°„ï¼šç”Ÿæˆçš„æ¯ä¸ªå­—æ®µå¿…é¡»ä¸¥æ ¼å¯¹åº”æ¨¡æ¿è¡¨å¤´æ˜ å°„ä¸­çš„å®šä¹‰å’Œè¦æ±‚
â­ æ ¸å¿ƒæ•°æ®æºå†³å®šè¡Œæ•°ï¼šæ ¹æ®æ ¸å¿ƒæ•°æ®æºä¸­çš„æ•°æ®æ¡ç›®æ•°é‡æ¥ç¡®å®šéœ€è¦ç”Ÿæˆçš„CSVè¡Œæ•°
â­ å…¨æ•°æ®æºä¿¡æ¯å¡«å……ï¼šä»æ‰€æœ‰å¯ç”¨æ•°æ®æºä¸­è·å–ä¿¡æ¯æ¥å¡«å……è¡¨å¤´æ˜ å°„è¦æ±‚çš„å­—æ®µå†…å®¹
â­ ä¿æŒé€»è¾‘ä¸€è‡´æ€§ï¼šç¡®ä¿ç”Ÿæˆçš„æ•°æ®åœ¨ä¸šåŠ¡é€»è¾‘ä¸Šåˆç†ä¸”å­—æ®µé—´ç›¸äº’åè°ƒ

ã€è¾“å…¥å†…å®¹ã€‘
1. æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼ˆJSON æ ¼å¼ï¼‰ï¼šæè¿°ç›®æ ‡è¡¨æ ¼æ¯ä¸€åˆ—çš„æ¥æºã€è®¡ç®—é€»è¾‘æˆ–æ¨ç†è§„åˆ™
2. æ ¸å¿ƒæ•°æ®æºï¼šä¸»è¦ç”¨äºåˆæˆçš„æ•°æ®é›†
3. å‚è€ƒæ•°æ®æºï¼šç”¨äºè¡¥å……éªŒè¯çš„æ•°æ®é›†
4. è¡¥å……ä¿¡æ¯ï¼šä¸šåŠ¡èƒŒæ™¯å’Œå¡«å……è§„åˆ™

ã€è¯¦ç»†æ¨ç†è¦æ±‚ã€‘
ğŸ” å¯¹äºæ¯ä¸€è¡Œæ•°æ®ï¼Œä½ å¿…é¡»è¿›è¡Œé€è¡Œé€åˆ—çš„æ·±åº¦æ¨ç†ï¼š

ç¬¬ä¸€å±‚æ¨ç†ï¼šé€è¡Œæ•°æ®å¤„ç†
- è¯†åˆ«å½“å‰å¤„ç†çš„æ•°æ®è¡Œ
- ç¡®å®šè¯¥è¡Œæ•°æ®çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
- æ˜ç¡®è¯¥è¡Œæ•°æ®åœ¨æ ¸å¿ƒæ•°æ®æºä¸­çš„ä½ç½®å’Œä¸Šä¸‹æ–‡

ç¬¬äºŒå±‚æ¨ç†ï¼šé€åˆ—å­—æ®µæ¨ç†
å¯¹äºæ¯ä¸ªç›®æ ‡å­—æ®µï¼Œè¿›è¡Œä»¥ä¸‹é“¾å¼æ¨ç†ï¼š
1. å­—æ®µå®šä¹‰ç†è§£ï¼šè¡¨å¤´æ˜ å°„ä¸­è¿™ä¸ªå­—æ®µè¦æ±‚ä»€ä¹ˆç±»å‹çš„æ•°æ®ï¼Ÿ
2. æ•°æ®æºæœç´¢ï¼šåœ¨æ‰€æœ‰æ•°æ®æºä¸­æŸ¥æ‰¾èƒ½æ»¡è¶³è¯¥å­—æ®µè¦æ±‚çš„ä¿¡æ¯
3. æ•°æ®åŒ¹é…éªŒè¯ï¼šæ‰¾åˆ°çš„æ•°æ®æ˜¯å¦ç¬¦åˆè¡¨å¤´æ˜ å°„ä¸­çš„å­—æ®µå®šä¹‰ï¼Ÿ
4. æ•°æ®ä¼˜å…ˆçº§é€‰æ‹©ï¼šå¦‚æœå¤šä¸ªæ•°æ®æºéƒ½æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¦‚ä½•é€‰æ‹©æœ€åˆé€‚çš„ï¼Ÿ
5. æ•°æ®è½¬æ¢å¤„ç†ï¼šéœ€è¦è¿›è¡Œä»€ä¹ˆæ ¼å¼è½¬æ¢æˆ–è®¡ç®—ä»¥åŒ¹é…è¡¨å¤´è¦æ±‚ï¼Ÿ
6. åˆç†æ€§éªŒè¯ï¼šæœ€ç»ˆç¡®å®šçš„æ•°æ®æ˜¯å¦é€»è¾‘åˆç†ä¸”ç¬¦åˆè¡¨å¤´æ˜ å°„ï¼Ÿ

ã€æ¨ç†æ­¥éª¤ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œå¹¶å±•ç¤ºæ¯ä¸€æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼š

æ­¥éª¤1ï¼šæ•°æ®æºå…¨é¢åˆ†æ
- è¯†åˆ«æ ¸å¿ƒæ•°æ®æºçš„æ¡ç›®æ•°é‡ï¼Œç¡®å®šéœ€è¦ç”Ÿæˆçš„CSVè¡Œæ•°
- è¯†åˆ«å‚è€ƒæ•°æ®æºå’Œè¡¥å……ä¿¡æ¯çš„å†…å®¹å’Œç»“æ„
- ç†è§£å„æ•°æ®æºä¸­å¯ç”¨äºå¡«å……è¡¨å¤´å­—æ®µçš„ä¿¡æ¯
- é¢„ä¼°æ•°æ®å¤„ç†çš„å¤æ‚åº¦å’Œæ½œåœ¨é—®é¢˜

æ­¥éª¤2ï¼šæ˜ å°„è§„åˆ™æ·±åº¦è§£æ
- é€ä¸€åˆ†ææ¯ä¸ªç›®æ ‡å­—æ®µçš„å®šä¹‰å’Œè¦æ±‚
- æ˜ç¡®æ¯ä¸ªå­—æ®µéœ€è¦ä»€ä¹ˆç±»å‹çš„æ•°æ®å’Œæ ¼å¼
- ç¡®å®šå“ªäº›å­—æ®µå¯ä»¥ä»å“ªäº›æ•°æ®æºè·å–ä¿¡æ¯
- è¯†åˆ«éœ€è¦è®¡ç®—ã€æ¨ç†æˆ–æ ¼å¼è½¬æ¢çš„å­—æ®µ

æ­¥éª¤3ï¼šé€è¡Œæ•°æ®å¤„ç†ï¼ˆChain of Thoughtï¼‰
å¯¹äºæ¯ä¸€è¡Œæ•°æ®ï¼Œè¿›è¡Œä»¥ä¸‹è¯¦ç»†æ¨ç†ï¼š

ã€æ•°æ®è¡Œ X çš„å¤„ç†ã€‘
â†’ è¡Œæ•°æ®è¯†åˆ«ï¼šå½“å‰å¤„ç†çš„æ˜¯ç¬¬Xè¡Œï¼Œå¯¹åº”æ ¸å¿ƒæ•°æ®æºä¸­çš„ç¬¬Xä¸ªæ¡ç›®
â†’ é€åˆ—å­—æ®µæ¨ç†ï¼š
  â”œâ”€â”€ å­—æ®µ1ï¼š[å­—æ®µåç§°]
  â”‚   â”œâ”€â”€ å®šä¹‰ç†è§£ï¼šè¡¨å¤´æ˜ å°„ä¸­è¯¥å­—æ®µè¦æ±‚ä»€ä¹ˆç±»å‹çš„æ•°æ®ï¼Ÿ
  â”‚   â”œâ”€â”€ æ•°æ®æºæœç´¢ï¼šåœ¨æ‰€æœ‰æ•°æ®æºä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ [æœç´¢ç»“æœ]
  â”‚   â”œâ”€â”€ æ•°æ®éªŒè¯ï¼šæ‰¾åˆ°çš„æ•°æ®æ˜¯å¦ç¬¦åˆè¡¨å¤´æ˜ å°„è¦æ±‚ï¼Ÿ
  â”‚   â”œâ”€â”€ ä¼˜å…ˆçº§é€‰æ‹©ï¼šï¼ˆå¦‚æœ‰å¤šä¸ªæ¥æºï¼‰é€‰æ‹©æœ€åˆé€‚çš„æ•°æ®æº
  â”‚   â”œâ”€â”€ è½¬æ¢å¤„ç†ï¼šéœ€è¦è¿›è¡Œ [å…·ä½“è½¬æ¢] ä»¥åŒ¹é…è¡¨å¤´è¦æ±‚
  â”‚   â””â”€â”€ æœ€ç»ˆç¡®å®šï¼šè¯¥å­—æ®µçš„å€¼ä¸º [æœ€ç»ˆå€¼]
  â”œâ”€â”€ å­—æ®µ2ï¼š[å­—æ®µåç§°]
  â”‚   â”œâ”€â”€ å®šä¹‰ç†è§£ï¼šè¡¨å¤´æ˜ å°„ä¸­è¯¥å­—æ®µè¦æ±‚ä»€ä¹ˆç±»å‹çš„æ•°æ®ï¼Ÿ
  â”‚   â”œâ”€â”€ æ•°æ®æºæœç´¢ï¼šåœ¨æ‰€æœ‰æ•°æ®æºä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ [æœç´¢ç»“æœ]
  â”‚   â””â”€â”€ ... ï¼ˆé‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼‰
  â””â”€â”€ å­—æ®µNï¼š[å­—æ®µåç§°]
      â””â”€â”€ ... ï¼ˆé‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼‰
â†’ è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼šè¯¥è¡Œæ˜¯å¦åŒ…å«æ‰€æœ‰è¡¨å¤´æ˜ å°„è¦æ±‚çš„å­—æ®µï¼Ÿ
â†’ è¡Œæ•°æ®ä¸€è‡´æ€§éªŒè¯ï¼šå„å­—æ®µé—´æ˜¯å¦é€»è¾‘ä¸€è‡´ä¸”ç¬¦åˆä¸šåŠ¡è§„åˆ™ï¼Ÿ

æ­¥éª¤4ï¼šæ•°æ®è´¨é‡å…¨é¢éªŒè¯
- éªŒè¯æ¯ä¸ªå­—æ®µçš„åˆç†æ€§å’Œå‡†ç¡®æ€§
- æ£€æŸ¥ä¸æ ¸å¿ƒæ•°æ®æºçš„ä¸€è‡´æ€§
- ç¡®è®¤å­—æ®µé¡ºåºå’Œæ ¼å¼æ­£ç¡®
- è¿›è¡Œè·¨è¡Œæ•°æ®çš„ä¸€è‡´æ€§æ£€æŸ¥

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

  === æ¨ç†è¿‡ç¨‹ ===
  [è¯¦ç»†å±•ç¤ºä½ çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹ï¼Œå¿…é¡»åŒ…æ‹¬ï¼š
  - æ•°æ®æºå…¨é¢åˆ†æï¼ˆæ ¸å¿ƒæ•°æ®æºè¡Œæ•°ç¡®å®šï¼Œå„æ•°æ®æºå¯ç”¨ä¿¡æ¯ï¼‰
  - æ˜ å°„è§„åˆ™æ·±åº¦è§£æï¼ˆæ¯ä¸ªè¡¨å¤´å­—æ®µçš„å…·ä½“è¦æ±‚ï¼‰
  - é€è¡Œé€åˆ—çš„Chain of Thoughtæ¨ç†è¿‡ç¨‹
  - æ¯ä¸ªå­—æ®µçš„6æ­¥è¯¦ç»†æ¨ç†é“¾ï¼ˆä¸¥æ ¼æŒ‰ç…§è¡¨å¤´æ˜ å°„è¦æ±‚ï¼‰
  - æ•°æ®è´¨é‡éªŒè¯ç»“æœ]

=== æœ€ç»ˆç­”æ¡ˆ ===
[ä»…è¾“å‡ºçº¯å‡€çš„ CSV æ•°æ®è¡Œï¼Œä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”]

  ã€è´¨é‡è¦æ±‚ã€‘
  âœ… æ¨ç†è¿‡ç¨‹å¿…é¡»å±•ç¤ºæ¯ä¸€è¡Œæ¯ä¸€åˆ—çš„è¯¦ç»†æ€è€ƒé“¾è·¯
  âœ… ç”Ÿæˆçš„CSVè¡Œæ•°å¿…é¡»ä¸æ ¸å¿ƒæ•°æ®æºæ¡ç›®æ•°é‡ä¸¥æ ¼å¯¹åº”
  âœ… æ¯ä¸ªå­—æ®µçš„å†…å®¹å¿…é¡»ä¸¥æ ¼ç¬¦åˆè¡¨å¤´æ˜ å°„ä¸­çš„å®šä¹‰å’Œè¦æ±‚
  âœ… å¯ä»¥ä»æ‰€æœ‰æ•°æ®æºä¸­è·å–ä¿¡æ¯ï¼Œä½†å¿…é¡»æœåŠ¡äºè¡¨å¤´æ˜ å°„çš„è¦æ±‚
  âœ… æœ€ç»ˆç­”æ¡ˆä»…åŒ…å«CSVæ•°æ®ï¼Œä¸å«ä»»ä½•å…¶ä»–å†…å®¹
  âœ… å­—æ®µé¡ºåºå¿…é¡»ä¸æ¨¡æ¿è¡¨å¤´æ˜ å°„å®Œå…¨ä¸€è‡´
  âœ… ä¸¥ç¦é—æ¼å­—æ®µã€é‡å¤å­—æ®µæˆ–è¾“å‡ºç©ºå€¼
  âœ… æ¯ä¸ªå­—æ®µçš„æ¨ç†è¿‡ç¨‹éƒ½å¿…é¡»æ¸…æ™°å¯è¿½æº¯ä¸”åŸºäºè¡¨å¤´æ˜ å°„

æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼š
{state["headers_mapping"]}
"""

            print("ğŸ“‹ ç³»ç»Ÿæç¤ºå‡†å¤‡å®Œæˆ")
            print("ç³»ç»Ÿæç¤ºè¯ï¼š", system_prompt)
            
            def process_single_chunk(chunk_data):
                """å¤„ç†å•ä¸ªchunkçš„å‡½æ•°"""
                chunk, index = chunk_data
                try:
                    user_input = f"""
                    æ•°æ®çº§ï¼š
                    {chunk}
                    """             
                    print("ç”¨æˆ·è¾“å…¥æç¤ºè¯", system_prompt)
                    print(f"ğŸ¤– Processing chunk {index + 1}/{len(state['combined_data_array'])}...")
                    response = invoke_model(
                        model_name="deepseek-ai/DeepSeek-V3", 
                        messages=[SystemMessage(content=system_prompt), HumanMessage(content=user_input)],
                        temperature=0.2, silent_mode=True
                    )
                    print(f"âœ… Completed chunk {index + 1}")
                    return (index, response)
                except Exception as e:
                    print(f"âŒ Error processing chunk {index + 1}: {e}")
                    return (index, f"Error processing chunk {index + 1}: {e}")
            
            # Prepare chunk data with indices
            chunks_with_indices = [(chunk, i) for i, chunk in enumerate(state["combined_data_array"])]
            
            if not chunks_with_indices:
                print("âš ï¸ æ²¡æœ‰æ•°æ®å—éœ€è¦å¤„ç†")
                print("âœ… _generate_CSV_based_on_combined_data æ‰§è¡Œå®Œæˆ(æ— æ•°æ®)")
                print("=" * 50)
                return {"CSV_data": []}
            
            # Dynamically adjust max_workers based on actual data size
            max_workers = min(15, len(chunks_with_indices))  # Use fewer workers if we have less data
            print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(chunks_with_indices)} ä¸ªæ•°æ®å—...")
            print(f"ğŸ‘¥ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘å·¥ä½œè€…")
            
            # Use ThreadPoolExecutor for concurrent processing
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            results = {}
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all tasks
                future_to_index = {executor.submit(process_single_chunk, chunk_data): chunk_data[1] 
                                for chunk_data in chunks_with_indices}
                print(f"âœ… å·²æäº¤ {len(future_to_index)} ä¸ªå¹¶å‘ä»»åŠ¡")
                
                # Collect results as they complete
                completed_count = 0
                for future in as_completed(future_to_index):
                    try:
                        index, response = future.result()
                        results[index] = response
                        completed_count += 1
                        print(f"âœ… å®Œæˆç¬¬ {completed_count}/{len(chunks_with_indices)} ä¸ªä»»åŠ¡")
                    except Exception as e:
                        index = future_to_index[future]
                        print(f"âŒ ç¬¬ {index + 1} ä¸ªæ•°æ®å—å¤„ç†å¼‚å¸¸: {e}")
                        results[index] = f"æ•°æ®å— {index + 1} å¤„ç†å¼‚å¸¸: {e}"
            
            # Sort results by index to maintain order
            sorted_results = [results[i] for i in sorted(results.keys())]
            
            print(f"ğŸ‰ æˆåŠŸå¹¶å‘å¤„ç† {len(sorted_results)} ä¸ªæ•°æ®å—")
            
            # Save CSV data to output folder using helper function
            try:
                from utils.file_process import save_csv_to_output
                saved_file_path = save_csv_to_output(sorted_results, state["session_id"])
                print(f"âœ… CSVæ•°æ®å·²ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶å¤¹: {saved_file_path}")
            except Exception as e:
                print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                print("âš ï¸ æ•°æ®ä»ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¯ç»§ç»­å¤„ç†")
            
            print("âœ… _generate_CSV_based_on_combined_data æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            # print(f"ğŸ” ç”Ÿæˆçš„CSVæ•°æ®: {sorted_results}")
            return {
                "CSV_data": sorted_results
            }
        
        else:
            return state
    
        
    def _extract_empty_row_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–æ¨¡æ¿è¡¨æ ¼ä¸­çš„ç©ºè¡Œhtmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            empty_row_html = extract_empty_row_html_code_based(state["template_file"])
            print("empty_row_html", empty_row_html)
            return {"empty_row_html": empty_row_html}
        except Exception as e:
            print(f"âŒ _extract_empty_row_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"empty_row_html": ""}

    def _extract_headers_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–å‡ºhtmlæ¨¡æ¿è¡¨æ ¼çš„è¡¨å¤´htmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            headers_html = extract_headers_html_code_based(state["template_file"])
            print("headers_html", headers_html)
            return {"headers_html": headers_html}
        except Exception as e:
            print(f"âŒ _extract_headers_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"headers_html": ""}

    def _extract_footer_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–å‡ºhtmlæ¨¡æ¿è¡¨æ ¼çš„ç»“å°¾htmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            footer_html = extract_footer_html_code_based(state["template_file"])
            print("footer_html", footer_html)
            return {"footer_html": footer_html}
        except Exception as e:
            print(f"âŒ _extract_footer_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"footer_html": ""}

    def _transform_data_to_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """å°†æ•°æ®è½¬æ¢ä¸ºhtmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            # Read CSV data file path
            csv_file_path = f"conversations/{state['session_id']}/CSV_files/synthesized_table_with_only_data.csv"
            
            # Get empty row HTML template from state
            empty_row_html = state.get("empty_row_html", "")
            if not empty_row_html:
                print("âš ï¸ æœªæ‰¾åˆ°ç©ºè¡ŒHTMLæ¨¡æ¿")
                return {"filled_row": ""}
            
            # Use the utility function to transform data
            filled_row_html = transform_data_to_html_code_based(
                csv_file_path=csv_file_path,
                empty_row_html=empty_row_html,
                session_id=state["session_id"],
                template_file_path=state["template_file"]
            )
            
            return {"filled_row": filled_row_html}
            
        except Exception as e:
            print(f"âŒ _transform_data_to_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {"filled_row": ""}
    
    def _combine_html_tables(self, state: FilloutTableState) -> FilloutTableState:
        """å°†è¡¨å¤´ï¼Œæ•°æ®ï¼Œè¡¨å°¾htmlæ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶æ·»åŠ å…¨å±€ç¾åŒ–æ ·å¼"""
        try:
            # è·å–å„éƒ¨åˆ†HTML
            headers_html = state.get("headers_html", "")
            data_html = state.get("filled_row", "")
            footer_html = state.get("footer_html", "")
            
            # Use the utility function to combine HTML parts
            combined_html = combine_html_parts(
                headers_html=headers_html,
                data_html=data_html,
                footer_html=footer_html
            )
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_path = f"conversations/{state['session_id']}/output/combined_html.html"
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as file:
                file.write(combined_html)
            
            print(f"âœ… ç¾åŒ–è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_path}")
            
            return {"combined_html": combined_html}
        except Exception as e:
            print(f"âŒ _combine_html_tables æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {"combined_html": ""}
    
    def _shield_for_transform_data_to_html(self, state: FilloutTableState) -> FilloutTableState:
        """Shield node for transform_data_to_html"""
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _shield_for_transform_data_to_html")
        print("=" * 50)
        
        try:
            # Ensure all required components are available
            if not state["CSV_data"] or not state["empty_row_html"] or not state["headers_html"] or not state["footer_html"]:
                print("âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶ï¼Œæ— æ³•è½¬æ¢ä¸ºHTML")
                return state
            
            print("âœ… _shield_for_transform_data_to_html æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return state
        
        except Exception as e:
            print(f"âŒ _shield_for_transform_data_to_html æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return state
    
    def run_fillout_table_agent(self, session_id: str,
                                template_file: str,
                                data_file_path: list[str],
                                headers_mapping: dict[str, str],
                                modify_after_first_fillout: bool = False,
                                village_name: str = ""
                                ) -> None:
        """This function will run the fillout table agent using invoke method with manual debug printing"""
        print("\nğŸš€ å¯åŠ¨ FilloutTableAgent")
        print("=" * 60)
        print("æ¨¡æ¿æ–‡ä»¶ï¼š", template_file)
        
        initial_state = self.create_initialize_state(
            session_id = session_id,
            template_file = template_file,
            data_file_path = data_file_path,
            headers_mapping=headers_mapping,
            modify_after_first_fillout=modify_after_first_fillout,
            village_name=village_name
        )

        config = {"configurable": {"thread_id": session_id}}
        
        print(f"ğŸ“‹ åˆå§‹çŠ¶æ€åˆ›å»ºå®Œæˆï¼Œä¼šè¯ID: {session_id}")
        print(f"ğŸ“„ æ¨¡æ¿æ–‡ä»¶: {initial_state['template_file']}")
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶æ•°é‡: {len(initial_state['data_file_path'])}")

        print("-" * 60)

        while True:
            try:
                print(f"\nğŸ”„ æ‰§è¡ŒçŠ¶æ€å›¾ï¼Œå½“å‰ä¼šè¯ID: {session_id}")
                print("-" * 50)
                
                final_state = self.graph.invoke(initial_state, config=config)
                
                if "__interrupt__" in final_state:
                    interrupt_value = final_state["__interrupt__"][0].value
                    print(f"ğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                    user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                    initial_state = Command(resume=user_response)
                    continue
                
                print("\nâœ… FilloutTableAgentæ‰§è¡Œå®Œæ¯•")
                print("=" * 60)
                
                # Print final results
                if "filled_row" in final_state and final_state["filled_row"]:
                    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ç”Ÿæˆ")
                    if len(str(final_state["filled_row"])) > 500:
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(str(final_state['filled_row']))} å­—ç¬¦")
                    else:
                        print(f"ğŸ“„ å†…å®¹: {final_state['filled_row']}")
                        
                if "messages" in final_state and final_state["messages"]:
                    latest_message = final_state["messages"][-1]
                    if hasattr(latest_message, 'content'):
                        print(f"ğŸ’¬ æœ€ç»ˆæ¶ˆæ¯: {latest_message.content}")
                        
                break
                
            except Exception as e:
                print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                print("-" * 50)
                break
    


if __name__ == "__main__":
    # fillout_table_agent = FilloutTableAgent()
    # fillout_table_agent.run_fillout_table_agent( session_id = "1")
    # file_content = retrieve_file_content(session_id= "1", file_paths = [r"D:\asianInfo\ExcelAssist\ç‡•äº‘æ‘æµ‹è¯•æ ·ä¾‹\ç‡•äº‘æ‘æ®‹ç–¾äººè¡¥è´´\å¾…å¡«è¡¨\ç‡•äº‘æ‘æ®‹ç–¾äººè¡¥è´´ç”³é¢†ç™»è®°.xlsx"])

    # file_list = [r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\7.2æ¥é¾™é•‡é™„ä»¶4.xlsx", r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\10.24æ¥é¾™é•‡é™„ä»¶4ï¼šè„±è´«äººå£å°é¢è´·æ¬¾è´´æ¯å‘æ”¾æ˜ç»†è¡¨.xlsx", r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\12.3é™„ä»¶4ï¼šè„±è´«äººå£å°é¢è´·æ¬¾è´´æ¯ç”³æŠ¥æ±‡æ€»è¡¨.xlsx"]
    # fillout_table_agent = FilloutTableAgent()
    # combined_data = fillout_table_agent._combine_data_split_into_chunks(file_list)
    # print(combined_data)
    fillout_table_agent = FilloutTableAgent()
    fillout_table_agent.run_fillout_table_agent(session_id = "1",
                                                template_file = r"conversations\1\user_uploaded_files\template\ä¸ƒç”°æ‘_è¡¨æ ¼æ¨¡æ¿_20250721_161945.txt",
                                                data_file_path = ['åŸä¿åå†Œ.xls', 'å†œä¿åå†Œ.xls'],
                                                headers_mapping={
  "è¡¨æ ¼æ ‡é¢˜": "ä¸ƒç”°æ‘ä½ä¿è¡¥è´´æ±‡æ€»è¡¨",
  "è¡¨æ ¼ç»“æ„": {
    "åŸºæœ¬ä¿¡æ¯": [
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: åºå·",
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: æˆ·ä¸»å§“å",
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: èº«ä»½è¯å·ç ",
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: ä½ä¿è¯å·",
      "æ¨ç†è§„åˆ™: å±…æ°‘ç±»å‹(åŸä¿/å†œä¿) - æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨åˆ¤æ–­ï¼ŒåŸä¿åå†Œ.xlså¯¹åº”'åŸä¿'ï¼Œå†œä¿åå†Œ.xlså¯¹åº”'å†œä¿'"
    ],
    "ä¿éšœæƒ…å†µ": {
      "ä¿éšœäººæ•°": [
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: ä¿éšœäººæ•°.åˆ†è§£.é‡ç‚¹ä¿éšœäººæ•°",
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: ä¿éšœäººæ•°.åˆ†è§£.æ®‹ç–¾äººæ•°"
      ],
      "é¢†å–é‡‘é¢": [
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†å–é‡‘é¢.åˆ†è§£.å®¶åº­è¡¥å·®",
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†å–é‡‘é¢.åˆ†è§£.é‡ç‚¹æ•‘åŠ©60å…ƒ",
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†å–é‡‘é¢.åˆ†è§£.é‡ç‚¹æ•‘åŠ©100å…ƒ",
        "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†å–é‡‘é¢.åˆ†è§£.æ®‹ç–¾äººæ•‘åŠ©"
      ]
    },
    "é¢†å–ä¿¡æ¯": [
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†æ¬¾äººç­¾å­—(ç« )",
      "åŸä¿åå†Œ.xls/å†œä¿åå†Œ.xls: é¢†æ¬¾æ—¶é—´"
    ]
  }
})