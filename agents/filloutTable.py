import sys
from pathlib import Path
import io
import contextlib

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, filter_out_system_messages
from utilities.file_process import (read_txt_file, 
                                    process_excel_files_with_chunking)
from utilities.modelRelated import invoke_model
from utilities.html_generator import (
    extract_empty_row_html_code_based,
    extract_headers_html_code_based,
    extract_footer_html_code_based,
    transform_data_to_html_code_based,
    combine_html_parts
)
import uuid
import json
import os
import pandas as pd
from bs4 import BeautifulSoup
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re
import csv

from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.constants import Send
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


# import other agents
from agents.processUserInput import ProcessUserInputAgent

load_dotenv()

class FilloutTableState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    data_file_path: list[str]
    supplement_files_summary: str
    template_file: str
    template_file_completion_code: str
    fill_CSV_2_template_code: str
    combined_data: str
    filled_row: str
    error_message: str
    error_message_summary: str
    template_completion_code_execution_successful: bool
    CSV2Teplate_template_completion_code_execution_successful: bool
    retry: int
    combined_data_array: list[str]
    headers_mapping: str
    largest_file_row_num: int
    combined_html: str
    # Use lambda reducers for concurrent updates
    empty_row_html: Annotated[str, lambda old, new: new if new else old]
    headers_html: Annotated[str, lambda old, new: new if new else old]
    footer_html: Annotated[str, lambda old, new: new if new else old]
    CSV_data: Annotated[list[str], lambda old, new: new if new else old]



class FilloutTableAgent:
    def __init__(self):
        self.graph = self._build_graph()
        



    def _build_graph(self):
        """Build the LangGraph workflow for filling out tables"""
        graph = StateGraph(FilloutTableState)
        
        # Add nodes
        graph.add_node("combine_data_split_into_chunks", self._combine_data_split_into_chunks)
        graph.add_node("generate_CSV_based_on_combined_data", self._generate_CSV_based_on_combined_data)
        graph.add_node("transform_data_to_html", self._transform_data_to_html_code_based)  # Use code-based function
        graph.add_node("extract_empty_row_html", self._extract_empty_row_html_code_based)
        graph.add_node("extract_headers_html", self._extract_headers_html_code_based)
        graph.add_node("extract_footer_html", self._extract_footer_html_code_based)
        graph.add_node("combine_html_tables", self._combine_html_tables)
        graph.add_node("shield_for_transform_data_to_html", self._shield_for_transform_data_to_html)
        
        # Define the workflow
        graph.add_edge(START, "combine_data_split_into_chunks")
        graph.add_conditional_edges("combine_data_split_into_chunks", self._route_after_combine_data_split_into_chunks)
        graph.add_edge("extract_empty_row_html", "shield_for_transform_data_to_html")
        graph.add_edge("extract_headers_html", "shield_for_transform_data_to_html")
        graph.add_edge("extract_footer_html", "shield_for_transform_data_to_html")
        graph.add_edge("generate_CSV_based_on_combined_data", "shield_for_transform_data_to_html")
        graph.add_edge("shield_for_transform_data_to_html", "transform_data_to_html")
        graph.add_edge("transform_data_to_html", "combine_html_tables")
        graph.add_edge("combine_html_tables", END)
        

        
        # Compile the graph
        return graph.compile()

    
    def create_initialize_state(self, session_id: str,
                                 template_file: str = None,
                                 data_file_path: list[str] = None,
                                 headers_mapping: dict[str, str] = None,
                                 supplement_files_summary: str = "") -> FilloutTableState:
        """This node will initialize the state of the graph"""
        return {
            "messages": [],
            "session_id": session_id,
            "data_file_path": data_file_path, # excel files(xls) that has raw data
            "template_file": template_file, # txt file of template file in html format
            "template_file_completion_code": "",
            "fill_CSV_2_template_code": "",
            "combined_data": "",
            "filled_row": "",
            "error_message": "",
            "error_message_summary": "",
            "template_completion_code_execution_successful": False,
            "CSV2Teplate_template_completion_code_execution_successful": False,
            "retry": 0,
            "combined_data_array": [],
            "headers_mapping": headers_mapping,
            "CSV_data": [],
            "largest_file_row_num": 66,
            "supplement_files_summary": supplement_files_summary,
            "empty_row_html": "",
            "headers_html": "",
            "footer_html": "",
            "combined_html": ""
            
        }
    
    def _combine_data_split_into_chunks(self, state: FilloutTableState) -> FilloutTableState:
        """æ•´åˆæ‰€æœ‰éœ€è¦ç”¨åˆ°çš„æ•°æ®ï¼Œå¹¶ç”Ÿå°†å…¶åˆ†æ‰¹ï¼Œç”¨äºåˆ†æ‰¹ç”Ÿæˆè¡¨æ ¼"""
        # return
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _combine_data_split_into_chunks")
        print("=" * 50)
        
        try:
            # Get Excel file paths from state
            excel_file_paths = []
            print(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(state["data_file_path"])} ä¸ªæ•°æ®æ–‡ä»¶")
            
            # Convert data files to Excel paths if they're not already
            for file_path in state["data_file_path"]:
                print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
                if file_path.endswith('.txt'):
                    # Try to find corresponding Excel file
                    excel_path = file_path.replace('.txt', '.xlsx')
                    if Path(excel_path).exists():
                        excel_file_paths.append(excel_path)
                        print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                    else:
                        # Try .xls extension
                        excel_path = file_path.replace('.txt', '.xls')
                        if Path(excel_path).exists():
                            excel_file_paths.append(excel_path)
                            print(f"âœ… æ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {excel_path}")
                        else:
                            print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„Excelæ–‡ä»¶: {file_path}")
                elif file_path.endswith(('.xlsx', '.xls', '.xlsm')):
                    excel_file_paths.append(file_path)
                    print(f"âœ… ç›´æ¥ä½¿ç”¨Excelæ–‡ä»¶: {file_path}")
            
            if not excel_file_paths:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Excelæ–‡ä»¶")
                print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
                print("=" * 50)
                return {"combined_data_array": []}
            
            print(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(excel_file_paths)} ä¸ªExcelæ–‡ä»¶è¿›è¡Œåˆ†å—")
            

            print("ğŸ”„ æ­£åœ¨è°ƒç”¨process_excel_files_with_chunkingå‡½æ•°...")
            print("state['headers_mapping']çš„ç±»å‹: ", type(state["headers_mapping"]))
            chunked_result = process_excel_files_with_chunking(excel_file_paths=excel_file_paths, 
                                                             session_id=state["session_id"],
                                                             chunk_nums=15, largest_file=None,  # Let function auto-detect
                                                             data_json_path="agents/data.json")
            
            # Extract chunks and row count from the result
            chunked_data = chunked_result["combined_chunks"]
            largest_file_row_count = chunked_result["largest_file_row_count"]
            
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunked_data)} ä¸ªæ•°æ®å—")
            print(f"ğŸ“Š æœ€å¤§æ–‡ä»¶è¡Œæ•°: {largest_file_row_count}")
            for chunk in chunked_data:
                print(f"==================ğŸ” æ•°æ®å— ==================:")
                print(chunk)
            print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "combined_data_array": chunked_data,
                "largest_file_row_num": largest_file_row_count
            }
            
        except Exception as e:
            print(f"âŒ _combine_data_split_into_chunks æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            print("âœ… _combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ(é”™è¯¯)")
            print("=" * 50)
            return {
                "combined_data_array": []
            }

    def _route_after_combine_data_split_into_chunks(self, state: FilloutTableState) -> str:
        """å¹¶è¡Œæ‰§è¡Œæ¨¡æ¿ä»£ç çš„ç”Ÿæˆå’ŒCSVæ•°æ®çš„åˆæˆ"""
        print("\nğŸ”€ å¼€å§‹æ‰§è¡Œ: _route_after_combine_data_split_into_chunks")
        print("=" * 50)
        
        print("ğŸ”„ åˆ›å»ºå¹¶è¡Œä»»åŠ¡...")
        sends = []
        sends.append(Send("generate_CSV_based_on_combined_data", state))
        sends.append(Send("extract_empty_row_html", state))
        sends.append(Send("extract_headers_html", state))
        sends.append(Send("extract_footer_html", state)) 
        print("âœ… åˆ›å»ºäº†4ä¸ªå¹¶è¡Œä»»åŠ¡:")
        print("   - generate_CSV_based_on_combined_data")
        print("   - extract_empty_row_html")
        print("   - extract_headers_html")
        print("   - extract_footer_html")
    
        
        print("âœ… _route_after_combine_data_split_into_chunks æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return sends
    
    def _generate_CSV_based_on_combined_data(self, state: FilloutTableState) -> FilloutTableState:
        """æ ¹æ®æ•´åˆçš„æ•°æ®ï¼Œæ˜ å°„å…³ç³»ï¼Œæ¨¡æ¿ç”Ÿæˆæ–°çš„æ•°æ®"""
        # return state
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _generate_CSV_based_on_combined_data")
        print("=" * 50)
        
#         system_prompt = f"""
# ä½ æ˜¯ä¸€åä¸“ä¸šä¸”ä¸¥è°¨çš„ç»“æ„åŒ–æ•°æ®å¡«æŠ¥ä¸“å®¶ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œè®¡ç®—èƒ½åŠ›ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®åŸå§‹æ•°æ®å’Œæ¨¡æ¿æ˜ å°„è§„åˆ™ï¼Œå°†æ•°æ®å‡†ç¡®è½¬æ¢ä¸ºç›®æ ‡ CSV æ ¼å¼ï¼Œè¾“å‡ºç»“æ„åŒ–ã€å¹²å‡€çš„æ•°æ®è¡Œã€‚

# ã€è¾“å…¥å†…å®¹ã€‘
# 1. æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼ˆJSON æ ¼å¼ï¼‰ï¼šæè¿°ç›®æ ‡è¡¨æ ¼æ¯ä¸€åˆ—çš„æ¥æºã€è®¡ç®—é€»è¾‘æˆ–æ¨ç†è§„åˆ™ï¼›
# 2. åŸå§‹æ•°æ®é›†ï¼šåŒ…æ‹¬è¡¨å¤´ç»“æ„çš„ JSON å’Œ CSV æ•°æ®å—ï¼Œå…¶ä¸­æ¯æ¡æ•°æ®è¡Œå‰ä¸€è¡Œæ ‡æ³¨äº†å­—æ®µåç§°ï¼Œç”¨äºè¾…åŠ©å­—æ®µåŒ¹é…ã€‚

# ã€ä»»åŠ¡æµç¨‹ã€‘
# 1. è¯·ä½ é€å­—æ®µåˆ†ææ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼Œæ˜ç¡®è¯¥å­—æ®µçš„æ¥æºæˆ–æ¨ç†é€»è¾‘ï¼›
# 2. è‹¥å­—æ®µæ¥è‡ªåŸå§‹æ•°æ®ï¼Œè¯·å…ˆå®šä½æ¥æºå­—æ®µå¹¶æ ¡éªŒå…¶æ ¼å¼ï¼›
# 3. è‹¥å­—æ®µéœ€æ¨ç†ï¼ˆå¦‚æ—¥æœŸæ ¼å¼è½¬æ¢ã€å¹´é¾„è®¡ç®—ã€é€»è¾‘åˆ¤æ–­ç­‰ï¼‰ï¼Œè¯·å…ˆåœ¨è„‘ä¸­é€æ­¥æ¨å¯¼ï¼Œç¡®ä¿æ€è·¯æ¸…æ™°ï¼›
# 4. è‹¥å­—æ®µéœ€è®¡ç®—ï¼Œè¯·å…ˆæ˜ç¡®æ‰€éœ€å…¬å¼å¹¶é€æ­¥è®¡ç®—å‡ºç»“æœï¼›
# 5. åœ¨å®Œæˆæ‰€æœ‰å­—æ®µæ¨ç†åï¼Œå†å°†ç»“æœæŒ‰ç…§å­—æ®µé¡ºåºåˆå¹¶ä¸ºä¸€è¡Œ CSV æ•°æ®ï¼›
# 6. åœ¨æ¯æ¬¡è¾“å‡ºå‰ï¼Œè¯·å…ˆ**åœ¨è„‘ä¸­é€é¡¹éªŒè¯å­—æ®µæ˜¯å¦åˆç†ã€æ ¼å¼æ˜¯å¦è§„èŒƒ**ã€‚

# ğŸ’¡ è¯·ä½ åƒä¸€ä½äººç±»ä¸“å®¶ä¸€æ ·ï¼Œ**ä¸€æ­¥ä¸€æ­¥æ€è€ƒå†åšå†³å®š**ï¼Œä¸è¦è·³è¿‡ä»»ä½•é€»è¾‘è¿‡ç¨‹ã€‚

# ã€è¾“å‡ºè¦æ±‚ã€‘
# - ä»…è¾“å‡ºçº¯å‡€çš„ CSV æ•°æ®è¡Œï¼Œä¸åŒ…å«è¡¨å¤´ã€æ³¨é‡Šæˆ–ä»»ä½•å¤šä½™å†…å®¹ï¼›
# - ä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”å­—æ®µï¼›
# - æ¯è¡Œæ•°æ®å­—æ®µé¡ºåºå¿…é¡»ä¸æ¨¡æ¿è¡¨å¤´æ˜ å°„å®Œå…¨ä¸€è‡´ï¼›
# - ä¸¥ç¦é—æ¼å­—æ®µã€é‡å¤å­—æ®µã€å¤šè¾“å‡ºç©ºå€¼æˆ–ç©ºè¡Œï¼›
# - è¾“å‡ºä¸­ä¸å¾—å‡ºç° Markdown åŒ…è£¹ï¼ˆå¦‚ ```ï¼‰æˆ–é¢å¤–è¯´æ˜æ–‡å­—ã€‚

# æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼š
# {state["headers_mapping"]}
# """ 
        system_prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šä¸”ä¸¥è°¨çš„ç»“æ„åŒ–æ•°æ®å¡«æŠ¥ä¸“å®¶ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œè®¡ç®—èƒ½åŠ›ã€‚

è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥è§£å†³è¿™ä¸ªæ•°æ®è½¬æ¢é—®é¢˜ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
æ ¹æ®åŸå§‹æ•°æ®å’Œæ¨¡æ¿æ˜ å°„è§„åˆ™ï¼Œå°†æ•°æ®å‡†ç¡®è½¬æ¢ä¸ºç›®æ ‡ CSV æ ¼å¼ã€‚

ã€è¾“å…¥å†…å®¹ã€‘
1. æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼ˆJSON æ ¼å¼ï¼‰ï¼šæè¿°ç›®æ ‡è¡¨æ ¼æ¯ä¸€åˆ—çš„æ¥æºã€è®¡ç®—é€»è¾‘æˆ–æ¨ç†è§„åˆ™ï¼›
2. åŸå§‹æ•°æ®é›†ï¼šåŒ…æ‹¬è¡¨å¤´ç»“æ„çš„ JSON å’Œ CSV æ•°æ®å—ã€‚

ã€æ¨ç†æ­¥éª¤ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œå¹¶å±•ç¤ºæ¯ä¸€æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼š

æ­¥éª¤1ï¼šç†è§£æ˜ å°„è§„åˆ™
- é€ä¸€åˆ†ææ¯ä¸ªç›®æ ‡å­—æ®µçš„å®šä¹‰
- æ˜ç¡®æ•°æ®æ¥æºå’Œè½¬æ¢è§„åˆ™

æ­¥éª¤2ï¼šå®šä½åŸå§‹æ•°æ®
- åœ¨åŸå§‹æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”å­—æ®µ
- éªŒè¯æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§

æ­¥éª¤3ï¼šæ‰§è¡Œè½¬æ¢é€»è¾‘
- å¯¹äºè®¡ç®—å­—æ®µï¼šæ˜ç¡®å…¬å¼å¹¶é€æ­¥è®¡ç®—
- å¯¹äºæ¨ç†å­—æ®µï¼šå±•ç¤ºé€»è¾‘åˆ¤æ–­è¿‡ç¨‹
- å¯¹äºæ ¼å¼è½¬æ¢ï¼šè¯´æ˜è½¬æ¢è§„åˆ™

æ­¥éª¤4ï¼šè´¨é‡æ£€æŸ¥
- éªŒè¯æ¯ä¸ªå­—æ®µçš„åˆç†æ€§
- æ£€æŸ¥æ ¼å¼è§„èŒƒæ€§
- ç¡®è®¤å­—æ®µé¡ºåºæ­£ç¡®

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

=== æ¨ç†è¿‡ç¨‹ ===
[å±•ç¤ºä½ çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¯ä¸ªå­—æ®µçš„åˆ†æã€å®šä½ã€è½¬æ¢å’ŒéªŒè¯]

=== æœ€ç»ˆç­”æ¡ˆ ===
[ä»…è¾“å‡ºçº¯å‡€çš„ CSV æ•°æ®è¡Œï¼Œä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”]

ã€è´¨é‡è¦æ±‚ã€‘
- æ¨ç†è¿‡ç¨‹å¿…é¡»è¯¦ç»†å±•ç¤ºæ¯ä¸ªæ­¥éª¤çš„æ€è€ƒ
- æœ€ç»ˆç­”æ¡ˆä»…åŒ…å«CSVæ•°æ®ï¼Œä¸å«ä»»ä½•å…¶ä»–å†…å®¹
- å­—æ®µé¡ºåºå¿…é¡»ä¸æ¨¡æ¿è¡¨å¤´æ˜ å°„å®Œå…¨ä¸€è‡´
- ä¸¥ç¦é—æ¼å­—æ®µã€é‡å¤å­—æ®µæˆ–è¾“å‡ºç©ºå€¼

æ¨¡æ¿è¡¨å¤´æ˜ å°„ï¼š
{state["headers_mapping"]}
"""

        print("ğŸ“‹ ç³»ç»Ÿæç¤ºå‡†å¤‡å®Œæˆ")
        print("ç³»ç»Ÿæç¤ºè¯ï¼š", system_prompt)
        
        def process_single_chunk(chunk_data):
            """å¤„ç†å•ä¸ªchunkçš„å‡½æ•°"""
            chunk, index = chunk_data
            try:
                user_input = f"""
                æ•°æ®çº§ï¼š
                {chunk}
                """             
                print("ç”¨æˆ·è¾“å…¥æç¤ºè¯", system_prompt)
                print(f"ğŸ¤– Processing chunk {index + 1}/{len(state['combined_data_array'])}...")
                response = invoke_model(
                    model_name="deepseek-ai/DeepSeek-V3", 
                    messages=[SystemMessage(content=system_prompt), HumanMessage(content=user_input)],
                    temperature=0.2
                )
                print(f"âœ… Completed chunk {index + 1}")
                return (index, response)
            except Exception as e:
                print(f"âŒ Error processing chunk {index + 1}: {e}")
                return (index, f"Error processing chunk {index + 1}: {e}")
        
        # Prepare chunk data with indices
        chunks_with_indices = [(chunk, i) for i, chunk in enumerate(state["combined_data_array"])]
        
        if not chunks_with_indices:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å—éœ€è¦å¤„ç†")
            print("âœ… _generate_CSV_based_on_combined_data æ‰§è¡Œå®Œæˆ(æ— æ•°æ®)")
            print("=" * 50)
            return {"CSV_data": []}
        
        # Dynamically adjust max_workers based on actual data size
        max_workers = min(15, len(chunks_with_indices))  # Use fewer workers if we have less data
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(chunks_with_indices)} ä¸ªæ•°æ®å—...")
        print(f"ğŸ‘¥ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘å·¥ä½œè€…")
        
        # Use ThreadPoolExecutor for concurrent processing
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_index = {executor.submit(process_single_chunk, chunk_data): chunk_data[1] 
                              for chunk_data in chunks_with_indices}
            print(f"âœ… å·²æäº¤ {len(future_to_index)} ä¸ªå¹¶å‘ä»»åŠ¡")
            
            # Collect results as they complete
            completed_count = 0
            for future in as_completed(future_to_index):
                try:
                    index, response = future.result()
                    results[index] = response
                    completed_count += 1
                    print(f"âœ… å®Œæˆç¬¬ {completed_count}/{len(chunks_with_indices)} ä¸ªä»»åŠ¡")
                except Exception as e:
                    index = future_to_index[future]
                    print(f"âŒ ç¬¬ {index + 1} ä¸ªæ•°æ®å—å¤„ç†å¼‚å¸¸: {e}")
                    results[index] = f"æ•°æ®å— {index + 1} å¤„ç†å¼‚å¸¸: {e}"
        
        # Sort results by index to maintain order
        sorted_results = [results[i] for i in sorted(results.keys())]
        
        print(f"ğŸ‰ æˆåŠŸå¹¶å‘å¤„ç† {len(sorted_results)} ä¸ªæ•°æ®å—")
        
        # Save CSV data to output folder using helper function
        try:
            from utilities.file_process import save_csv_to_output
            saved_file_path = save_csv_to_output(sorted_results, state["session_id"])
            print(f"âœ… CSVæ•°æ®å·²ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶å¤¹: {saved_file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            print("âš ï¸ æ•°æ®ä»ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¯ç»§ç»­å¤„ç†")
        
        print("âœ… _generate_CSV_based_on_combined_data æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        # print(f"ğŸ” ç”Ÿæˆçš„CSVæ•°æ®: {sorted_results}")
        return {
            "CSV_data": sorted_results
        }
    
    def _extract_empty_row_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–æ¨¡æ¿è¡¨æ ¼ä¸­çš„ç©ºè¡Œhtmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            empty_row_html = extract_empty_row_html_code_based(state["template_file"])
            return {"empty_row_html": empty_row_html}
        except Exception as e:
            print(f"âŒ _extract_empty_row_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"empty_row_html": ""}

    def _extract_headers_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–å‡ºhtmlæ¨¡æ¿è¡¨æ ¼çš„è¡¨å¤´htmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            headers_html = extract_headers_html_code_based(state["template_file"])
            return {"headers_html": headers_html}
        except Exception as e:
            print(f"âŒ _extract_headers_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"headers_html": ""}

    def _extract_footer_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """æå–å‡ºhtmlæ¨¡æ¿è¡¨æ ¼çš„ç»“å°¾htmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            footer_html = extract_footer_html_code_based(state["template_file"])
            return {"footer_html": footer_html}
        except Exception as e:
            print(f"âŒ _extract_footer_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            return {"footer_html": ""}

    def _transform_data_to_html_code_based(self, state: FilloutTableState) -> FilloutTableState:
        """å°†æ•°æ®è½¬æ¢ä¸ºhtmlä»£ç  - åŸºäºä»£ç çš„é«˜æ•ˆå®ç°"""
        try:
            # Read CSV data file path
            csv_file_path = f"conversations/{state['session_id']}/CSV_files/synthesized_table_with_only_data.csv"
            
            # Get empty row HTML template from state
            empty_row_html = state.get("empty_row_html", "")
            if not empty_row_html:
                print("âš ï¸ æœªæ‰¾åˆ°ç©ºè¡ŒHTMLæ¨¡æ¿")
                return {"filled_row": ""}
            
            # Use the utility function to transform data
            filled_row_html = transform_data_to_html_code_based(
                csv_file_path=csv_file_path,
                empty_row_html=empty_row_html,
                session_id=state["session_id"]
            )
            
            return {"filled_row": filled_row_html}
            
        except Exception as e:
            print(f"âŒ _transform_data_to_html_code_based æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {"filled_row": ""}
    
    def _combine_html_tables(self, state: FilloutTableState) -> FilloutTableState:
        """å°†è¡¨å¤´ï¼Œæ•°æ®ï¼Œè¡¨å°¾htmlæ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶æ·»åŠ å…¨å±€ç¾åŒ–æ ·å¼"""
        try:
            # è·å–å„éƒ¨åˆ†HTML
            headers_html = state.get("headers_html", "")
            data_html = state.get("filled_row", "")
            footer_html = state.get("footer_html", "")
            
            # Use the utility function to combine HTML parts
            combined_html = combine_html_parts(
                headers_html=headers_html,
                data_html=data_html,
                footer_html=footer_html
            )
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_path = f"conversations/{state['session_id']}/output/combined_html.html"
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as file:
                file.write(combined_html)
            
            print(f"âœ… ç¾åŒ–è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_path}")
            
            return {"combined_html": combined_html}
        except Exception as e:
            print(f"âŒ _combine_html_tables æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {"combined_html": ""}
    
    def _shield_for_transform_data_to_html(self, state: FilloutTableState) -> FilloutTableState:
        """Shield node for transform_data_to_html"""
        print("\nğŸ”„ å¼€å§‹æ‰§è¡Œ: _shield_for_transform_data_to_html")
        print("=" * 50)
        
        try:
            # Ensure all required components are available
            if not state["CSV_data"] or not state["empty_row_html"] or not state["headers_html"] or not state["footer_html"]:
                print("âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶ï¼Œæ— æ³•è½¬æ¢ä¸ºHTML")
                return state
            
            print("âœ… _shield_for_transform_data_to_html æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return state
        
        except Exception as e:
            print(f"âŒ _shield_for_transform_data_to_html æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return state
    
    def run_fillout_table_agent(self, session_id: str,
                                template_file: str,
                                data_file_path: list[str],
                                headers_mapping: dict[str, str]
                                ) -> None:
        """This function will run the fillout table agent using invoke method with manual debug printing"""
        print("\nğŸš€ å¯åŠ¨ FilloutTableAgent")
        print("=" * 60)
        print("æ¨¡æ¿æ–‡ä»¶ï¼š", template_file)
        
        initial_state = self.create_initialize_state(
            session_id = session_id,
            template_file = template_file,
            data_file_path = data_file_path,
            headers_mapping=headers_mapping
        )

        config = {"configurable": {"thread_id": session_id}}
        
        print(f"ğŸ“‹ åˆå§‹çŠ¶æ€åˆ›å»ºå®Œæˆï¼Œä¼šè¯ID: {session_id}")
        print(f"ğŸ“„ æ¨¡æ¿æ–‡ä»¶: {initial_state['template_file']}")
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶æ•°é‡: {len(initial_state['data_file_path'])}")

        print("-" * 60)

        while True:
            try:
                print(f"\nğŸ”„ æ‰§è¡ŒçŠ¶æ€å›¾ï¼Œå½“å‰ä¼šè¯ID: {session_id}")
                print("-" * 50)
                
                final_state = self.graph.invoke(initial_state, config=config)
                
                if "__interrupt__" in final_state:
                    interrupt_value = final_state["__interrupt__"][0].value
                    print(f"ğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                    user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                    initial_state = Command(resume=user_response)
                    continue
                
                print("\nâœ… FilloutTableAgentæ‰§è¡Œå®Œæ¯•")
                print("=" * 60)
                
                # Print final results
                if "filled_row" in final_state and final_state["filled_row"]:
                    print(f"ğŸ“Š æœ€ç»ˆç»“æœå·²ç”Ÿæˆ")
                    if len(str(final_state["filled_row"])) > 500:
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(str(final_state['filled_row']))} å­—ç¬¦")
                    else:
                        print(f"ğŸ“„ å†…å®¹: {final_state['filled_row']}")
                        
                if "messages" in final_state and final_state["messages"]:
                    latest_message = final_state["messages"][-1]
                    if hasattr(latest_message, 'content'):
                        print(f"ğŸ’¬ æœ€ç»ˆæ¶ˆæ¯: {latest_message.content}")
                        
                break
                
            except Exception as e:
                print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                print("-" * 50)
                break
    


if __name__ == "__main__":
    # fillout_table_agent = FilloutTableAgent()
    # fillout_table_agent.run_fillout_table_agent( session_id = "1")
    # file_content = retrieve_file_content(session_id= "1", file_paths = [r"D:\asianInfo\ExcelAssist\ç‡•äº‘æ‘æµ‹è¯•æ ·ä¾‹\ç‡•äº‘æ‘æ®‹ç–¾äººè¡¥è´´\å¾…å¡«è¡¨\ç‡•äº‘æ‘æ®‹ç–¾äººè¡¥è´´ç”³é¢†ç™»è®°.xlsx"])

    # file_list = [r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\7.2æ¥é¾™é•‡é™„ä»¶4.xlsx", r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\10.24æ¥é¾™é•‡é™„ä»¶4ï¼šè„±è´«äººå£å°é¢è´·æ¬¾è´´æ¯å‘æ”¾æ˜ç»†è¡¨.xlsx", r"D:\asianInfo\æ•°æ®\æ–°æ§æ‘\12.3é™„ä»¶4ï¼šè„±è´«äººå£å°é¢è´·æ¬¾è´´æ¯ç”³æŠ¥æ±‡æ€»è¡¨.xlsx"]
    # fillout_table_agent = FilloutTableAgent()
    # combined_data = fillout_table_agent._combine_data_split_into_chunks(file_list)
    # print(combined_data)
    fillout_table_agent = FilloutTableAgent()
    fillout_table_agent.run_fillout_table_agent(session_id = "1",
                                                template_file = r"D:\asianInfo\ExcelAssist\conversations\1\user_uploaded_files\template\ç§æ¤é™©æŠ•ä¿æ¸…å•æ¨¡ç‰ˆ.txt",
                                                data_file_path = [r"D:\asianInfo\ExcelAssist\files\table_files\original\2024å¹´å†œä½œç‰©ç™»è®°.xlsx",
                                                                  r"D:\asianInfo\ExcelAssist\files\table_files\original\ç§æ¤æˆ·é“¶è¡Œå¡å·ç™»è®°.xlsx"],
                                                headers_mapping={
  "è¡¨æ ¼ç»“æ„": {
    "åºå·": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: åºå·"],
    "å§“å": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: å§“å"],
    "èº«ä»½è¯å·ç ": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: èº«ä»½è¯å·ç "],
    "ç”µè¯å·ç ": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: ç”µè¯å·ç "],
    "ç‰ç±³ï¼ˆäº©ï¼‰": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: ç‰ç±³ï¼ˆäº©ï¼‰"],
    "æ°´ç¨»ï¼ˆäº©ï¼‰": ["2024å¹´å†œä½œç‰©ç™»è®°.txt: æ°´ç¨»ï¼ˆäº©ï¼‰"],
    "å¼€æˆ·é“¶è¡Œ": ["ç§æ¤æˆ·é“¶è¡Œå¡å·ç™»è®°.txt: å¼€æˆ·é“¶è¡Œ"],
    "é“¶è¡Œè´¦å·": ["ç§æ¤æˆ·é“¶è¡Œå¡å·ç™»è®°.txt: é“¶è¡Œè´¦å·"],
    "æ˜¯å¦è„±è´«æˆ·": ["ç§æ¤æˆ·é“¶è¡Œå¡å·ç™»è®°.txt: æ˜¯å¦è„±è´«æˆ·"],
    "å¤‡æ³¨": ["æ¨ç†è§„åˆ™: æ ¹æ®ç‰ç±³æ°´ç¨»ç§æ¤ä¿é™©ç›¸å…³è¯´æ˜.txtä¸­çš„ä¿é™©é‡‘é¢å’Œèµ”å¿è®¡ç®—è§„åˆ™ï¼Œå¯ä»¥åœ¨æ­¤å­—æ®µå¤‡æ³¨ä¿é™©é‡‘é¢æˆ–èµ”å¿ç›¸å…³ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼šç‰ç±³æ¯äº©ä¿é™©é‡‘é¢ä¸º800å…ƒï¼Œæ°´ç¨»æ¯äº©ä¿é™©é‡‘é¢ä¸º1000å…ƒï¼›èµ”å¿é‡‘é¢=æ¯äº©ä¿é™©é‡‘é¢Ã—æŸå¤±é¢ç§¯Ã—æŸå¤±ç¨‹åº¦ï¼Œç´¯è®¡èµ”å¿ä¸è¶…è¿‡æ€»ä¿é™©é‡‘é¢ã€‚"]       
  },
  "è¡¨æ ¼æ€»ç»“": "è¯¥è¡¨æ ¼ä¸ºç‡•äº‘æ‘2024å¹´å†œä½œç‰©ä¿é™©æŠ•ä¿æ¸…å•ï¼Œç”¨äºè®°å½•å†œæˆ·æŠ•ä¿ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸ªäººåŸºæœ¬ä¿¡æ¯ã€æŠ•ä¿ä½œç‰©é¢ç§¯ã€é“¶è¡Œè´¦æˆ·ä¿¡æ¯åŠè„±è´«æˆ·æ ‡è¯†ç­‰ï¼Œé€‚ç”¨äºæ‘çº§å†œä½œç‰©ä¿é™©æŠ•ä¿ç®¡ç†ã€‚æ‰€æœ‰å­—æ®µå‡æ¥è‡ª2024å¹´å†œä½œç‰©ç™»è®°.txtå’Œç§æ¤æˆ·é“¶è¡Œå¡å·ç™»è®°.txtä¸¤ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå¤‡æ³¨å­—æ®µæ ¹æ®ç‰ç±³æ°´ç¨»ç§æ¤ä¿é™©ç›¸å…³è¯´æ˜.txtä¸­çš„ä¿é™©é‡‘é¢å’Œèµ”å¿è®¡ç®—è§„åˆ™è¿›è¡Œæ¨ç†å¡«å†™ã€‚"
})