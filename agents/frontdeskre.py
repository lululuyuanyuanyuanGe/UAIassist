import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, filter_out_system_messages
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content
from utilities.modelRelated import model_creation, detect_provider

import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


load_dotenv()


class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    messages_s: Annotated[list[BaseMessage], add_messages]
    table_structure: str
    upload_files_path: list[str]
    new_upload_files_path: list[str] # Track the new uploaded files
    upload_files_processed_path: list[str]
    new_upload_files_processed_path: list[str]
    upload_template: str # This variable will hold the actual content of the template
    previous_node: str # Track the previous node
    session_id: str
    


class FrontdeskAgent:
    """
    ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ¨¡æ¿ï¼Œè‹¥æœªæä¾›æ¨¡æ¿ï¼Œå’Œç”¨æˆ·æ²Ÿé€šç¡®å®šè¡¨æ ¼ç»“æ„
    """



    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.llm_c = model_creation(model_name=model_name, temperature=2) # complex logic use user selected model
        self.llm_s = model_creation(model_name="gpt-3.5-turbo", temperature=2) # simple logic use 3-5turbo



    def _build_graph(self) -> StateGraph:
        """This function will build the graph of the frontdesk agent"""

        graph = StateGraph(FrontdeskState)

        graph.add_node("entry", self._entry_node)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("route_after_collect_user_input", self._route_after_collect_user_input)
        graph.add_node("file_upload", self._file_upload)

        graph.add_edge(START, "entry")
        graph.add_edge("entry", "collect_user_input")
        graph.add_edge("collect_user_input", "route_after_collect_user_input")
        graph.add_edge("route_after_collect_user_input", "file_upload")
        graph.add_edge("file_upload", END)
        return graph



    def _entry_node(self, state: FrontdeskState) -> FrontdeskState:
        """This is the starting node of our frontdesk agent"""
        # Enrich this later, it should include a short description of the agent's ability and how to use it
        print("ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè¡¨æ ¼å¤„ç†åŠ©æ‰‹ï¼")
        # Here we will add a human in the loop to get user's response



    def _check_template(self, state: FrontdeskState) -> FrontdeskState:
        """This node will check if the user has provided a template"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¡«è¡¨åŠ©æ‰‹æ™ºèƒ½ä½“ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„è¾“å…¥æ¥å†³å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨ï¼Œå¦‚æœç”¨æˆ·æä¾›äº†æ¨¡æ¿ï¼Œ
        è¯·è¿”å›[YES]ï¼Œå¦åˆ™è¿”å›[NO]ï¼Œå¦å¤–ç”¨æˆ·å¯èƒ½ä¸Šä¼ æ–‡ä»¶"""
        # user turbo at here
        response = self.llm_s.invoke([SystemMessage(content=system_prompt)] + state["messages"][-1])
        return {"messages": response}
    


    def _route_after_check_template(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the user's input"""
        if state["messages"][-1].content == "[YES]":
            return "template_provided"
        else:
            return "no_template_provided"
        


    def _analyze_template(self, state: FrontdeskState) -> FrontdeskState:
        """This node will analyze the template to determine if it a complex template
        (both row, column headers) or a simple template (only column headers)"""
        system_prompt = """ä½ éœ€è¦æ ¹æ®htmlä»£ç åˆ¤æ–­è¿™ä¸ªæ¨¡æ¿æ˜¯å¤æ‚æ¨¡æ¿è¿˜æ˜¯ç®€å•æ¨¡æ¿ï¼Œåˆ¤æ–­è§„åˆ™ä¸ºï¼š
        1. å¦‚æœhtmlä»£ç ä¸­åŒ…å«rowå’Œcolumn headersï¼Œåˆ™è¿”å›[YES]
        2. å¦‚æœhtmlä»£ç ä¸­åªåŒ…å«column headersï¼Œåˆ™è¿”å›[NO]
        3. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        4. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        5. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        """
        # use 3-5turbo at here
    


    def _complex_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """This node will be use to analyze the complex table template, we will skip for now"""
        pass

    def _simple_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """This node will be use to analyze the simple table template, we"""
        pass







    def _collect_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """This is the node where we get user's input"""
        user_input = interrupt("ç”¨æˆ·ï¼š")
        return {"messages": user_input}



    def _route_after_collect_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """This node act as a safety check node, it will analyze the user's input and determine if it's a valid input,
        based on the LLM's previous response, at the same time it will route the agent to the correct node"""
        # We should let LLM decide the route
        
        user_upload_files = detect_and_process_file_paths(state["messages"][-1])
        # Filter out the new uploaded files
        new_upload_files = [item for item in user_upload_files if item not in state["upload_files_path"]]
        if new_upload_files:
            state["new_upload_files_path"] = new_upload_files
            state["upload_files_path"].extend(new_upload_files)
            return "file_upload"
        
        # User didn't upload new files
        elif not user_upload_files:
            system_prompt = """ä½ éœ€è¦åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆè¾“å…¥ï¼Œåˆ¤æ–­æ ‡å‡†ä¸º"""
            LLM_response_and_user_input = [state["messages"][-2], state["messages"][-1]]
            LLM_decision = self.llm_s.invoke([SystemMessage(content=system_prompt)] + LLM_response_and_user_input)
            # If it is a valid input we conitnue the normal execution flow, otherwise we will keep leting user 
            # input messages until it is a valid input
            if LLM_decision.content == "[YES]":
                return "valid_input"
            else:
                print(f"âŒ Invalid input: {state['messages'][-1].content}")
                return "invalid_input"
    


    def _uploaded_files(self, state: FrontdeskState) -> FrontdeskState:
        """This node will upload user's file to our system"""
        # For now we simply store the file content 
        result = retrieve_file_content(state["new_upload_files_path"], state["session_id"])
        state["new_upload_files_processed_path"] = result
        state["upload_files_processed_path"].extend(result)
        print(f"âœ… File uploaded: {state['upload_files_processed_path']}")
        return "check_template"
    


    def _analyze_uploaded_files_related_to_agent_task(self, state: FrontdeskState) -> FrontdeskState:
        """This node will analyze the uploaded files to determine if it's a valid file that is related
        to our agent's task in general, if it does we will keep the file, also we will summarize the file's content
        and store it as json format in the data.json file, basically it will append to the data.json file,
        it should contains the file;s name as the key, the value should be the description of the file's content
        and important information in the file, if it is not a related file we will remove delete this file from 
        our system."""
        
        import json
        import os
        from pathlib import Path
        
        # Load existing data.json or create empty dict
        data_json_path = Path("agents/data.json")
        try:
            if data_json_path.exists() and data_json_path.stat().st_size > 0:
                with open(data_json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
        except (json.JSONDecodeError, FileNotFoundError):
            existing_data = {}
            print("âš ï¸ åˆ›å»ºæ–°çš„ data.json æ–‡ä»¶")
        
        relevant_files = []
        irrelevant_files = []
        
        # Process each newly uploaded file
        for file_path in state.get("new_upload_files_processed_path", []):
            try:
                source_path = Path(file_path)
                if not source_path.exists():
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:2000] if len(file_content) > 2000 else file_content
                
                # Create analysis prompt
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡ä»¶åˆ†æåŠ©æ‰‹ã€‚ä½ éœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦ä¸è¡¨æ ¼ç”Ÿæˆã€æ•°æ®å¡«å†™ã€æ¨¡æ¿å¤„ç†ç­‰ä»»åŠ¡ç›¸å…³ã€‚
                ä¸ä»»åŠ¡ç›¸å…³çš„æ–‡ä»¶å°†ä¼šè¢«å­˜å‚¨èµ·æ¥ï¼Œåˆ¤æ–­æ ‡å‡†å¦‚ä¸‹
                
                è¡¨æ ¼æ–‡ä»¶å·²ç»å…¨éƒ¨è½¬æ¢ä¸ºhtmlä»£ç ï¼Œä½ éœ€è¦æ ¹æ®å†…å®¹åˆ¤æ–­è¿™ä¸ªæ–‡ä»¶æ˜¯å¦åŒ…å«å…·ä½“çš„æ•°æ®ï¼Œå¦‚æœåŒ…å«å…·ä½“æ•°æ®æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ª
                ç”¨æˆ·å¯¹æ•°æ®åº“çš„è¡¥å……ï¼Œå› æ­¤è¿™ä¸ªæ–‡ä»¶æ˜¯ç›¸å…³çš„ï¼Œå¦‚æœåªæœ‰è¡¨æ ¼ç»“æ„æ²¡æœ‰å…·ä½“æ•°æ®ï¼Œåˆ™æ„å‘³ç€è¿™ä¸ªæ–‡ä»¶æ˜¯è¡¨æ ¼çš„æ¨¡æ¿ã€‚å¦‚æœç”¨æˆ·
                ä¸Šä¼ çš„æ˜¯çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä½ éœ€è¦åˆ¤æ–­è¿™ä¸ªæ–‡æœ¬æ–‡ä»¶å’Œè¡¨æ ¼å¡«å†™æ˜¯å¦æœ‰å…³ï¼Œæœ‰äº›æ–‡æœ¬å¯èƒ½åŒ…å«å¡«å†™çš„è§„åˆ™ï¼Œæˆ–è€…æ³•å¾‹æ¡æ–‡ï¼Œæ”¿ç­–ä¿¡æ¯
                è¿™äº›éƒ½ä¼šè¾…åŠ©æˆ‘ä»¬ä»¥åå¯¹è¡¨æ ¼çš„å¡«å†™ï¼Œå› æ­¤è¿™ä¸ªæ–‡ä»¶æ˜¯ç›¸å…³çš„ã€‚
                
                æ–‡ä»¶å: {source_path.name}
                æ–‡ä»¶è·¯å¾„: {file_path}
                æ–‡ä»¶å†…å®¹é¢„è§ˆ:
                {analysis_content}
                
                è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
                ç›¸å…³æ€§: [YES/NO]
                æ˜¯å¦ä¸ºæ¨¡æ¿: [YES/NO]
                æ‘˜è¦: [æ–‡ä»¶å†…å®¹çš„ç®€è¦æè¿°ï¼Œé‡ç‚¹æè¿°å…¶ä¸­çš„é‡è¦ä¿¡æ¯ã€æ•°æ®ç»“æ„ã€è¡¨æ ¼ç»“æ„ç­‰ï¼Œåˆ—å‡ºæ‰€æœ‰è¡¨å¤´]
                é‡è¦ä¿¡æ¯: [æå–æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œæ³•å¾‹æ¡æ–‡ï¼Œæ”¿ç­–ä¿¡æ¯ï¼Œè¡¨æ ¼å¡«å†™è§„åˆ™]
                """
                
                # Get LLM analysis
                analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
                analysis_text = analysis_response.content
                
                # Parse LLM response
                is_relevant = False
                summary = ""
                important_info = ""
                
                if "[YES]" in analysis_text.upper() or "ç›¸å…³æ€§: YES" in analysis_text:
                    is_relevant = True
                
                # Extract summary and important info
                lines = analysis_text.split('\n')
                for line in lines:
                    if line.startswith('æ‘˜è¦:') or 'æ‘˜è¦ï¼š' in line:
                        summary = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()
                    elif line.startswith('é‡è¦ä¿¡æ¯:') or 'é‡è¦ä¿¡æ¯ï¼š' in line:
                        important_info = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip()
                
                # If no structured response, use the entire analysis as summary
                if not summary:
                    summary = analysis_text.strip()
                
                if is_relevant:
                    # Keep the file and add to data.json
                    file_info = {
                        "description": summary,
                        "important_info": important_info,
                        "file_path": str(file_path),
                        "file_type": source_path.suffix.lower(),
                        "file_size": source_path.stat().st_size,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    existing_data[source_path.name] = file_info
                    relevant_files.append(file_path)
                    print(f"âœ… ç›¸å…³æ–‡ä»¶å·²ä¿ç•™: {source_path.name}")
                    print(f"   æ‘˜è¦: {summary[:100]}...")
                    
                else:
                    # Mark for deletion
                    irrelevant_files.append(file_path)
                    print(f"âŒ ä¸ç›¸å…³æ–‡ä»¶å°†è¢«åˆ é™¤: {source_path.name}")
                    print(f"   åŸå› : {summary[:100]}...")
                    
            except Exception as e:
                print(f"âŒ åˆ†ææ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
                # On error, keep the file to be safe
                relevant_files.append(file_path)
        
        # Delete irrelevant files
        for file_path in irrelevant_files:
            try:
                file_to_delete = Path(file_path)
                if file_to_delete.exists():
                    os.remove(file_to_delete)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸ç›¸å…³æ–‡ä»¶: {file_to_delete.name}")
                
                # Also remove from state lists
                if file_path in state.get("upload_files_processed_path", []):
                    state["upload_files_processed_path"].remove(file_path)
                if file_path in state.get("new_upload_files_processed_path", []):
                    state["new_upload_files_processed_path"].remove(file_path)
                    
            except Exception as e:
                print(f"âŒ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
        
        # Save updated data.json
        try:
            with open(data_json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²æ›´æ–° data.jsonï¼ŒåŒ…å« {len(existing_data)} ä¸ªæ–‡ä»¶è®°å½•")
        except Exception as e:
            print(f"âŒ ä¿å­˜ data.json æ—¶å‡ºé”™: {e}")
        
        # Update state to only include relevant files
        state["new_upload_files_processed_path"] = [f for f in state.get("new_upload_files_processed_path", []) if f in relevant_files]
        
        # Add analysis summary to messages
        analysis_summary = f"""
ğŸ“‹ æ–‡ä»¶åˆ†æå®Œæˆ:
âœ… ç›¸å…³æ–‡ä»¶: {len(relevant_files)} ä¸ª
âŒ ä¸ç›¸å…³æ–‡ä»¶: {len(irrelevant_files)} ä¸ª (å·²åˆ é™¤)
ğŸ“ æ•°æ®åº“è®°å½•: {len(existing_data)} ä¸ªæ–‡ä»¶

ç›¸å…³æ–‡ä»¶åˆ—è¡¨:
{chr(10).join([f"â€¢ {Path(f).name}" for f in relevant_files])}
"""
        
        return {
            "messages": [SystemMessage(content=analysis_summary)]
        }



def _analyze_uploaded_files_related_to

# after we analyze the how related the uploaded files to our system, we will determine if it is related to the
# question the LLM just asked, if that it is related, we will store the content of the file in the state
# and pass it for the LLM to analyze