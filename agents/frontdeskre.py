import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, filter_out_system_messages
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content
from utilities.modelRelated import model_creation, detect_provider

import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


# import other agents
from agents.processUserInput import ProcessUserInputAgent

load_dotenv()


class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    messages_s: Annotated[list[BaseMessage], add_messages]
    table_structure: str
    previous_node: str # Track the previous node
    session_id: str
    


class FrontdeskAgent:
    """
    ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ¨¡æ¿ï¼Œè‹¥æœªæä¾›æ¨¡æ¿ï¼Œå’Œç”¨æˆ·æ²Ÿé€šç¡®å®šè¡¨æ ¼ç»“æ„
    """



    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.llm_c = model_creation(model_name=model_name, temperature=2) # complex logic use user selected model
        self.llm_s = model_creation(model_name="gpt-3.5-turbo", temperature=2) # simple logic use 3-5turbo
        self.graph = self._build_graph()



    def _build_graph(self):
        """This function will build the graph of the frontdesk agent"""

        graph = StateGraph(FrontdeskState)

        graph.add_node("entry", self._entry_node)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("route_after_collect_user_input", self._route_after_collect_user_input)

        graph.add_edge(START, "entry")
        graph.add_edge("entry", "collect_user_input")
        # Add the missing nodes first
        graph.add_node("complex_template_handle", self._complex_template_analysis)
        graph.add_node("simple_template_handle", self._simple_template_analysis)
        graph.add_node("confirm_template", self._analyze_template)
        graph.add_node("confirm_table_structure", self._check_template)
        
        graph.add_conditional_edges(
            "collect_user_input",
            self._route_after_collect_user_input,
            {"complex_template_handle": "complex_template_handle",
             "simple_template_handle": "simple_template_handle",
             "confirm_template": "confirm_template",
             "confirm_table_structure": "confirm_table_structure",
             }
        )
        
        # Add edges to END for the terminal nodes
        graph.add_edge("complex_template_handle", END)
        graph.add_edge("simple_template_handle", END)
        graph.add_edge("confirm_template", END)
        graph.add_edge("confirm_table_structure", END)

        # Compile the graph to make it executable with stream() method
        # You can add checkpointer if needed: graph.compile(checkpointer=MemorySaver())
        return graph.compile()



    def _create_initial_state(self, session_id: str = "1") -> FrontdeskState:
        """This function will create the initial state of the frontdesk agent"""
        return {
            "messages": [],
            "messages_s": [],
            "table_structure": "",
            "session_id": session_id,
            "previous_node": ""
        }


    def _entry_node(self, state: FrontdeskState) -> FrontdeskState:
        """This is the starting node of our frontdesk agent"""
        # Enrich this later, it should include a short description of the agent's ability and how to use it
        welcome_message = "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè¡¨æ ¼å¤„ç†åŠ©æ‰‹ï¼"
        print(welcome_message)
        return {
            "messages": [AIMessage(content=welcome_message)],
            "previous_node": "entry"
        }

    def _collect_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """This node will collect user's input by calling the process user input agent,
        it should return a summary message that contains the next node to route to"""
        
        print(f"ğŸ”„ å¼€å§‹æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼Œå½“å‰ä¼šè¯ID: {state.get('session_id', 'unknown')}")
        
        # Create an instance of the ProcessUserInputAgent
        process_user_input_agent = ProcessUserInputAgent()
        
        # Create initial state for the agent
        current_state = process_user_input_agent.create_initial_state(previous_AI_messages = state["messages"])
        
        config = {"configurable": {"thread_id": state["session_id"]}}
        
        max_interrupt_count = 5
        interrupt_count = 0
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œæœ€å¤§ä¸­æ–­æ¬¡æ•°: {max_interrupt_count}")
        
        while interrupt_count < max_interrupt_count:
            has_interrupt = False
            final_chunk = None

            try:
                print(f"ğŸ”„ å¼€å§‹æµå¼å¤„ç†ï¼Œå½“å‰ä¸­æ–­æ¬¡æ•°: {interrupt_count}")
                
                for chunk in process_user_input_agent.graph.stream(current_state, config = config, stream_mode = "updates"):
                    final_chunk = chunk
                    print(f"ğŸ“¦ æ”¶åˆ°chunk: {list(chunk.keys())}")

                    # check if there is an interrupt
                    if "__interrupt__" in chunk:
                        has_interrupt = True
                        interrupt_count += 1
                        interrupt_value = chunk['__interrupt__'][0].value
                        print(f"\nğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")

                        user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")

                        # Resume the agnet with command and prepare for next iteration
                        current_state = Command(resume = user_response)
                        print(f"ğŸ”„ ç”¨æˆ·å“åº”å·²è®¾ç½®ï¼Œå‡†å¤‡ä¸‹ä¸€è½®å¤„ç†")
                        break

                if not has_interrupt:
                    print(f"âœ… å¤„ç†å®Œæˆï¼Œæ²¡æœ‰ä¸­æ–­")
                    # No more interrupts, processing is complete
                    break
                    
            except Exception as e:
                print(f"âŒ å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {type(e).__name__}: {e}")
                import traceback
                print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                
                # Return error message
                return {
                    "messages": [AIMessage(content=f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")],
                    "previous_node": "collect_user_input"
                }
        
        # Handle case where max interrupts reached
        if interrupt_count >= max_interrupt_count:
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§ä¸­æ–­æ¬¡æ•° ({max_interrupt_count})")
            return {
                "messages": [AIMessage(content="å·²è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°ï¼Œè¯·é‡æ–°å¼€å§‹")],
                "previous_node": "collect_user_input"
            }
        
        # Extract the final result
        try:
            print(f"ğŸ”„ æå–æœ€ç»ˆç»“æœï¼Œfinal_chunkç±»å‹: {type(final_chunk)}")
            
            if final_chunk and "summary_user_input" in final_chunk:
                summary_data = final_chunk["summary_user_input"]
                
                # Handle both cases: summary_message field or direct content
                if "summary_message" in summary_data:
                    summary_content = summary_data["summary_message"]
                elif "process_user_input_messages" in summary_data and summary_data["process_user_input_messages"]:
                    # Extract from the last message
                    last_msg = summary_data["process_user_input_messages"][-1]
                    if hasattr(last_msg, 'content'):
                        summary_content = last_msg.content
                    else:
                        summary_content = str(last_msg)
                else:
                    summary_content = str(summary_data)
                
                print(f"âœ… æˆåŠŸæå–æ€»ç»“ä¿¡æ¯: {str(summary_content)[:100]}...")
                
                # Create the message with the summary content  
                # Content should always be a JSON string now
                if isinstance(summary_content, str):
                    # Content is already a JSON string, use it directly
                    result_message = AIMessage(content=summary_content)
                else:
                    # Convert to JSON string if it's not already
                    import json
                    result_message = AIMessage(content=json.dumps(summary_content, ensure_ascii=False))
                    
                result_message.name = "summary_message"
                
                return {
                    "messages": [result_message],
                    "previous_node": "collect_user_input"
                }
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ€»ç»“ä¿¡æ¯ï¼Œfinal_chunk: {final_chunk}")
                return {
                    "messages": [AIMessage(content="æœªèƒ½è·å–æœ‰æ•ˆçš„å¤„ç†ç»“æœ")],
                    "previous_node": "collect_user_input"
                }
                
        except Exception as e:
            print(f"âŒ æå–ç»“æœæ—¶å‡ºé”™: {type(e).__name__}: {e}")
            return {
                "messages": [AIMessage(content=f"æå–ç»“æœæ—¶å‡ºé”™: {e}")],
                "previous_node": "collect_user_input"
            }



    def _route_after_collect_user_input(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the summary message from the ProcessUserInputAgent"""
        try:
            # Check if the last message has the expected structure
            last_message = state["messages"][-1]
            if hasattr(last_message, 'content') and isinstance(last_message.content, str):
                # Content is a JSON string - parse it
                import json
                try:
                    content_dict = json.loads(last_message.content)
                    next_node = content_dict.get("next_node", "previous_node")
                    print(f"âœ… æˆåŠŸè§£æJSON: {content_dict}")
                except json.JSONDecodeError:
                    print(f"âš ï¸ æ— æ³•è§£æJSONå†…å®¹: {last_message.content}")
                    next_node = "previous_node"
            else:
                print(f"âš ï¸ æ¶ˆæ¯æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›å­—ç¬¦ä¸²ï¼Œå¾—åˆ°: {type(last_message.content)}")
                next_node = "previous_node"
            
            print(f"ğŸ”„ è·¯ç”±å†³å®š: {next_node}")
            
            if next_node == "complex_template":
                return "complex_template_handle"
            elif next_node == "simple_template":
                return "simple_template_handle"
            else:
                return state.get("previous_node", "entry")  # Fallback to previous node
                
        except Exception as e:
            print(f"âŒ è·¯ç”±å†³å®šæ—¶å‡ºé”™: {e}")
            import traceback
            print(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return state.get("previous_node", "entry")  # Safe fallback
    



                    

            

    def _check_template(self, state: FrontdeskState) -> FrontdeskState:
        """This node will check if the user has provided a template"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¡«è¡¨åŠ©æ‰‹æ™ºèƒ½ä½“ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„è¾“å…¥æ¥å†³å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨ï¼Œå¦‚æœç”¨æˆ·æä¾›äº†æ¨¡æ¿ï¼Œ
        è¯·è¿”å›[YES]ï¼Œå¦åˆ™è¿”å›[NO]ï¼Œå¦å¤–ç”¨æˆ·å¯èƒ½ä¸Šä¼ æ–‡ä»¶"""
        # user turbo at here
        response = self.llm_s.invoke([SystemMessage(content=system_prompt)] + state["messages"][-1])
        return {"messages": response}
    


    def _route_after_check_template(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the user's input"""
        if state["messages"][-1].content == "[YES]":
            return "template_provided"
        else:
            return "no_template_provided"
        


    def _analyze_template(self, state: FrontdeskState) -> FrontdeskState:
        """This node will analyze the template to determine if it a complex template
        (both row, column headers) or a simple template (only column headers)"""
        system_prompt = """ä½ éœ€è¦æ ¹æ®htmlä»£ç åˆ¤æ–­è¿™ä¸ªæ¨¡æ¿æ˜¯å¤æ‚æ¨¡æ¿è¿˜æ˜¯ç®€å•æ¨¡æ¿ï¼Œåˆ¤æ–­è§„åˆ™ä¸ºï¼š
        1. å¦‚æœhtmlä»£ç ä¸­åŒ…å«rowå’Œcolumn headersï¼Œåˆ™è¿”å›[YES]
        2. å¦‚æœhtmlä»£ç ä¸­åªåŒ…å«column headersï¼Œåˆ™è¿”å›[NO]
        3. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        4. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        5. å¦‚æœhtmlä»£ç ä¸­æ—¢åŒ…å«row headersåˆåŒ…å«column headersï¼Œåˆ™è¿”å›[YES]
        """
        # use 3-5turbo at here
    


    def _complex_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """This node will be use to analyze the complex table template, we will skip for now"""
        pass

    def _simple_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """This node will be use to analyze the simple table template, we"""
        pass

    
    def run_frontdesk_agent(self, session_id: str = "1") -> None:
        """This function will run the frontdesk agent"""
        initial_state = self._create_initial_state(session_id)
        config = {"configurable": {"thread_id": session_id}}
        current_state = initial_state

        while True:
            try:
                has_interrupt = False
                for chunk in self.graph.stream(current_state, config = config, stream_mode = "updates"):
                    for node_name, node_output in chunk.items():
                        print(f"\nğŸ“ Node: {node_name}")
                        print("-" * 30)

                        # check if there is an interrupt
                        if "__interrupt__" in chunk:
                            has_interrupt = True
                            interrupt_value = chunk['__interrupt__'][0].value
                            print(f"\nğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                            user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")

                            # set the next input
                            current_state = Command(resume=user_response)
                            break

                        if isinstance(node_output, dict):
                            if "messages" in node_output and node_output["messages"]:
                                latest_message = node_output["messages"][-1]
                                if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
                                    print(f"ğŸ’¬ æ™ºèƒ½ä½“å›å¤: {latest_message.content}")

                            for key, value in node_output.items():
                                if key != "messages" and value:
                                    print(f"ğŸ“Š {key}: {value}")
                        print("-" * 30)
                
                if not has_interrupt:
                    break

            
            except Exception as e:
                print(f"âŒ å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")
                break

            





if __name__ == "__main__":
    frontdesk_agent = FrontdeskAgent()
    frontdesk_agent.run_frontdesk_agent()