import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime

from utilities.modelRelated import invoke_model, invoke_model_with_tools
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content


from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv


from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool

# import other agents
from agents.processUserInput import ProcessUserInputAgent

load_dotenv()


@tool
def _collect_user_input(session_id: str, previous_AI_messages: BaseMessage) -> str:
    """è¿™æ˜¯ä¸€ä¸ªç”¨æ¥æ”¶é›†ç”¨æˆ·è¾“å…¥çš„å·¥å…·ï¼Œä½ éœ€è¦è°ƒç”¨è¿™ä¸ªå·¥å…·æ¥æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼Œ
    å‚æ•°ï¼š
        state: å½“å‰FrontdeskAgengtçš„çŠ¶æ€ï¼ŒåŒ…å«å½“å‰çš„messagesï¼Œsession_idï¼Œprevious_node
    è¿”å›ï¼š
        FrontdeskState: åŒ…å«å½“å‰çš„messagesï¼Œsession_idï¼Œprevious_node, ä»¥åŠprocess_user_input_agentçš„è¿”å›ç»“æœç­‰
    """

    print(f"ğŸ”„ å¼€å§‹æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼Œå½“å‰ä¼šè¯ID: {session_id}")
    
    # Create an instance of the ProcessUserInputAgent
    process_user_input_agent = ProcessUserInputAgent()
    
    final_chunk = process_user_input_agent.run_process_user_input_agent(session_id = session_id, previous_AI_messages = previous_AI_messages)
    
    # Extract the final result
    try:
        print(f"ğŸ”„ æå–æœ€ç»ˆç»“æœï¼Œfinal_chunkç±»å‹: {type(final_chunk)}")
        
        if final_chunk and "summary_user_input" in final_chunk:
            summary_data = final_chunk["summary_user_input"]
            
            # Handle both cases: summary_message field or direct content
            if "summary_message" in summary_data:
                print("summary_message in summary_data")
                summary_content = summary_data["summary_message"]
            elif "process_user_input_messages" in summary_data and summary_data["process_user_input_messages"]:
                # Extract from the last message
                last_msg = summary_data["process_user_input_messages"][-1]
                if hasattr(last_msg, 'content'):
                    summary_content = last_msg.content
                else:
                    summary_content = str(last_msg)
            else:
                summary_content = str(summary_data)
            
            print(f"âœ… æˆåŠŸæå–æ€»ç»“ä¿¡æ¯: {str(summary_content)[:100]}...")
            
            # Create the message with the summary content  
            # Content should always be a JSON string now
            if isinstance(summary_content, str):
                # Content is already a JSON string, use it directly
                result_message = AIMessage(content=summary_content)
            else:
                # Convert to JSON string if it's not already
                import json
                result_message = AIMessage(content=json.dumps(summary_content, ensure_ascii=False))
                
            result_message.name = "summary_message"
            
            return Command(
                update = {
                    "messages": [result_message],
                    "chat_history": summary_content,
                }   
            )
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ€»ç»“ä¿¡æ¯ï¼Œfinal_chunk: {final_chunk}")
            return Command(
                update = {
                    "messages": [AIMessage(content="æœªèƒ½è·å–æœ‰æ•ˆçš„å¤„ç†ç»“æœ")],
                    "chat_history": "æœªèƒ½è·å–æœ‰æ•ˆçš„å¤„ç†ç»“æœ",
                }
            )
            
    except Exception as e:
        print(f"âŒ æå–ç»“æœæ—¶å‡ºé”™: {type(e).__name__}: {e}")
        return Command(
            update = {
                "messages": [AIMessage(content=f"æå–ç»“æœæ—¶å‡ºé”™: {e}")],
                "chat_history": f"æå–ç»“æœæ—¶å‡ºé”™: {e}",
            }
        )
    

class FrontdeskState(TypedDict):
    chat_history: Annotated[list[str], add_messages]
    messages: Annotated[list[BaseMessage], add_messages]
    table_structure: str
    previous_node: str # Track the previous node
    session_id: str
    template_structure: str
    table_summary: str


class FrontdeskAgent:
    """
    ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ¨¡æ¿ï¼Œè‹¥æœªæä¾›æ¨¡æ¿ï¼Œå’Œç”¨æˆ·æ²Ÿé€šç¡®å®šè¡¨æ ¼ç»“æ„
    """



    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.tools = [_collect_user_input]
        self.graph = self._build_graph()




    def _build_graph(self):
        """This function will build the graph of the frontdesk agent"""

        graph = StateGraph(FrontdeskState)

        graph.add_node("entry", self._entry_node)
        graph.add_node("collect_user_input", ToolNode(self.tools))
        graph.add_node("force_collect_user_input", self._force_collect_user_input)
        graph.add_node("complex_template_handle", self._complex_template_analysis)
        graph.add_node("simple_template_handle", self._simple_template_analysis)
        graph.add_node("chat_with_user_to_determine_template", self._chat_with_user_to_determine_template)

        graph.add_edge(START, "entry")
        graph.add_edge("entry", "force_collect_user_input")
        graph.add_conditional_edges("force_collect_user_input", self._route_after_collect_user_input)
        graph.add_conditional_edges("collect_user_input", self._route_after_collect_user_input)
        graph.add_conditional_edges("chat_with_user_to_determine_template", self._route_after_chat_with_user_to_determine_template)
        graph.add_conditional_edges("simple_template_handle", self._route_after_simple_template_analysis)

        
        # Compile the graph to make it executable with stream() method
        # You can add checkpointer if needed: graph.compile(checkpointer=MemorySaver())
        return graph.compile()



    def _create_initial_state(self, session_id: str = "1") -> FrontdeskState:
        """This function will create the initial state of the frontdesk agent"""
        return {
            "chat_history": [],
            "messages": [],
            "messages_s": [],
            "table_structure": "",
            "session_id": session_id,
            "previous_node": ""
        }


    def _entry_node(self, state: FrontdeskState) -> FrontdeskState:
        """This is the starting node of our frontdesk agent"""
        # Enrich this later, it should include a short description of the agent's ability and how to use it
        welcome_message = "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè¡¨æ ¼å¤„ç†åŠ©æ‰‹ï¼"
        print(welcome_message)
        return {
            "messages": [AIMessage(content=welcome_message)],
            "previous_node": "chat_with_user_to_determine_template"
        }
    

    def _force_collect_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """ç›´æ¥è°ƒç”¨å·¥å…·æ”¶é›†ç”¨æˆ·è¾“å…¥"""
        session_id = state["session_id"]
        previous_AI_messages = state["messages"][-1]
         # âœ… Use .invoke() method with proper input format
        tool_input = {
        "session_id": session_id,
        "previous_AI_messages": previous_AI_messages
        }
        command_result = _collect_user_input.invoke(tool_input)

        return command_result.update

    def _route_after_collect_user_input(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the summary message from the ProcessUserInputAgent"""

        try:
            # Check if the last message has the expected structure
            last_message = state["messages"][-1]
            if hasattr(last_message, 'content') and isinstance(last_message.content, str):
                # Content is a JSON string - parse it
                import json
                try:
                    content_dict = json.loads(last_message.content)
                    next_node = content_dict.get("next_node", "previous_node")
                    print(f"âœ… æˆåŠŸè§£æJSON: {content_dict}")
                except json.JSONDecodeError:
                    print(f"âš ï¸ æ— æ³•è§£æJSONå†…å®¹: {last_message.content}")
                    next_node = "previous_node"
            else:
                print(f"âš ï¸ æ¶ˆæ¯æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›å­—ç¬¦ä¸²ï¼Œå¾—åˆ°: {type(last_message.content)}")
                next_node = "previous_node"
            
            print(f"ğŸ”„ è·¯ç”±å†³å®š: {next_node}")
            
            if next_node == "complex_template":
                return "complex_template_handle"
            elif next_node == "simple_template":
                return "simple_template_handle"
            else:
                return state.get("previous_node", "entry")  # Fallback to previous node
                
        except Exception as e:
            print(f"âŒ è·¯ç”±å†³å®šæ—¶å‡ºé”™: {e}")
            import traceback
            print(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return state.get("previous_node", "entry")  # Safe fallback
            


    def _complex_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """This node will be use to analyze the complex table template, we will skip for now"""
        pass

    def _chat_with_user_to_determine_template(self, state: FrontdeskState) -> FrontdeskState:
        """This node will chat with the user to determine the template, when the template is not provided"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½excelè¡¨æ ¼ç”ŸæˆåŠ©æ‰‹ï¼Œç°åœ¨ä½ éœ€è¦å’Œç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œæ¥ç¡®è®¤ç”¨æˆ·æƒ³è¦ç”Ÿæˆçš„è¡¨æ ¼ç»“æ„
        å†…å®¹ï¼Œè¡¨æ ¼å¯èƒ½æ¶‰åŠåˆ°å¤æ‚çš„å¤šçº§è¡¨å¤´ï¼Œå› æ­¤ä½ éœ€è¦å¼„æ¸…æ¥šæ‰€æœ‰çš„ç»“æ„å±‚çº§ï¼Œä¸æ–­è¯¢é—®ç”¨æˆ·ï¼ŒçŸ¥é“ä½ ææ¸…æ¥šå…¨éƒ¨éœ€æ±‚ï¼Œå¹¶è¿”å›
        ä»¥ä¸‹æ ¼å¼ï¼š
        1. æå–è¡¨æ ¼çš„å¤šçº§è¡¨å¤´ç»“æ„ï¼›
   - ä½¿ç”¨åµŒå¥—çš„ key-value å½¢å¼è¡¨ç¤ºå±‚çº§å…³ç³»ï¼›
   - æ¯ä¸€çº§è¡¨å¤´åº”ä»¥å¯¹è±¡å½¢å¼å±•ç¤ºå…¶å­çº§å­—æ®µæˆ–å­è¡¨å¤´ï¼›
   - ä¸éœ€è¦é¢å¤–å­—æ®µï¼ˆå¦‚ nullã€isParent ç­‰ï¼‰ï¼Œä»…ä¿ç•™ç»“æ„æ¸…æ™°çš„å±‚çº§æ˜ å°„ï¼›

2. æä¾›ä¸€ä¸ªå¯¹è¯¥è¡¨æ ¼å†…å®¹çš„ç®€è¦æ€»ç»“ï¼›
   - å†…å®¹åº”åŒ…æ‹¬è¡¨æ ¼ç”¨é€”ã€ä¸»è¦ä¿¡æ¯ç±»åˆ«ã€é€‚ç”¨èŒƒå›´ç­‰ï¼›
   - è¯­è¨€ç®€æ´ï¼Œä¸è¶…è¿‡ 150 å­—ï¼›

è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
{
  "è¡¨æ ¼ç»“æ„": {
    "é¡¶å±‚è¡¨å¤´åç§°": {
      "äºŒçº§è¡¨å¤´åç§°": [
        "å­—æ®µ1",
        "å­—æ®µ2",
        ...
      ],
      ...
    },
    ...
  },
  "è¡¨æ ¼æ€»ç»“": "è¯¥è¡¨æ ¼çš„ä¸»è¦ç”¨é€”åŠå†…å®¹è¯´æ˜..."
}

        è¯·å¿½ç•¥æ‰€æœ‰ HTML æ ·å¼æ ‡ç­¾ï¼Œåªå…³æ³¨è¡¨æ ¼ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

        ä½ ä¹Ÿå¯ä»¥è°ƒç”¨å·¥å…·æ¥æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼Œæ¥å¸®åŠ©ä½ åˆ†æè¡¨æ ¼ç»“æ„ï¼Œæœ‰ä»»ä½•ä¸ç¡®å®šçš„åœ°æ–¹ä¸€å®šè¦è¯¢é—®ç”¨æˆ·ï¼Œç›´åˆ°ä½ å®Œå…¨æ˜ç¡®è¡¨æ ¼ç»“æ„ä¸ºæ­¢
        """

        response = invoke_model_with_tools(model_name="Qwen/Qwen3-8B", messages=[SystemMessage(content=system_prompt)] + state["messages"], tools=self.tools)
        
        # åˆ›å»ºAIMessageæ—¶éœ€è¦ä¿ç•™tool_callsä¿¡æ¯
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ›å»ºåŒ…å«tool_callsçš„AIMessage
            ai_message = AIMessage(content=response.content or "", tool_calls=response.tool_calls)
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ŒåªåŒ…å«å†…å®¹
            ai_message = AIMessage(content=str(response.content) if hasattr(response, 'content') else str(response))
        
        return {"table_structure": str(response),
                "previous_node": "complex_template_handle",
                "messages": [ai_message]
                }
    
    def _route_after_chat_with_user_to_determine_template(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the user's input"""
        latest_message = state["messages"][-1]
        if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
            return "collect_user_input"
        else:
            return "END"

    def _simple_template_analysis(self, state: FrontdeskState) -> FrontdeskState:
        """å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„ç®€å•æ¨¡æ¿"""
        prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·é˜…è¯»ç”¨æˆ·ä¸Šä¼ çš„ HTML æ ¼å¼çš„ Excel æ–‡ä»¶ï¼Œå¹¶å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä½ ä¹Ÿå¯ä»¥è°ƒç”¨å·¥å…·æ¥æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼Œæ¥å¸®åŠ©ä½ åˆ†æè¡¨æ ¼ç»“æ„ï¼Œæœ‰ä»»ä½•ä¸ç¡®å®šçš„åœ°æ–¹ä¸€å®šè¦è¯¢é—®ç”¨æˆ·ï¼Œç›´åˆ°ä½ å®Œå…¨æ˜ç¡®è¡¨æ ¼ç»“æ„ä¸ºæ­¢
1. æå–è¡¨æ ¼çš„å¤šçº§è¡¨å¤´ç»“æ„ï¼›
   - ä½¿ç”¨åµŒå¥—çš„ key-value å½¢å¼è¡¨ç¤ºå±‚çº§å…³ç³»ï¼›
   - æ¯ä¸€çº§è¡¨å¤´åº”ä»¥å¯¹è±¡å½¢å¼å±•ç¤ºå…¶å­çº§å­—æ®µæˆ–å­è¡¨å¤´ï¼›
   - ä¸éœ€è¦é¢å¤–å­—æ®µï¼ˆå¦‚ nullã€isParent ç­‰ï¼‰ï¼Œä»…ä¿ç•™ç»“æ„æ¸…æ™°çš„å±‚çº§æ˜ å°„ï¼›

2. æä¾›ä¸€ä¸ªå¯¹è¯¥è¡¨æ ¼å†…å®¹çš„ç®€è¦æ€»ç»“ï¼›
   - å†…å®¹åº”åŒ…æ‹¬è¡¨æ ¼ç”¨é€”ã€ä¸»è¦ä¿¡æ¯ç±»åˆ«ã€é€‚ç”¨èŒƒå›´ç­‰ï¼›
   - è¯­è¨€ç®€æ´ï¼Œä¸è¶…è¿‡ 150 å­—ï¼›

è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
{
  "è¡¨æ ¼ç»“æ„": {
    "é¡¶å±‚è¡¨å¤´åç§°": {
      "äºŒçº§è¡¨å¤´åç§°": [
        "å­—æ®µ1",
        "å­—æ®µ2",
        ...
      ],
      ...
    },
    ...
  },
  "è¡¨æ ¼æ€»ç»“": "è¯¥è¡¨æ ¼çš„ä¸»è¦ç”¨é€”åŠå†…å®¹è¯´æ˜..."
}

è¯·å¿½ç•¥æ‰€æœ‰ HTML æ ·å¼æ ‡ç­¾ï¼Œåªå…³æ³¨è¡¨æ ¼ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚"""

        response = invoke_model_with_tools(model_name="Qwen/Qwen3-8B", messages=[SystemMessage(content=prompt)] + state["messages"], tools=[self._collect_user_input])
        
        # åˆ›å»ºAIMessageæ—¶éœ€è¦ä¿ç•™tool_callsä¿¡æ¯
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ›å»ºåŒ…å«tool_callsçš„AIMessage
            ai_message = AIMessage(content=response.content or "", tool_calls=response.tool_calls)
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ŒåªåŒ…å«å†…å®¹
            ai_message = AIMessage(content=str(response.content) if hasattr(response, 'content') else str(response))
        
        return {"template_structure": str(response),
                "previous_node": "simple_template_handle",
                "messages": [ai_message]
                }
        
    def _route_after_simple_template_analysis(self, state: FrontdeskState) -> str:
        """This node will route the agent to the next node based on the user's input"""
        latest_message = state["messages"][-1]
        if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
            return "collect_user_input"
        else:
            return "END"

    
    def run_frontdesk_agent(self, session_id: str = "1") -> None:
        """This function will run the frontdesk agent"""
        # initial_state = self._create_initial_state(session_id)
        # config = {"configurable": {"thread_id": session_id}}
        # current_state = initial_state

        # while True:
        #     try:
        #         has_interrupt = False
        #         for chunk in self.graph.stream(current_state, config = config, stream_mode = "updates"):
        #             for node_name, node_output in chunk.items():
        #                 print(f"\nğŸ“ Node: {node_name}")
        #                 print("-" * 30)

        #                 # check if there is an interrupt
        #                 if "__interrupt__" in chunk:
        #                     has_interrupt = True
        #                     interrupt_value = chunk['__interrupt__'][0].value
        #                     print(f"\nğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
        #                     user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")

        #                     # set the next input
        #                     current_state = Command(resume=user_response)
        #                     break

        #                 if isinstance(node_output, dict):
        #                     if "messages" in node_output and node_output["messages"]:
        #                         latest_message = node_output["messages"][-1]
        #                         if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
        #                             print(f"ğŸ’¬ æ™ºèƒ½ä½“å›å¤: {latest_message.content}")

        #                     for key, value in node_output.items():
        #                         if key != "messages" and value:
        #                             print(f"ğŸ“Š {key}: {value}")
        #                 print("-" * 30)
                
        #         if not has_interrupt:
        #             break

            
        #     except Exception as e:
        #         print(f"âŒ å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {e}")
        #         break

        for chunk in self.graph.stream(self._create_initial_state(session_id), stream_mode = "updates"):
            for node_name, node_output in chunk.items():
                print(f"\nğŸ“ Node: {node_name}")
                print("-" * 30)
                if isinstance(node_output, dict):
                    if "messages" in node_output and node_output["messages"]:
                        latest_message = node_output["messages"][-1]
                        if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
                            print(f"ğŸ’¬ æ™ºèƒ½ä½“å›å¤: {latest_message.content}")
                    for key, value in node_output.items():
                        if key != "messages" and value:
                            print(f"ğŸ“Š {key}: {value}")
                print("-" * 30)

            

frontdesk_agent = FrontdeskAgent()
graph = frontdesk_agent.graph



if __name__ == "__main__":
    frontdesk_agent = FrontdeskAgent()
    frontdesk_agent.run_frontdesk_agent()