import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, filter_out_system_messages
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content
from utilities.modelRelated import model_creation, detect_provider

import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

from langgraph.graph import StateGraph, END, START, Send
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


load_dotenv()

class ProcessUserInputState(TypedDict):
    process_user_input_messages: Annotated[list[BaseMessage], add_messages]
    user_input: str
    upload_files_path: list[str]
    new_upload_files_path: list[str] # Track the new uploaded files
    upload_files_processed_path: list[str]
    new_upload_files_processed_path: list[str]
    uploaded_template_files_path: list[str]
    supplement_files_path: dict[str, list[str]]
    irrelevant_files_path: list[str]
    session_id: str
    
class ProcessUserInputAgent:

    @tool
    def request_user_clarification(question: str, context: str = "") -> str:
        """
        è¯¢é—®ç”¨æˆ·æ¾„æ¸…ï¼Œå’Œç”¨æˆ·ç¡®è®¤ï¼Œæˆ–è€…è¯¢é—®ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œå½“ä½ ä¸ç¡®å®šçš„æ—¶å€™è¯·è¯¢é—®ç”¨æˆ·

        å‚æ•°ï¼š
            question: é—®é¢˜
            contexnt: å¯é€‰è¡¥å……å†…å®¹ï¼Œè§£é‡Šä¸ºç”šæ¶é­”ä½ éœ€è¦ä¸€ä¸‹ä¿¡æ¯
        """
        prompt = f"{question}\n{context}"
        user_response = interrupt({"prompt": prompt})

        return user_response
    
    tools = [request_user_clarification]

    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.llm_c = model_creation(model_name=model_name, temperature=2) # complex logic use user selected model
        self.llm_c_with_tools = self.llm_c.bind_tools(self.tools)
        self.llm_s = model_creation(model_name="gpt-3.5-turbo", temperature=2) # simple logic use 3-5turbo
        self.llm_s_with_tools = self.llm_s.bind_tools(self.tools)


    def _build_graph(self) -> StateGraph:
        """This function will build the graph for the process user input agent"""
        graph = StateGraph(ProcessUserInputState)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("file_upload", self._file_upload)
        graph.add_node("analyze_file", self._analyze_file)
        graph.add_node("analyze_uploaded_files", self._analyze_uploaded_files)
        graph.add_node("process_supplement", self._process_supplement)

        graph.add_edge(START, "collect_user_input")
        graph.add_conditional_edges(
            "collect_user_input", 
            self._route_after_collect_user_input,
            {
                "file_upload": "file_upload",
                "valid_input": "",
                "invalid_input": "collect_user_input"
            }
            )

    

    clarification_tool_node = ToolNode(tools)



    def _collect_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This is the node where we get user's input"""
        user_input = interrupt("ç”¨æˆ·ï¼š")
        return {"user_input": user_input}



    def _route_after_collect_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node act as a safety check node, it will analyze the user's input and determine if it's a valid input,
        based on the LLM's previous response, at the same time it will route the agent to the correct node"""
        # We should let LLM decide the route
        
        user_upload_files = detect_and_process_file_paths(state["process_user_input_messages"][-1])
        # Filter out the new uploaded files
        new_upload_files = [item for item in user_upload_files if item not in state["upload_files_path"]]
        if new_upload_files:
            state["new_upload_files_path"] = new_upload_files
            state["upload_files_path"].extend(new_upload_files)
            return "file_upload"
        
        # User didn't upload new files
        elif not user_upload_files:
            system_prompt = """ä½ éœ€è¦åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆè¾“å…¥ï¼Œåˆ¤æ–­æ ‡å‡†ä¸º"""
            LLM_response_and_user_input = [state["process_user_input_messages"][-2], state["process_user_input_messages"][-1]]
            LLM_decision = self.llm_s.invoke([SystemMessage(content=system_prompt)] + LLM_response_and_user_input)
            # If it is a valid input we conitnue the normal execution flow, otherwise we will keep leting user 
            # input messages until it is a valid input
            if LLM_decision.content == "[YES]":
                return "valid_input"
            else:
                print(f"âŒ Invalid input: {state['process_user_input_messages'][-1].content}")
                return "invalid_input"
    


    def _uploaded_files(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will upload user's file to our system"""
        # For now we simply store the file content 
        result = retrieve_file_content(state["new_upload_files_path"], state["session_id"])
        state["new_upload_files_processed_path"] = result
        state["upload_files_processed_path"].extend(result)
        print(f"âœ… File uploaded: {state['upload_files_processed_path']}")
        return "analyze_file"
    


    def _analyze_uploaded_files(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will analyze the user's uploaded files, it need to classify the file into template
        supplement, or irrelevant"""
        
        import json
        import os
        from pathlib import Path
        
        # Initialize classification results
        classification_results = {
            "template": [],
            "supplement": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
            "irrelevant": []
        }
        
        # Process files in batch for efficiency
        files_content = []
        for file_path in state["new_upload_files_processed_path"]:
            try:
                source_path = Path(file_path)
                if not source_path.exists():
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:2000] if len(file_content) > 2000 else file_content
                
                files_content.append({
                    "file_path": file_path,
                    "file_name": source_path.name,
                    "content": analysis_content
                })
                
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
                continue
        
        if not files_content:
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [],
                "process_user_input_messages": [SystemMessage(content="æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")]
            }
        
        # Create analysis prompt in Chinese
        files_info = "\n\n".join([
            f"æ–‡ä»¶å: {item['file_name']}\næ–‡ä»¶è·¯å¾„: {item['file_path']}\næ–‡ä»¶å†…å®¹:\n{item['content']}"
            for item in files_content
        ])
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹å¹¶è¿›è¡Œåˆ†ç±»ã€‚å…±æœ‰å››ç§ç±»å‹ï¼š

        1. **æ¨¡æ¿ç±»å‹ (template)**: ç©ºç™½è¡¨æ ¼æ¨¡æ¿ï¼Œåªæœ‰è¡¨å¤´æ²¡æœ‰å…·ä½“æ•°æ®
        2. **è¡¥å……è¡¨æ ¼ (supplement-è¡¨æ ¼)**: å·²å¡«å†™çš„å®Œæ•´è¡¨æ ¼ï¼Œç”¨äºè¡¥å……æ•°æ®åº“
        3. **è¡¥å……æ–‡æ¡£ (supplement-æ–‡æ¡£)**: åŒ…å«é‡è¦ä¿¡æ¯çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¦‚æ³•å¾‹æ¡æ–‡ã€æ”¿ç­–ä¿¡æ¯ç­‰
        4. **æ— å…³æ–‡ä»¶ (irrelevant)**: ä¸è¡¨æ ¼å¡«å†™æ— å…³çš„æ–‡ä»¶

        æ³¨æ„ï¼šæ‰€æœ‰æ–‡ä»¶å·²è½¬æ¢ä¸ºtxtæ ¼å¼ï¼Œè¡¨æ ¼ä»¥HTMLä»£ç å½¢å¼å‘ˆç°ï¼Œè¯·æ ¹æ®å†…å®¹è€Œéæ–‡ä»¶åæˆ–åç¼€åˆ¤æ–­ã€‚

        ç”¨æˆ·è¾“å…¥: {state.get("user_input", "")}

        æ–‡ä»¶ä¿¡æ¯:
        {files_info}

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
        {{
            "template": ["æ–‡ä»¶è·¯å¾„1", "æ–‡ä»¶è·¯å¾„2"],
            "supplement": {{"è¡¨æ ¼": ["æ–‡ä»¶è·¯å¾„1"], "æ–‡æ¡£": ["æ–‡ä»¶è·¯å¾„2"]}},
            "irrelevant": ["æ–‡ä»¶è·¯å¾„1"]
        }}"""
        
        try:
            # Get LLM analysis
            analysis_response = self.llm_c_with_tools.invoke([SystemMessage(content=system_prompt)])
            
            # Handle tool calls if LLM needs clarification
            if hasattr(analysis_response, 'tool_calls') and analysis_response.tool_calls:
                # Add the analysis response to process messages for tool handling
                return {
                    "process_user_input_messages": [analysis_response]
                }
            
            # Parse JSON response
            try:
                # Extract JSON from response
                response_content = analysis_response.content.strip()
                # Remove markdown code blocks if present
                if response_content.startswith('```'):
                    response_content = response_content.split('\n', 1)[1]
                    response_content = response_content.rsplit('\n', 1)[0]
                
                classification_results = json.loads(response_content)
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æé”™è¯¯: {e}")
                print(f"LLMå“åº”: {analysis_response.content}")
                # Fallback: keep all files as irrelevant for safety
                classification_results = {
                    "template": [],
                    "supplement": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                    "irrelevant": [item['file_path'] for item in files_content]
                }
            
            # Update state with classification results
            uploaded_template_files = classification_results.get("template", [])
            supplement_files = classification_results.get("supplement", {"è¡¨æ ¼": [], "æ–‡æ¡£": []})
            irrelevant_files = classification_results.get("irrelevant", [])
            
            # Create analysis summary message
            analysis_summary = f"""ğŸ“‹ æ–‡ä»¶åˆ†æå®Œæˆ:
            âœ… æ¨¡æ¿æ–‡ä»¶: {len(uploaded_template_files)} ä¸ª
            âœ… è¡¥å……è¡¨æ ¼: {len(supplement_files.get("è¡¨æ ¼", []))} ä¸ª  
            âœ… è¡¥å……æ–‡æ¡£: {len(supplement_files.get("æ–‡æ¡£", []))} ä¸ª
            âŒ æ— å…³æ–‡ä»¶: {len(irrelevant_files)} ä¸ª

            åˆ†ç±»è¯¦æƒ…:
            æ¨¡æ¿: {[Path(f).name for f in uploaded_template_files]}
            è¡¨æ ¼: {[Path(f).name for f in supplement_files.get("è¡¨æ ¼", [])]}
            æ–‡æ¡£: {[Path(f).name for f in supplement_files.get("æ–‡æ¡£", [])]}
            æ— å…³: {[Path(f).name for f in irrelevant_files]}"""
            
            return {
                "uploaded_template_files_path": uploaded_template_files,
                "supplement_files_path": supplement_files,
                "irrelevant_files_path": irrelevant_files,
                "process_user_input_messages": [SystemMessage(content=analysis_summary)]
            }
            
        except Exception as e:
            print(f"âŒ åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
            # Fallback: keep all files as irrelevant for safety
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [item['file_path'] for item in files_content],
                "process_user_input_messages": [SystemMessage(content=f"æ–‡ä»¶åˆ†æå‡ºé”™: {e}")]
            }
                
    def _route_after_analyze_uploaded_files(self, state: ProcessUserInputState):
        if state.get("user_clarification_request"):
            return [Send("request_user_clarification", state)]
        
        sends = []
        if state.get("template_files"):
            sends.append(Send("_process_template", state))
        if state.get("supplement_files"):
            sends.append(Send("_process_supplement", state))
        if state.get("irrelevant_files"):
            sends.append(Send("_process_irrelevant", state))
        
        return sends
    
    def _process_supplement(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the supplement files, it will analyze the supplement files and summarize the content of the files"""
        
        # Load existing data.json
        data_json_path = Path("agents/data.json")
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            data = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        
        table_files = state["supplement_files_path"]["è¡¨æ ¼"]
        document_files = state["supplement_files_path"]["æ–‡æ¡£"]
        
        # Process table files
        for table_file in table_files:
            try:
                source_path = Path(table_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼åˆ†æä¸“å®¶ï¼Œç°åœ¨è¿™ä¸ªexcelè¡¨æ ¼å·²ç»è¢«è½¬æ¢æˆäº†HTMLæ ¼å¼ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»è¿™ä¸ªè¡¨æ ¼ï¼Œåˆ†æè¡¨æ ¼çš„ç»“æ„ï¼Œå¹¶æ€»ç»“è¡¨æ ¼çš„å†…å®¹ï¼Œæ‰€æœ‰çš„è¡¨å¤´ã€åˆ—åã€æ•°æ®éƒ½è¦æ€»ç»“å‡ºæ¥ã€‚

                æ–‡ä»¶å†…å®¹:
                {file_content}

                è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
                {{
                    "è¡¨æ ¼ç»“æ„": "æè¿°è¡¨æ ¼çš„æ•´ä½“ç»“æ„",
                    "è¡¨å¤´ä¿¡æ¯": ["åˆ—å1", "åˆ—å2", "åˆ—å3"],
                    "æ•°æ®æ¦‚è¦": "æ•°æ®çš„æ€»ä½“æè¿°å’Œé‡è¦ä¿¡æ¯",
                    "è¡Œæ•°ç»Ÿè®¡": "æ€»è¡Œæ•°",
                    "å…³é”®å­—æ®µ": ["é‡è¦å­—æ®µ1", "é‡è¦å­—æ®µ2"]
                }}"""
                                
                analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
                
                # Store in data.json
                data["è¡¨æ ¼"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(table_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¡¨æ ¼æ–‡ä»¶å‡ºé”™ {table_file}: {e}")

        # Process document files
        for document_file in document_files:
            try:
                source_path = Path(document_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æä¸“å®¶ï¼Œç°åœ¨è¿™ä¸ªæ–‡æ¡£å·²ç»è¢«è½¬æ¢æˆäº†txtæ ¼å¼ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»è¿™ä¸ªæ–‡æ¡£ï¼Œåˆ†ææ–‡æ¡£çš„å†…å®¹ï¼Œå¹¶æ€»ç»“æ–‡æ¡£çš„å†…å®¹ã€‚æ–‡æ¡£å¯èƒ½åŒ…å«é‡è¦çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ³•å¾‹æ¡æ–‡ã€æ”¿ç­–è§„å®šç­‰ï¼Œä½ ä¸èƒ½é—æ¼è¿™äº›ä¿¡æ¯ã€‚

                æ–‡ä»¶å†…å®¹:
                {file_content}

                è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
                {{
                    "æ–‡æ¡£ç±»å‹": "åˆ¤æ–­æ–‡æ¡£çš„ç±»å‹ï¼ˆå¦‚æ”¿ç­–æ–‡ä»¶ã€æ³•å¾‹æ¡æ–‡ã€è¯´æ˜æ–‡æ¡£ç­‰ï¼‰",
                    "ä¸»è¦å†…å®¹": "æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹æ¦‚è¦",
                    "é‡è¦æ¡æ¬¾": ["é‡è¦æ¡æ¬¾1", "é‡è¦æ¡æ¬¾2"],
                    "å…³é”®ä¿¡æ¯": ["å…³é”®ä¿¡æ¯1", "å…³é”®ä¿¡æ¯2"],
                    "åº”ç”¨åœºæ™¯": "è¿™äº›ä¿¡æ¯åœ¨è¡¨æ ¼å¡«å†™ä¸­çš„ç”¨é€”"
                }}"""
                                
                analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
                
                # Store in data.json
                data["æ–‡æ¡£"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(document_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"âœ… æ–‡æ¡£æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡æ¡£æ–‡ä»¶å‡ºé”™ {document_file}: {e}")
        
        # Save updated data.json
        try:
            with open(data_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"âœ… å·²æ›´æ–° data.jsonï¼Œè¡¨æ ¼æ–‡ä»¶ {len(data['è¡¨æ ¼'])} ä¸ªï¼Œæ–‡æ¡£æ–‡ä»¶ {len(data['æ–‡æ¡£'])} ä¸ª")
        except Exception as e:
            print(f"âŒ ä¿å­˜ data.json æ—¶å‡ºé”™: {e}")
        
        # Create summary message
        summary_message = f"""ğŸ“Š è¡¥å……æ–‡ä»¶å¤„ç†å®Œæˆ:
âœ… è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ªå·²åˆ†æå¹¶å­˜å‚¨
âœ… æ–‡æ¡£æ–‡ä»¶: {len(document_files)} ä¸ªå·²åˆ†æå¹¶å­˜å‚¨
ğŸ“ æ•°æ®åº“å·²æ›´æ–°ï¼Œæ€»è®¡è¡¨æ ¼ {len(data['è¡¨æ ¼'])} ä¸ªï¼Œæ–‡æ¡£ {len(data['æ–‡æ¡£'])} ä¸ª"""
        
        return {
            "process_user_input_messages": [SystemMessage(content=summary_message)]
        }
        
        
    def _process_irrelevant(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the irrelevant files, it will delete the irrelevant files from the conversations folder"""
        
        deleted_files = []
        failed_deletes = []
        
        for file_path in state["irrelevant_files_path"]:
            try:
                file_to_delete = Path(file_path)
                if file_to_delete.exists():
                    os.remove(file_to_delete)
                    deleted_files.append(file_to_delete.name)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ— å…³æ–‡ä»¶: {file_to_delete.name}")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
                    
            except Exception as e:
                failed_deletes.append(Path(file_path).name)
                print(f"âŒ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
        
        # Clean up state lists - remove deleted files from all relevant lists ï¼ï¼ï¼ï¼ï¼ï¼ï¼ Might not be needed
        for file_path in state["irrelevant_files_path"]:
            # Remove from processed files lists
            if file_path in state.get("upload_files_processed_path", []):
                state["upload_files_processed_path"].remove(file_path)
            if file_path in state.get("new_upload_files_processed_path", []):
                state["new_upload_files_processed_path"].remove(file_path)
        
        # Create summary message
        summary_message = f"""ğŸ—‘ï¸ æ— å…³æ–‡ä»¶å¤„ç†å®Œæˆ:
        âœ… æˆåŠŸåˆ é™¤: {len(deleted_files)} ä¸ªæ–‡ä»¶
        âŒ åˆ é™¤å¤±è´¥: {len(failed_deletes)} ä¸ªæ–‡ä»¶

        åˆ é™¤çš„æ–‡ä»¶: {', '.join(deleted_files) if deleted_files else 'æ— '}
        å¤±è´¥çš„æ–‡ä»¶: {', '.join(failed_deletes) if failed_deletes else 'æ— '}"""
        
        return {
            "process_user_input_messages": [SystemMessage(content=summary_message)]
        }
    
    def _process_template(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the template files, it will analyze the template files and determine if it's a valid template"""
        
        template_files = state["uploaded_template_files_path"]
        
        # If multiple templates, ask user to choose
        if len(template_files) > 1:
            template_names = [Path(f).name for f in template_files]
            question = f"æ£€æµ‹åˆ°å¤šä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡æ¿ï¼š\n" + \
                      "\n".join([f"{i+1}. {name}" for i, name in enumerate(template_names)]) + \
                      "\nè¯·è¾“å…¥åºå·ï¼ˆå¦‚ï¼š1ï¼‰ï¼š"
            
            try:
                user_choice = self.request_user_clarification(question, "ç³»ç»Ÿéœ€è¦ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡æ¿æ–‡ä»¶è¿›è¡Œåç»­å¤„ç†")
                
                # Parse user choice
                try:
                    choice_index = int(user_choice.strip()) - 1
                    if 0 <= choice_index < len(template_files):
                        selected_template = template_files[choice_index]
                        # Remove non-selected templates
                        rejected_templates = [f for i, f in enumerate(template_files) if i != choice_index]
                        
                        # Delete rejected template files
                        for rejected_file in rejected_templates:
                            try:
                                Path(rejected_file).unlink()
                                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœªé€‰ä¸­çš„æ¨¡æ¿: {Path(rejected_file).name}")
                            except Exception as e:
                                print(f"âŒ åˆ é™¤æ¨¡æ¿æ–‡ä»¶å‡ºé”™: {e}")
                        
                        # Update state to only include selected template
                        template_files = [selected_template]
                        
                    else:
                        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                        selected_template = template_files[0]
                        template_files = [selected_template]
                        
                except ValueError:
                    print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                    selected_template = template_files[0]
                    template_files = [selected_template]
                    
            except Exception as e:
                print(f"âŒ ç”¨æˆ·é€‰æ‹©å‡ºé”™: {e}")
                selected_template = template_files[0]
                template_files = [selected_template]
        
        # Analyze the selected template for complexity
        template_file = template_files[0]
        
        try:
            source_path = Path(template_file)
            template_content = source_path.read_text(encoding='utf-8')
            
            # Create prompt to determine if template is complex or simple
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­è¿™ä¸ªè¡¨æ ¼æ¨¡æ¿æ˜¯å¤æ‚æ¨¡æ¿è¿˜æ˜¯ç®€å•æ¨¡æ¿ã€‚

            åˆ¤æ–­æ ‡å‡†ï¼š
            - **å¤æ‚æ¨¡æ¿**: è¡¨æ ¼åŒæ—¶åŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼Œå³æ—¢æœ‰è¡Œæ ‡é¢˜åˆæœ‰åˆ—æ ‡é¢˜çš„äºŒç»´è¡¨æ ¼ç»“æ„
            - **ç®€å•æ¨¡æ¿**: è¡¨æ ¼åªåŒ…å«åˆ—è¡¨å¤´ï¼Œæ¯è¡Œæ˜¯ç‹¬ç«‹çš„æ•°æ®è®°å½•

            æ¨¡æ¿å†…å®¹ï¼ˆHTMLæ ¼å¼ï¼‰ï¼š
            {template_content}

            è¯·ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œç„¶ååªå›å¤ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
            [Complex] - å¦‚æœæ˜¯å¤æ‚æ¨¡æ¿ï¼ˆåŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼‰
            [Simple] - å¦‚æœæ˜¯ç®€å•æ¨¡æ¿ï¼ˆåªåŒ…å«åˆ—è¡¨å¤´ï¼‰"""
            
            analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
            
            # Parse response
            response_content = analysis_response.content.strip()
            if "[Complex]" in response_content:
                template_type = "[Complex]"
            elif "[Simple]" in response_content:
                template_type = "[Simple]"
            else:
                # Default to Simple if unclear
                template_type = "[Simple]"
                print("âš ï¸ æ— æ³•ç¡®å®šæ¨¡æ¿ç±»å‹ï¼Œé»˜è®¤ä¸ºç®€å•æ¨¡æ¿")
            
            # Create analysis summary
            summary_message = f"""ğŸ“‹ æ¨¡æ¿åˆ†æå®Œæˆ:
            âœ… é€‰å®šæ¨¡æ¿: {Path(template_file).name}
            ğŸ” æ¨¡æ¿ç±»å‹: {template_type}
            ğŸ“ æ¨¡æ¿è·¯å¾„: {template_file}

            {template_type}"""
            
            return {
                "uploaded_template_files_path": template_files,  # Only selected template
                "process_user_input_messages": [SystemMessage(content=summary_message)]
            }
            
        except Exception as e:
            print(f"âŒ åˆ†ææ¨¡æ¿æ—¶å‡ºé”™: {e}")
            return {
                "uploaded_template_files_path": template_files,
                "process_user_input_messages": [SystemMessage(content=f"æ¨¡æ¿åˆ†æå‡ºé”™: {e}\né»˜è®¤ä¸º[Simple]")]
            }