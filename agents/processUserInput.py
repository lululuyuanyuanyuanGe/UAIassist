import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content, extract_filename
from utilities.modelRelated import invoke_model

import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

from langgraph.graph import StateGraph, END, START
from langgraph.constants import Send
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

load_dotenv()

class ProcessUserInputState(TypedDict):
    process_user_input_messages: Annotated[list[BaseMessage], add_messages]
    user_input: str
    upload_files_path: list[str] # Store all uploaded files
    new_upload_files_path: list[str] # Track the new uploaded files in this round
    new_upload_files_processed_path: list[str] # Store the processed new uploaded files
    uploaded_template_files_path: list[str]
    supplement_files_path: dict[str, list[str]]
    irrelevant_files_path: list[str]
    all_files_irrelevant: bool  # Flag to indicate all files are irrelevant
    text_input_validation: str  # Store validation result [Valid] or [Invalid]
    previous_AI_messages: list[BaseMessage]
    summary_message: str  # Add the missing field
    template_complexity: str

    
class ProcessUserInputAgent:

    @tool
    def request_user_clarification(question: str, context: str = "") -> str:
        """
        è¯¢é—®ç”¨æˆ·æ¾„æ¸…ï¼Œå’Œç”¨æˆ·ç¡®è®¤ï¼Œæˆ–è€…è¯¢é—®ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œå½“ä½ ä¸ç¡®å®šçš„æ—¶å€™è¯·è¯¢é—®ç”¨æˆ·

        å‚æ•°ï¼š
            question: é—®é¢˜
            contexnt: å¯é€‰è¡¥å……å†…å®¹ï¼Œè§£é‡Šä¸ºç”šæ¶é­”ä½ éœ€è¦ä¸€ä¸‹ä¿¡æ¯
        """
        prompt = f"{question}\n{context}"
        user_response = interrupt({"prompt": prompt})

        return user_response
    
    tools = [request_user_clarification]

    def __init__(self):
        self.memory = MemorySaver()
        self.graph = self._build_graph().compile(checkpointer=self.memory)


    def _build_graph(self) -> StateGraph:
        """This function will build the graph for the process user input agent"""
        graph = StateGraph(ProcessUserInputState)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("file_upload", self._file_upload)
        graph.add_node("analyze_uploaded_files", self._analyze_uploaded_files)
        graph.add_node("process_template", self._process_template)
        graph.add_node("process_supplement", self._process_supplement)
        graph.add_node("process_irrelevant", self._process_irrelevant)
        graph.add_node("analyze_text_input", self._analyze_text_input)
        graph.add_node("clarification_tool_node", ToolNode(self.tools))
        graph.add_node("summary_user_input", self._summary_user_input)
        
        graph.add_edge(START, "collect_user_input")

        graph.add_conditional_edges(
            "collect_user_input",
            self._route_after_collect_user_input,
            {
                "file_upload": "file_upload",
                "analyze_text_input": "analyze_text_input",
            }
        )

        graph.add_edge("file_upload", "analyze_uploaded_files")

        graph.add_conditional_edges(
            "analyze_uploaded_files",
            self._route_after_analyze_uploaded_files # Since we are using the send objects, we don't need to specify the edges
        )

        # After tool execution, re-analyze uploaded files with user input
        graph.add_edge("clarification_tool_node", "analyze_uploaded_files")

        graph.add_edge("process_template", "summary_user_input")
        graph.add_edge("process_supplement", "summary_user_input")
        graph.add_edge("process_irrelevant", "summary_user_input")

        graph.add_conditional_edges(
            "analyze_text_input",
            self._route_after_analyze_text_input,
            {
                "valid_text_input": "summary_user_input",
                "invalid_text_input": "collect_user_input",
            }
        )

        graph.add_edge("summary_user_input", END)
        return graph



    def create_initial_state(self, previous_AI_messages: list[BaseMessage] = None) -> ProcessUserInputState:
        """This function initializes the state of the process user input agent"""
        return {
            "process_user_input_messages": [],
            "user_input": "",
            "upload_files_path": [],
            "new_upload_files_path": [],
            "new_upload_files_processed_path": [],
            "uploaded_template_files_path": [],
            "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
            "irrelevant_files_path": [],
            "all_files_irrelevant": False,
            "text_input_validation": None,
            "previous_AI_messages": previous_AI_messages,
            "summary_message": "",
            "template_complexity": ""
        }


    def _collect_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This is the node where we get user's input"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _collect_user_input")
        print("=" * 50)
        print("âŒ¨ï¸ ç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        
        user_input = interrupt("ç”¨æˆ·ï¼š")
        
        print(f"ğŸ“¥ æ¥æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")
        print("âœ… _collect_user_input æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {
            "process_user_input_messages": [HumanMessage(content=user_input)],
            "user_input": user_input
        }



    def _route_after_collect_user_input(self, state: ProcessUserInputState) -> str:
        """This node act as a safety check node, it will analyze the user's input and determine if it's a valid input,
        based on the LLM's previous response, at the same time it will route the agent to the correct node"""
        
        # Extract content from the message object
        latest_message = state["process_user_input_messages"][-1]
        message_content = latest_message.content if hasattr(latest_message, 'content') else str(latest_message)
        
        # Check if there are files in the user input
        user_upload_files = detect_and_process_file_paths(message_content)
        if user_upload_files:
            # Files detected - route to file_upload 
            # Note: We cannot modify state in routing functions, so file_upload node will re-detect files
            return "file_upload"
        
        # User didn't upload any new files, we will analyze the text input
        return "analyze_text_input"



    def _file_upload(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will upload user's file to our system"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _file_upload")
        print("=" * 50)
        
        # Re-detect files from user input since routing functions cannot modify state
        latest_message = state["process_user_input_messages"][-1]
        message_content = latest_message.content if hasattr(latest_message, 'content') else str(latest_message)
        
        print("ğŸ“ æ­£åœ¨æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„...")
        detected_files = detect_and_process_file_paths(message_content)
        print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(detected_files)} ä¸ªæ–‡ä»¶")
        
        # Load data.json with error handling
        data_file = Path("agents/data.json")
        try:
            with open(data_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f"âš ï¸ data.jsonæ–‡ä»¶å‡ºé”™: {e}")
            # Initialize empty structure if file is missing or corrupted
            data = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        
        print("ğŸ” æ­£åœ¨æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨...")
        for file in detected_files:
            file_name = Path(file).name
            if file_name in data["è¡¨æ ¼"] or file_name in data["æ–‡æ¡£"]:
                detected_files.remove(file)
                print(f"âš ï¸ æ–‡ä»¶ {file} å·²å­˜åœ¨")
        
        if not detected_files:
            print("âš ï¸ æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "new_upload_files_path": [],
                "new_upload_files_processed_path": []
            }
        
        print(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(detected_files)} ä¸ªæ–°æ–‡ä»¶...")
        
        # Process the files using the correct session_id
        result = retrieve_file_content(detected_files, "files")
        
        print(f"âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {result}")
        print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        # Update state with new files
        # Safely handle the case where upload_files_path might not exist in state
        existing_files = state.get("upload_files_path", [])
        return {
            "new_upload_files_path": detected_files,
            "upload_files_path": existing_files + detected_files,
            "new_upload_files_processed_path": result
        }
    


    def _analyze_uploaded_files(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will analyze the user's uploaded files, it need to classify the file into template
        supplement, or irrelevant. If all files are irrelevant, it will flag for text analysis instead."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _analyze_uploaded_files")
        print("=" * 50)
        
        import json
        from pathlib import Path
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Initialize classification results
        classification_results = {
            "template": [],
            "supplement": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
            "irrelevant": []
        }
        
        # Process files one by one for better accuracy
        processed_files = []
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_to_process = state.get("new_upload_files_processed_path", [])
        
        print(f"ğŸ“ éœ€è¦åˆ†æçš„æ–‡ä»¶æ•°é‡: {len(new_files_to_process)}")
        
        if not new_files_to_process:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [],
                "all_files_irrelevant": True,  # Flag for routing to text analysis
                "process_user_input_messages": [SystemMessage(content="æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶ï¼Œå°†åˆ†æç”¨æˆ·æ–‡æœ¬è¾“å…¥")]
            }
        
        def analyze_single_file(file_path: str) -> tuple[str, str, str]:
            """Analyze a single file and return (file_path, classification, file_name)"""
            try:
                source_path = Path(file_path)
                print(f"ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶: {source_path.name}")
                
                if not source_path.exists():
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return file_path, "irrelevant", source_path.name
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:5000] if len(file_content) > 2000 else file_content
                
                # Create individual analysis prompt for this file
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹å¹¶è¿›è¡Œåˆ†ç±»ã€‚å…±æœ‰å››ç§ç±»å‹ï¼š

                1. **æ¨¡æ¿ç±»å‹ (template)**: ç©ºç™½è¡¨æ ¼æ¨¡æ¿ï¼Œåªæœ‰è¡¨å¤´æ²¡æœ‰å…·ä½“æ•°æ®
                2. **è¡¥å……è¡¨æ ¼ (supplement-è¡¨æ ¼)**: å·²å¡«å†™çš„å®Œæ•´è¡¨æ ¼ï¼Œç”¨äºè¡¥å……æ•°æ®åº“
                3. **è¡¥å……æ–‡æ¡£ (supplement-æ–‡æ¡£)**: åŒ…å«é‡è¦ä¿¡æ¯çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¦‚æ³•å¾‹æ¡æ–‡ã€æ”¿ç­–ä¿¡æ¯ç­‰
                4. **æ— å…³æ–‡ä»¶ (irrelevant)**: ä¸è¡¨æ ¼å¡«å†™æ— å…³çš„æ–‡ä»¶

                ä»”ç»†æ£€æŸ¥ä¸è¦æŠŠè¡¥å……æ–‡ä»¶é”™è¯¯åˆ’åˆ†ä¸ºæ¨¡æ¿æ–‡ä»¶åä¹‹äº¦ç„¶ï¼Œè¡¥å……æ–‡ä»¶é‡Œé¢æ˜¯æœ‰æ•°æ®çš„ï¼Œæ¨¡æ¿æ–‡ä»¶é‡Œé¢æ˜¯ç©ºçš„ï¼Œæˆ–è€…åªæœ‰ä¸€ä¸¤ä¸ªä¾‹å­æ•°æ®
                æ³¨æ„ï¼šæ‰€æœ‰æ–‡ä»¶å·²è½¬æ¢ä¸ºtxtæ ¼å¼ï¼Œè¡¨æ ¼ä»¥HTMLä»£ç å½¢å¼å‘ˆç°ï¼Œè¯·æ ¹æ®å†…å®¹è€Œéæ–‡ä»¶åæˆ–åç¼€åˆ¤æ–­ã€‚

                ç”¨æˆ·è¾“å…¥: {state.get("user_input", "")}

                å½“å‰åˆ†ææ–‡ä»¶:
                æ–‡ä»¶å: {source_path.name}
                æ–‡ä»¶è·¯å¾„: {file_path}
                æ–‡ä»¶å†…å®¹:
                {analysis_content}

                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œåªè¿”å›è¿™ä¸€ä¸ªæ–‡ä»¶çš„åˆ†ç±»ç»“æœï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
                {{
                    "classification": "template" | "supplement-è¡¨æ ¼" | "supplement-æ–‡æ¡£" | "irrelevant"
                }}"""
                
                # Get LLM analysis for this file
                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡ä»¶åˆ†ç±»...")
                analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])

                # Parse JSON response for this file
                try:
                    # Extract JSON from response
                    response_content = analysis_response.strip()
                    print(f"ğŸ“¥ LLMåˆ†ç±»å“åº”: {response_content}")
                    
                    # Remove markdown code blocks if present
                    if response_content.startswith('```'):
                        response_content = response_content.split('\n', 1)[1]
                        response_content = response_content.rsplit('\n', 1)[0]
                    
                    file_classification = json.loads(response_content)
                    classification_type = file_classification.get("classification", "irrelevant")
                    
                    print(f"âœ… æ–‡ä»¶ {source_path.name} åˆ†ç±»ä¸º: {classification_type}")
                    return file_path, classification_type, source_path.name
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ æ–‡ä»¶ {source_path.name} JSONè§£æé”™è¯¯: {e}")
                    print(f"LLMå“åº”: {analysis_response}")
                    # Fallback: mark as irrelevant for safety
                    return file_path, "irrelevant", source_path.name
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
                # Return irrelevant on error
                return file_path, "irrelevant", Path(file_path).name
        
        # Use ThreadPoolExecutor for parallel processing
        max_workers = min(len(new_files_to_process), 5)  # Limit to 5 concurrent requests
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file analysis tasks
            future_to_file = {
                executor.submit(analyze_single_file, file_path): file_path 
                for file_path in new_files_to_process
            }
            
            # Process completed tasks as they finish
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_path_result, classification_type, file_name = future.result()
                    
                    # Add to appropriate category
                    if classification_type == "template":
                        classification_results["template"].append(file_path_result)
                    elif classification_type == "supplement-è¡¨æ ¼":
                        classification_results["supplement"]["è¡¨æ ¼"].append(file_path_result)
                    elif classification_type == "supplement-æ–‡æ¡£":
                        classification_results["supplement"]["æ–‡æ¡£"].append(file_path_result)
                    else:  # irrelevant or unknown
                        classification_results["irrelevant"].append(file_path_result)
                    
                    processed_files.append(file_name)
                    
                except Exception as e:
                    print(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    # Add to irrelevant on error
                    classification_results["irrelevant"].append(file_path)
        
        print(f"ğŸ‰ å¹¶è¡Œæ–‡ä»¶åˆ†æå®Œæˆ:")
        print(f"  - æ¨¡æ¿æ–‡ä»¶: {len(classification_results['template'])} ä¸ª")
        print(f"  - è¡¥å……è¡¨æ ¼: {len(classification_results['supplement']['è¡¨æ ¼'])} ä¸ª")
        print(f"  - è¡¥å……æ–‡æ¡£: {len(classification_results['supplement']['æ–‡æ¡£'])} ä¸ª")
        print(f"  - æ— å…³æ–‡ä»¶: {len(classification_results['irrelevant'])} ä¸ª")
        print(f"  - æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ªæ–‡ä»¶")
        
        if not processed_files and not classification_results["irrelevant"]:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [],
                "all_files_irrelevant": True,  # Flag for routing to text analysis
                "process_user_input_messages": [SystemMessage(content="æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶ï¼Œå°†åˆ†æç”¨æˆ·æ–‡æœ¬è¾“å…¥")]
            }
        
        # Update state with classification results
        uploaded_template_files = classification_results.get("template", [])
        supplement_files = classification_results.get("supplement", {"è¡¨æ ¼": [], "æ–‡æ¡£": []})
        irrelevant_files = classification_results.get("irrelevant", [])
        
        # Check if all files are irrelevant
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_processed_count = len(state.get("new_upload_files_processed_path", []))
        all_files_irrelevant = (
            len(uploaded_template_files) == 0 and 
            len(supplement_files.get("è¡¨æ ¼", [])) == 0 and 
            len(supplement_files.get("æ–‡æ¡£", [])) == 0 and
            len(irrelevant_files) == new_files_processed_count
        )
        
        if all_files_irrelevant:
            print("âš ï¸ æ‰€æœ‰æ–‡ä»¶éƒ½è¢«åˆ†ç±»ä¸ºæ— å…³æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": irrelevant_files,
                "all_files_irrelevant": True,  # Flag for routing
            }
        else:
            # Some files are relevant, proceed with normal flow
            analysis_summary = f"""æ–‡ä»¶åˆ†æå®Œæˆ:
            æ¨¡æ¿æ–‡ä»¶: {len(uploaded_template_files)} ä¸ª
            è¡¥å……è¡¨æ ¼: {len(supplement_files.get("è¡¨æ ¼", []))} ä¸ª  
            è¡¥å……æ–‡æ¡£: {len(supplement_files.get("æ–‡æ¡£", []))} ä¸ª
            æ— å…³æ–‡ä»¶: {len(irrelevant_files)} ä¸ª"""
            
            print("âœ… æ–‡ä»¶åˆ†æå®Œæˆï¼Œå­˜åœ¨æœ‰æ•ˆæ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "uploaded_template_files_path": uploaded_template_files,
                "supplement_files_path": supplement_files,
                "irrelevant_files_path": irrelevant_files,
                "all_files_irrelevant": False,  # Flag for routing
                "process_user_input_messages": [SystemMessage(content=analysis_summary)]
            }
                
    def _route_after_analyze_uploaded_files(self, state: ProcessUserInputState):
        """Route after analyzing uploaded files. Uses Send objects for all routing."""
        print("Debug: route_after_analyze_uploaded_files")
        # Check if LLM request a tool call
        latest_message = state["process_user_input_messages"][-1]
        if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
            return [Send("clarification_tool_node", state)]
        
        # Check if all files are irrelevant - route to text analysis
        if state.get("all_files_irrelevant", False):
            # First clean up irrelevant files, then analyze text
            sends = []
            if state.get("irrelevant_files_path"):
                sends.append(Send("process_irrelevant", state))
            sends.append(Send("analyze_text_input", state))
            return sends
        
        # Some files are relevant - process them in parallel
        sends = []
        if state.get("uploaded_template_files_path"):
            print("Debug: process_template")
            sends.append(Send("process_template", state))
        if state.get("supplement_files_path", {}).get("è¡¨æ ¼") or state.get("supplement_files_path", {}).get("æ–‡æ¡£"):
            print("Debug: process_supplement")
            sends.append(Send("process_supplement", state))
        if state.get("irrelevant_files_path"):
            print("Debug: process_irrelevant")
            sends.append(Send("process_irrelevant", state))

        # The parallel nodes will automatically converge, then continue to analyze_text_input
        return sends if sends else [Send("analyze_text_input", state)]  # Fallback
    
    def _process_supplement(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the supplement files, it will analyze the supplement files and summarize the content of the files as well as stored the summary in data.json"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_supplement")
        print("=" * 50)
        print("Debug: Start to process_supplement")
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Load existing data.json with better error handling
        data_json_path = Path("agents/data.json")
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Ensure the structure exists
                if "è¡¨æ ¼" not in data:
                    data["è¡¨æ ¼"] = {}
                if "æ–‡æ¡£" not in data:
                    data["æ–‡æ¡£"] = {}
        except FileNotFoundError:
            print("ğŸ“ data.jsonä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„æ•°æ®ç»“æ„")
            data = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        except json.JSONDecodeError as e:
            print(f"âš ï¸ data.jsonæ ¼å¼é”™è¯¯: {e}")
            print("ğŸ“ å¤‡ä»½åŸæ–‡ä»¶å¹¶åˆ›å»ºæ–°çš„æ•°æ®ç»“æ„")
            # Backup the corrupted file
            backup_path = data_json_path.with_suffix('.json.backup')
            if data_json_path.exists():
                data_json_path.rename(backup_path)
                print(f"ğŸ“¦ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
            data = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        
        table_files = state["supplement_files_path"]["è¡¨æ ¼"]
        document_files = state["supplement_files_path"]["æ–‡æ¡£"]
        
        print(f"ğŸ“Š éœ€è¦å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ª")
        print(f"ğŸ“„ éœ€è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶: {len(document_files)} ä¸ª")
        
        # Collect new messages instead of directly modifying state
        new_messages = []
        
        def process_table_file(table_file: str) -> tuple[str, str, dict]:
            """Process a single table file and return (file_path, file_type, result_data)"""
            try:
                source_path = Path(table_file)
                print(f"ğŸ” æ­£åœ¨å¤„ç†è¡¨æ ¼æ–‡ä»¶: {source_path.name}")
                
                file_content = source_path.read_text(encoding='utf-8')
                file_content = file_content[:2000] if len(file_content) > 2000 else file_content
                file_name = extract_filename(table_file)
                
                # Define the JSON template separately to avoid f-string nesting issues
                json_template = '''{{
  "{file_name}": {{
    "è¡¨æ ¼ç»“æ„": {{
      "é¡¶å±‚è¡¨å¤´åç§°": {{
        "äºŒçº§è¡¨å¤´åç§°": [
          "å­—æ®µ1",
          "å­—æ®µ2",
          "..."
        ],
        "æ›´å¤šå­è¡¨å¤´": [
          "å­—æ®µA",
          "å­—æ®µB"
        ]
      }}
    }},
    "è¡¨æ ¼æ€»ç»“": "è¯¥è¡¨æ ¼çš„ä¸»è¦ç”¨é€”åŠå†…å®¹è¯´æ˜..."
  }}
}}'''.format(file_name=file_name)

                system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·é˜…è¯»ç”¨æˆ·ä¸Šä¼ çš„ HTML æ ¼å¼çš„ Excel æ–‡ä»¶ï¼Œå¹¶å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. æå–è¡¨æ ¼çš„å¤šçº§è¡¨å¤´ç»“æ„ï¼›
   - ä½¿ç”¨åµŒå¥—çš„ key-value å½¢å¼è¡¨ç¤ºå±‚çº§å…³ç³»ï¼›
   - æ¯ä¸€çº§è¡¨å¤´åº”ä»¥å¯¹è±¡å½¢å¼å±•ç¤ºå…¶å­çº§å­—æ®µæˆ–å­è¡¨å¤´ï¼›
   - ä¸éœ€è¦é¢å¤–å­—æ®µï¼ˆå¦‚ nullã€isParent ç­‰ï¼‰ï¼Œä»…ä¿ç•™ç»“æ„æ¸…æ™°çš„å±‚çº§æ˜ å°„ï¼›

2. æä¾›ä¸€ä¸ªå¯¹è¯¥è¡¨æ ¼å†…å®¹çš„ç®€è¦æ€»ç»“ï¼›
   - å†…å®¹åº”åŒ…æ‹¬è¡¨æ ¼ç”¨é€”ã€ä¸»è¦ä¿¡æ¯ç±»åˆ«ã€é€‚ç”¨èŒƒå›´ç­‰ï¼›
   - è¯­è¨€ç®€æ´ï¼Œä¸è¶…è¿‡ 150 å­—ï¼›

è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
{json_template}

è¯·å¿½ç•¥æ‰€æœ‰ HTML æ ·å¼æ ‡ç­¾ï¼Œåªå…³æ³¨è¡¨æ ¼ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

æ–‡ä»¶å†…å®¹:
{file_content}"""

                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè¡¨æ ¼åˆ†æ...")
                
                try:
                    analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])
                    print("ğŸ“¥ è¡¨æ ¼åˆ†æå“åº”æ¥æ”¶æˆåŠŸ")
                except Exception as llm_error:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_error}")
                    # Create fallback response  
                    analysis_response = f"è¡¨æ ¼æ–‡ä»¶åˆ†æå¤±è´¥: {str(llm_error)}ï¼Œæ–‡ä»¶å: {source_path.name}"
                
                # Create result data
                result_data = {
                    "file_key": source_path.name,
                    "new_entry": {
                        "summary": analysis_response,
                        "file_path": str(table_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": source_path.stat().st_size
                    },
                    "analysis_response": analysis_response
                }
                
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                return table_file, "table", result_data
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¡¨æ ¼æ–‡ä»¶å‡ºé”™ {table_file}: {e}")
                return table_file, "table", {
                    "file_key": Path(table_file).name,
                    "new_entry": {
                        "summary": f"è¡¨æ ¼æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
                        "file_path": str(table_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": 0
                    },
                    "analysis_response": f"è¡¨æ ¼æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                }

        def process_document_file(document_file: str) -> tuple[str, str, dict]:
            """Process a single document file and return (file_path, file_type, result_data)"""
            try:
                source_path = Path(document_file)
                print(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡æ¡£æ–‡ä»¶: {source_path.name}")
                
                file_content = source_path.read_text(encoding='utf-8')
                file_content = file_content[:2000] if len(file_content) > 2000 else file_content
                file_name = extract_filename(document_file)
                
                system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œå…·å¤‡æ³•å¾‹ä¸æ”¿ç­–è§£è¯»èƒ½åŠ›ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»ç”¨æˆ·æä¾›çš„ HTML æ ¼å¼æ–‡ä»¶ï¼Œå¹¶ä»ä¸­æå–å‡ºæœ€é‡è¦çš„ 1-2 æ¡å…³é”®ä¿¡æ¯è¿›è¡Œæ€»ç»“ï¼Œæ— éœ€æå–å…¨éƒ¨å†…å®¹ã€‚

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. å¿½ç•¥æ‰€æœ‰ HTML æ ‡ç­¾ï¼ˆå¦‚ <p>ã€<div>ã€<table> ç­‰ï¼‰ï¼Œåªå…³æ³¨æ–‡æœ¬å†…å®¹ï¼›

2. ä»æ–‡ä»¶ä¸­æå–ä½ è®¤ä¸ºæœ€é‡è¦çš„ä¸€åˆ°ä¸¤é¡¹æ ¸å¿ƒæ”¿ç­–ä¿¡æ¯ï¼ˆä¾‹å¦‚è¡¥è´´é‡‘é¢ã€é€‚ç”¨å¯¹è±¡ã€å®¡æ‰¹æµç¨‹ç­‰ï¼‰ï¼Œæˆ–è€…å…¶ä»–ä½ è§‰å¾—é‡è¦çš„ä¿¡æ¯ï¼Œé¿å…åŒ…å«æ¬¡è¦æˆ–é‡å¤å†…å®¹ï¼›

3. å¯¹æå–çš„ä¿¡æ¯è¿›è¡Œç»“æ„åŒ–æ€»ç»“ï¼Œè¯­è¨€æ­£å¼ã€é€»è¾‘æ¸…æ™°ã€ç®€æ´æ˜äº†ï¼›

4. è¾“å‡ºæ ¼å¼ä¸ºä¸¥æ ¼çš„ JSONï¼š
   {{
     "{file_name}": "å†…å®¹æ€»ç»“"
   }}

5. è‹¥æä¾›å¤šä¸ªæ–‡ä»¶ï¼Œéœ€åˆ†åˆ«å¤„ç†å¹¶åˆå¹¶è¾“å‡ºä¸ºä¸€ä¸ª JSON å¯¹è±¡ï¼›

6. è¾“å‡ºè¯­è¨€åº”ä¸è¾“å…¥æ–‡æ¡£ä¿æŒä¸€è‡´ï¼ˆè‹¥æ–‡æ¡£ä¸ºä¸­æ–‡ï¼Œåˆ™è¾“å‡ºä¸­æ–‡ï¼‰ï¼›

è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œå¯¹æä¾›çš„ HTML æ–‡ä»¶å†…å®¹è¿›è¡Œåˆ†æå¹¶è¿”å›ç»“æœã€‚

æ–‡ä»¶å†…å®¹:
{file_content}
""".format(file_name=file_name, file_content=file_content)

                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡æ¡£åˆ†æ...")
                
                try:
                    analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])
                    print("ğŸ“¥ æ–‡æ¡£åˆ†æå“åº”æ¥æ”¶æˆåŠŸ")
                except Exception as llm_error:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_error}")
                    # Create fallback response
                    analysis_response = f"æ–‡æ¡£æ–‡ä»¶åˆ†æå¤±è´¥: {str(llm_error)}ï¼Œæ–‡ä»¶å: {source_path.name}"

                # Create result data
                result_data = {
                    "file_key": source_path.name,
                    "new_entry": {
                        "summary": analysis_response,
                        "file_path": str(document_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": source_path.stat().st_size
                    },
                    "analysis_response": analysis_response
                }
                
                print(f"âœ… æ–‡æ¡£æ–‡ä»¶å·²åˆ†æ: {source_path.name}")
                return document_file, "document", result_data
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡æ¡£æ–‡ä»¶å‡ºé”™ {document_file}: {e}")
                return document_file, "document", {
                    "file_key": Path(document_file).name,
                    "new_entry": {
                        "summary": f"æ–‡æ¡£æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
                        "file_path": str(document_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": 0
                    },
                    "analysis_response": f"æ–‡æ¡£æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                }

        # Use ThreadPoolExecutor for parallel processing
        all_files = [(file, "table") for file in table_files] + [(file, "document") for file in document_files]
        total_files = len(all_files)
        
        if total_files == 0:
            print("âš ï¸ æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†")
            print("âœ… _process_supplement æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {"process_user_input_messages": new_messages}
        
        max_workers = min(total_files, 4)  # Limit to 4 concurrent requests for supplement processing
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†è¡¥å……æ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file processing tasks
            future_to_file = {}
            for file_path, file_type in all_files:
                if file_type == "table":
                    future = executor.submit(process_table_file, file_path)
                else:  # document
                    future = executor.submit(process_document_file, file_path)
                future_to_file[future] = (file_path, file_type)
            
            # Process completed tasks as they finish
            for future in as_completed(future_to_file):
                file_path, file_type = future_to_file[future]
                try:
                    processed_file_path, processed_file_type, result_data = future.result()
                    
                    # Add to new_messages
                    new_messages.append(AIMessage(content=result_data["analysis_response"]))
                    
                    # Update data.json structure
                    file_key = result_data["file_key"]
                    new_entry = result_data["new_entry"]
                    
                    if processed_file_type == "table":
                        if file_key in data["è¡¨æ ¼"]:
                            print(f"âš ï¸ è¡¨æ ¼æ–‡ä»¶ {file_key} å·²å­˜åœ¨ï¼Œå°†æ›´æ–°å…¶å†…å®¹")
                            # Preserve any additional fields that might exist
                            existing_entry = data["è¡¨æ ¼"][file_key]
                            for key, value in existing_entry.items():
                                if key not in new_entry:
                                    new_entry[key] = value
                        else:
                            print(f"ğŸ“ æ·»åŠ æ–°çš„è¡¨æ ¼æ–‡ä»¶: {file_key}")
                        data["è¡¨æ ¼"][file_key] = new_entry
                    else:  # document
                        if file_key in data["æ–‡æ¡£"]:
                            print(f"âš ï¸ æ–‡æ¡£æ–‡ä»¶ {file_key} å·²å­˜åœ¨ï¼Œå°†æ›´æ–°å…¶å†…å®¹")
                            # Preserve any additional fields that might exist
                            existing_entry = data["æ–‡æ¡£"][file_key]
                            for key, value in existing_entry.items():
                                if key not in new_entry:
                                    new_entry[key] = value
                        else:
                            print(f"ğŸ“ æ·»åŠ æ–°çš„æ–‡æ¡£æ–‡ä»¶: {file_key}")
                        data["æ–‡æ¡£"][file_key] = new_entry
                    
                except Exception as e:
                    print(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    # Create fallback entry
                    fallback_response = f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                    new_messages.append(AIMessage(content=fallback_response))
        
        print(f"ğŸ‰ å¹¶è¡Œæ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶")
        
        # Save updated data.json with atomic write
        try:
            # Write to a temporary file first to prevent corruption
            temp_path = data_json_path.with_suffix('.json.tmp')
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            # Atomic rename to replace the original file
            temp_path.replace(data_json_path)
            print(f"âœ… å·²æ›´æ–° data.jsonï¼Œè¡¨æ ¼æ–‡ä»¶ {len(data['è¡¨æ ¼'])} ä¸ªï¼Œæ–‡æ¡£æ–‡ä»¶ {len(data['æ–‡æ¡£'])} ä¸ª")
            
            # Log the files that were processed in this batch
            if table_files:
                print(f"ğŸ“Š æœ¬æ‰¹æ¬¡å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {[Path(f).name for f in table_files]}")
            if document_files:
                print(f"ğŸ“„ æœ¬æ‰¹æ¬¡å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶: {[Path(f).name for f in document_files]}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜ data.json æ—¶å‡ºé”™: {e}")
            # Clean up temp file if it exists
            temp_path = data_json_path.with_suffix('.json.tmp')
            if temp_path.exists():
                try:
                    temp_path.unlink()
                    print("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as cleanup_error:
                    print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
        
        print("âœ… _process_supplement æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        # Return the collected messages for proper state update
        return {"process_user_input_messages": new_messages}
        
        
    def _process_irrelevant(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the irrelevant files, it will delete the irrelevant files from the conversations folder"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_irrelevant")
        print("=" * 50)
        
        irrelevant_files = state["irrelevant_files_path"]
        print(f"ğŸ—‘ï¸ éœ€è¦åˆ é™¤çš„æ— å…³æ–‡ä»¶æ•°é‡: {len(irrelevant_files)}")
        
        deleted_files = []
        failed_deletes = []
        
        for file_path in irrelevant_files:
            try:
                file_to_delete = Path(file_path)
                print(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤: {file_to_delete.name}")
                
                if file_to_delete.exists():
                    os.remove(file_to_delete)
                    deleted_files.append(file_to_delete.name)
                    print(f"âœ… å·²åˆ é™¤æ— å…³æ–‡ä»¶: {file_to_delete.name}")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
                    
            except Exception as e:
                failed_deletes.append(Path(file_path).name)
                print(f"âŒ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")

        print(f"ğŸ“Š åˆ é™¤ç»“æœ: æˆåŠŸ {len(deleted_files)} ä¸ªï¼Œå¤±è´¥ {len(failed_deletes)} ä¸ª")
        print("âœ… _process_irrelevant æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {}  # Return empty dict since this node doesn't need to update any state keys

    
    def _process_template(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the template files, it will analyze the template files and determine if it's a valid template"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_template")
        print("=" * 50)
        
        template_files = state["uploaded_template_files_path"]
        print(f"ğŸ“‹ éœ€è¦å¤„ç†çš„æ¨¡æ¿æ–‡ä»¶æ•°é‡: {len(template_files)}")
        
        # If multiple templates, ask user to choose
        if len(template_files) > 1:
            print("âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©")
            template_names = [Path(f).name for f in template_files]
            question = f"æ£€æµ‹åˆ°å¤šä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡æ¿ï¼š\n" + \
                      "\n".join([f"{i+1}. {name}" for i, name in enumerate(template_names)]) + \
                      "\nè¯·è¾“å…¥åºå·ï¼ˆå¦‚ï¼š1ï¼‰ï¼š"
            
            try:
                print("ğŸ¤ æ­£åœ¨è¯·æ±‚ç”¨æˆ·ç¡®è®¤æ¨¡æ¿é€‰æ‹©...")
                user_choice = self.request_user_clarification(question, "ç³»ç»Ÿéœ€è¦ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡æ¿æ–‡ä»¶è¿›è¡Œåç»­å¤„ç†")
                
                # Parse user choice
                try:
                    choice_index = int(user_choice.strip()) - 1
                    if 0 <= choice_index < len(template_files):
                        selected_template = template_files[choice_index]
                        # Remove non-selected templates
                        rejected_templates = [f for i, f in enumerate(template_files) if i != choice_index]
                        
                        # Delete rejected template files
                        for rejected_file in rejected_templates:
                            try:
                                Path(rejected_file).unlink()
                                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœªé€‰ä¸­çš„æ¨¡æ¿: {Path(rejected_file).name}")
                            except Exception as e:
                                print(f"âŒ åˆ é™¤æ¨¡æ¿æ–‡ä»¶å‡ºé”™: {e}")
                        
                        # Update state to only include selected template
                        template_files = [selected_template]
                        print(f"âœ… ç”¨æˆ·é€‰æ‹©äº†æ¨¡æ¿: {Path(selected_template).name}")
                        
                    else:
                        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                        selected_template = template_files[0]
                        template_files = [selected_template]
                        
                except ValueError:
                    print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                    selected_template = template_files[0]
                    template_files = [selected_template]
                    
            except Exception as e:
                print(f"âŒ ç”¨æˆ·é€‰æ‹©å‡ºé”™: {e}")
                selected_template = template_files[0]
                template_files = [selected_template]
        
        # Analyze the selected template for complexity
        template_file = template_files[0]
        print(f"ğŸ” æ­£åœ¨åˆ†ææ¨¡æ¿å¤æ‚åº¦: {Path(template_file).name}")
        
        try:
            source_path = Path(template_file)
            template_content = source_path.read_text(encoding='utf-8')
            
            # Create prompt to determine if template is complex or simple
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­è¿™ä¸ªè¡¨æ ¼æ¨¡æ¿æ˜¯å¤æ‚æ¨¡æ¿è¿˜æ˜¯ç®€å•æ¨¡æ¿ã€‚

            åˆ¤æ–­æ ‡å‡†ï¼š
            - **å¤æ‚æ¨¡æ¿**: è¡¨æ ¼åŒæ—¶åŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼Œå³æ—¢æœ‰è¡Œæ ‡é¢˜åˆæœ‰åˆ—æ ‡é¢˜çš„äºŒç»´è¡¨æ ¼ç»“æ„
            - **ç®€å•æ¨¡æ¿**: è¡¨æ ¼åªåŒ…å«åˆ—è¡¨å¤´æˆ–è€…åªåŒ…å«è¡Œè¡¨å¤´ï¼Œä½†æ˜¯å¯ä»¥æ˜¯å¤šçº§è¡¨å¤´ï¼Œæ¯è¡Œæ˜¯ç‹¬ç«‹çš„æ•°æ®è®°å½•

            æ¨¡æ¿å†…å®¹ï¼ˆHTMLæ ¼å¼ï¼‰ï¼š
            {template_content}

            è¯·ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œç„¶ååªå›å¤ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
            [Complex] - å¦‚æœæ˜¯å¤æ‚æ¨¡æ¿ï¼ˆåŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼‰
            [Simple] - å¦‚æœæ˜¯ç®€å•æ¨¡æ¿ï¼ˆåªåŒ…å«åˆ—è¡¨å¤´ï¼‰"""
            

            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ¨¡æ¿å¤æ‚åº¦åˆ†æ...")
            
            analysis_response = invoke_model(model_name="Qwen/Qwen3-32B", messages=[SystemMessage(content=system_prompt)])
            
            # Extract the classification from the response
            if "[Complex]" in analysis_response:
                template_type = "[Complex]"
            elif "[Simple]" in analysis_response:
                template_type = "[Simple]"
            else:
                template_type = "[Simple]"  # Default fallback
                
            print(f"ğŸ“¥ æ¨¡æ¿åˆ†æç»“æœ: {template_type}")
            print("âœ… _process_template æ‰§è¡Œå®Œæˆ")
            print("=" * 50)

            return {"template_complexity": template_type,
                    "uploaded_template_files_path": [template_file]
                    }

        except Exception as e:
            print(f"âŒ æ¨¡æ¿åˆ†æLLMè°ƒç”¨å‡ºé”™: {e}")
            # Default to Simple if analysis fails
            template_type = "[Simple]"
            print("âš ï¸ æ¨¡æ¿åˆ†æå¤±è´¥ï¼Œé»˜è®¤ä¸ºç®€å•æ¨¡æ¿")
            print("âœ… _process_template æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "template_complexity": template_type,
                "uploaded_template_files_path": [template_file]
            }
        


    def _analyze_text_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node performs a safety check on user text input when all uploaded files are irrelevant.
        It validates if the user input contains meaningful table/Excel-related content.
        Returns [Valid] or [Invalid] based on the analysis."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _analyze_text_input")
        print("=" * 50)
        
        user_input = state["user_input"]
        print(f"ğŸ“ æ­£åœ¨åˆ†æç”¨æˆ·æ–‡æœ¬è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")
        
        if not user_input or user_input.strip() == "":
            print("âŒ ç”¨æˆ·è¾“å…¥ä¸ºç©º")
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content="âŒ ç”¨æˆ·è¾“å…¥ä¸ºç©ºï¼ŒéªŒè¯å¤±è´¥")]
            }
        
        # Create validation prompt for text input safety check
        # Get the previous AI message content safely
        previous_ai_content = ""
        if state.get("previous_AI_messages") and len(state["previous_AI_messages"]) > 0:
            latest_ai_msg = state["previous_AI_messages"][-1]
            if hasattr(latest_ai_msg, 'content'):
                previous_ai_content = latest_ai_msg.content
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¾“å…¥éªŒè¯ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥æ˜¯å¦ä¸è¡¨æ ¼ç”Ÿæˆã€Excelå¤„ç†ç›¸å…³ï¼Œå¹¶ä¸”æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œä½ çš„åˆ¤æ–­éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œ
        æˆ‘ä¼šæä¾›ä¸Šä¸€ä¸ªAIçš„å›å¤ï¼Œä»¥åŠç”¨æˆ·è¾“å…¥ï¼Œä½ éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸è¡¨æ ¼ç”Ÿæˆã€Excelå¤„ç†ç›¸å…³ï¼Œå¹¶ä¸”æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹ã€‚
        
        ä¸Šä¸€ä¸ªAIçš„å›å¤: {previous_ai_content}
        ç”¨æˆ·è¾“å…¥: {user_input}

        éªŒè¯æ ‡å‡†ï¼š
        1. **æœ‰æ•ˆè¾“å…¥ [Valid]**:
           - æ˜ç¡®æåˆ°éœ€è¦ç”Ÿæˆè¡¨æ ¼ã€å¡«å†™è¡¨æ ¼ã€Excelç›¸å…³æ“ä½œ
           - åŒ…å«å…·ä½“çš„è¡¨æ ¼è¦æ±‚ã€æ•°æ®æè¿°ã€å­—æ®µä¿¡æ¯
           - è¯¢é—®è¡¨æ ¼æ¨¡æ¿ã€è¡¨æ ¼æ ¼å¼ç›¸å…³é—®é¢˜
           - æä¾›äº†è¡¨æ ¼ç›¸å…³çš„æ•°æ®æˆ–ä¿¡æ¯

        2. **æ— æ•ˆè¾“å…¥ [Invalid]**:
           - å®Œå…¨ä¸è¡¨æ ¼/Excelæ— å…³çš„å†…å®¹
           - åƒåœ¾æ–‡æœ¬ã€éšæœºå­—ç¬¦ã€æ— æ„ä¹‰å†…å®¹
           - ç©ºç™½æˆ–åªæœ‰æ ‡ç‚¹ç¬¦å·
           - æ˜æ˜¾çš„æµ‹è¯•è¾“å…¥æˆ–æ— å…³é—®é¢˜

        è¯·ä»”ç»†åˆ†æç”¨æˆ·è¾“å…¥ï¼Œç„¶ååªå›å¤ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
        [Valid] - å¦‚æœè¾“å…¥ä¸è¡¨æ ¼ç›¸å…³ä¸”æœ‰æ„ä¹‰
        [Invalid] - å¦‚æœè¾“å…¥æ— å…³æˆ–æ— æ„ä¹‰"""
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡æœ¬è¾“å…¥éªŒè¯...")
            # Get LLM validation
            validation_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])
            # validation_response = self.llm_s.invoke([SystemMessage(content=system_prompt)])
            
            print(f"ğŸ“¥ éªŒè¯å“åº”: {validation_response}")
            
            if "[Valid]" in validation_response:
                validation_result = "[Valid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯é€šè¿‡ - å†…å®¹ä¸è¡¨æ ¼ç›¸å…³ä¸”æœ‰æ„ä¹‰"
            elif "[Invalid]" in validation_response:
                validation_result = "[Invalid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯å¤±è´¥ - å†…å®¹ä¸è¡¨æ ¼æ— å…³æˆ–æ— æ„ä¹‰"
            else:
                # Default to Invalid for safety
                validation_result = "[Invalid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯å¤±è´¥ - æ— æ³•ç¡®å®šè¾“å…¥æœ‰æ•ˆæ€§ï¼Œé»˜è®¤ä¸ºæ— æ•ˆ"
                print(f"âš ï¸ æ— æ³•è§£æéªŒè¯ç»“æœï¼ŒLLMå“åº”: {validation_response}")
            
            print(f"ğŸ“Š éªŒè¯ç»“æœ: {validation_result}")
            print(f"ğŸ“‹ çŠ¶æ€è¯´æ˜: {status_message}")
            
            # Create validation summary
            summary_message = f"""æ–‡æœ¬è¾“å…¥å®‰å…¨æ£€æŸ¥å®Œæˆ:
            
            **ç”¨æˆ·è¾“å…¥**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            **éªŒè¯ç»“æœ**: {validation_result}
            **çŠ¶æ€**: {status_message}"""
            
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "text_input_validation": validation_result,
                "process_user_input_messages": [SystemMessage(content=summary_message)]
            }
                
        except Exception as e:
            print(f"âŒ éªŒè¯æ–‡æœ¬è¾“å…¥æ—¶å‡ºé”™: {e}")
            
            # Default to Invalid for safety when there's an error
            error_message = f"""âŒ æ–‡æœ¬è¾“å…¥éªŒè¯å‡ºé”™: {e}
            
            ğŸ“„ **ç”¨æˆ·è¾“å…¥**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            ğŸ”’ **å®‰å…¨æªæ–½**: é»˜è®¤æ ‡è®°ä¸ºæ— æ•ˆè¾“å…¥"""
            
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ (å‡ºé”™)")
            print("=" * 50)
            
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content=error_message)]
            }



    def _route_after_analyze_text_input(self, state: ProcessUserInputState) -> str:
        """Route after text input validation based on [Valid] or [Invalid] result."""
        
        validation_result = state.get("text_input_validation", "[Invalid]")
        
        if validation_result == "[Valid]":
            # Text input is valid and table-related, proceed to summary
            return "valid_text_input"
        else:
            # Text input is invalid, route back to collect user input
            return "invalid_text_input"
        

    
    def _summary_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """Summary node that consolidates all information from this round and determines next routing."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _summary_user_input")
        print("=" * 50)
        
        print(f"ğŸ”„ å¼€å§‹æ€»ç»“ç”¨æˆ·è¾“å…¥ï¼Œå½“å‰æ¶ˆæ¯æ•°: {len(state.get('process_user_input_messages', []))}")
        
        # Extract content from all messages in this processing round
        process_user_input_messages_content =("\n").join([item.content for item in state["process_user_input_messages"]])
        print(f"ğŸ“ å¤„ç†çš„æ¶ˆæ¯å†…å®¹é•¿åº¦: {len(process_user_input_messages_content)} å­—ç¬¦")
        
        # Determine route decision based on template complexity (with proper parsing)
        template_complexity = state.get("template_complexity", "")
        print(f"ğŸ” åŸå§‹æ¨¡æ¿å¤æ‚åº¦: {repr(template_complexity)}")
        template_complexity = template_complexity.strip()
        print(f"ğŸ” æ¸…ç†åæ¨¡æ¿å¤æ‚åº¦: '{template_complexity}'")
        
        if "[Complex]" in template_complexity:
            route_decision = "complex_template"
        elif "[Simple]" in template_complexity:
            route_decision = "simple_template"
        else:
            route_decision = "previous_node"
        
        print(f"ğŸ¯ è·¯ç”±å†³å®š: {route_decision}")
        
        system_prompt = f"""
        æ ¹æ®å†å²å¯¹è¯æ€»ç»“è¿™è½®ç”¨æˆ·ä¿¡æ¯æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·éƒ½æä¾›äº†å“ªäº›æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡ä»¶ä¸Šä¼ ï¼Œæ–‡æœ¬è¾“å…¥ï¼Œæ¨¡æ¿ä¸Šä¼ ç­‰
        å†å²å¯¹è¯: {process_user_input_messages_content}ï¼Œ
        è¯·åªè¿”å›JSONæ ¼å¼ï¼Œæ— å…¶ä»–æ–‡å­—ï¼š
        {{
            "summary": "ç”¨æˆ·æœ¬è½®æä¾›çš„ä¿¡æ¯æ€»ç»“ï¼Œè¾“å…¥äº†ä»€ä¹ˆä¿¡æ¯ï¼Œæä¾›äº†å“ªäº›æ–‡ä»¶ç­‰"
        }}"""

        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆæ€»ç»“...")
            response = invoke_model(model_name="Qwen/Qwen3-32B", messages=[SystemMessage(content=system_prompt)])
            print(f"ğŸ“¥ LLMæ€»ç»“å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # Clean the response to handle markdown code blocks and malformed JSON
            cleaned_response = response.strip()
            
            # Remove markdown code blocks if present
            if '```json' in cleaned_response:
                print("ğŸ” æ£€æµ‹åˆ°markdownä»£ç å—ï¼Œæ­£åœ¨æ¸…ç†...")
                # Extract content between ```json and ```
                start_marker = '```json'
                end_marker = '```'
                start_index = cleaned_response.find(start_marker)
                if start_index != -1:
                    start_index += len(start_marker)
                    end_index = cleaned_response.find(end_marker, start_index)
                    if end_index != -1:
                        cleaned_response = cleaned_response[start_index:end_index].strip()
                    else:
                        # If no closing ```, take everything after ```json
                        cleaned_response = cleaned_response[start_index:].strip()
            elif '```' in cleaned_response:
                print("ğŸ” æ£€æµ‹åˆ°é€šç”¨ä»£ç å—ï¼Œæ­£åœ¨æ¸…ç†...")
                # Handle generic ``` blocks
                parts = cleaned_response.split('```')
                if len(parts) >= 3:
                    # Take the middle part (index 1)
                    cleaned_response = parts[1].strip()
            
            # If there are multiple JSON objects, take the first valid one
            if '}{' in cleaned_response:
                print("âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªJSONå¯¹è±¡ï¼Œå–ç¬¬ä¸€ä¸ª")
                cleaned_response = cleaned_response.split('}{')[0] + '}'
            
            print(f"ğŸ” æ¸…ç†åçš„å“åº”: {cleaned_response}")
            
            response_json = json.loads(cleaned_response)
            response_json["next_node"] = route_decision
            final_response = json.dumps(response_json, ensure_ascii=False)
            
            print(f"âœ… æ€»ç»“ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“Š æœ€ç»ˆå“åº”: {final_response}")
            print("âœ… _summary_user_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {"summary_message": final_response}
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"âŒ åŸå§‹å“åº”: {repr(response)}")
            # Fallback response
            fallback_response = {
                "summary": "ç”¨æˆ·æœ¬è½®æä¾›äº†æ–‡ä»¶ä¿¡æ¯ï¼Œä½†è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "next_node": route_decision
            }
            final_fallback = json.dumps(fallback_response, ensure_ascii=False)
            print(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨å“åº”: {final_fallback}")
            print("âœ… _summary_user_input æ‰§è¡Œå®Œæˆ (å¤‡ç”¨)")
            print("=" * 50)
            return {"summary_message": final_fallback}



    def run_process_user_input_agent(self, session_id: str = "1", previous_AI_messages: BaseMessage = None) -> List:
        """This function runs the process user input agent using invoke method instead of streaming"""
        print("\nğŸš€ å¼€å§‹è¿è¡Œ ProcessUserInputAgent")
        print("=" * 60)
        
        initial_state = self.create_initial_state(previous_AI_messages)
        config = {"configurable": {"thread_id": session_id}}
        
        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
        print(f"ğŸ“ åˆå§‹çŠ¶æ€å·²åˆ›å»º")
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œç”¨æˆ·è¾“å…¥å¤„ç†å·¥ä½œæµ...")
        
        try:
            # Use invoke instead of stream for simpler execution
            while True:
                final_state = self.graph.invoke(initial_state, config=config)
                if "__interrupt__" in final_state:
                    interrupt_value = final_state["__interrupt__"][0].value
                    print(f"ğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                    user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                    initial_state = Command(resume=user_response)
                    continue

                print("ğŸ‰æ‰§è¡Œå®Œæ¯•")
                summary_message = final_state.get("summary_message", "")
                template_file = final_state.get("uploaded_template_files_path", [])
                return [summary_message, template_file]
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # Return empty results on error
            error_summary = json.dumps({
                "summary": f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "next_node": "previous_node"
            }, ensure_ascii=False)
            return [error_summary, []]



# Langgraph studio to export the compiled graph
agent = ProcessUserInputAgent()
graph = agent.graph


if __name__ == "__main__":
    agent = ProcessUserInputAgent()
    # save_graph_visualization(agent.graph, "process_user_input_graph.png")
    agent.run_process_user_input_agent("")

