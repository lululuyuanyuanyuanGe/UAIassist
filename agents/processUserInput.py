import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.file_process import detect_and_process_file_paths, retrieve_file_content
from utilities.modelRelated import model_creation

import uuid
import json
import os
from pathlib import Path
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

from langgraph.graph import StateGraph, END, START
from langgraph.constants import Send
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


load_dotenv()

class ProcessUserInputState(TypedDict):
    process_user_input_messages: Annotated[list[BaseMessage], add_messages]
    user_input: str
    upload_files_path: list[str] # Store all uploaded files
    new_upload_files_path: list[str] # Track the new uploaded files in this round
    new_upload_files_processed_path: list[str] # Store the processed new uploaded files
    uploaded_template_files_path: list[str]
    supplement_files_path: dict[str, list[str]]
    irrelevant_files_path: list[str]
    all_files_irrelevant: bool  # Flag to indicate all files are irrelevant
    text_input_validation: str  # Store validation result [Valid] or [Invalid]
    previous_AI_messages: list[BaseMessage]
    session_id: str
    
class ProcessUserInputAgent:

    @tool
    def request_user_clarification(question: str, context: str = "") -> str:
        """
        ËØ¢ÈóÆÁî®Êà∑ÊæÑÊ∏ÖÔºåÂíåÁî®Êà∑Á°ÆËÆ§ÔºåÊàñËÄÖËØ¢ÈóÆÁî®Êà∑Ë°•ÂÖÖ‰ø°ÊÅØÔºåÂΩì‰Ω†‰∏çÁ°ÆÂÆöÁöÑÊó∂ÂÄôËØ∑ËØ¢ÈóÆÁî®Êà∑

        ÂèÇÊï∞Ôºö
            question: ÈóÆÈ¢ò
            contexnt: ÂèØÈÄâË°•ÂÖÖÂÜÖÂÆπÔºåËß£Èáä‰∏∫ÁîöÊÅ∂È≠î‰Ω†ÈúÄË¶Å‰∏Ä‰∏ã‰ø°ÊÅØ
        """
        prompt = f"{question}\n{context}"
        user_response = interrupt({"prompt": prompt})

        return user_response
    
    tools = [request_user_clarification]

    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.llm_c = model_creation(model_name=model_name, temperature=2) # complex logic use user selected model
        self.llm_c_with_tools = self.llm_c.bind_tools(self.tools)
        self.llm_s = model_creation(model_name="gpt-3.5-turbo", temperature=2) # simple logic use 3-5turbo
        self.llm_s_with_tools = self.llm_s.bind_tools(self.tools)
        self.memory = MemorySaver()
        self.graph = self._build_graph().compile(checkpointer=self.memory)


    def _build_graph(self) -> StateGraph:
        """This function will build the graph for the process user input agent"""
        graph = StateGraph(ProcessUserInputState)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("file_upload", self._file_upload)
        graph.add_node("analyze_uploaded_files", self._analyze_uploaded_files)
        graph.add_node("process_template", self._process_template)
        graph.add_node("process_supplement", self._process_supplement)
        graph.add_node("process_irrelevant", self._process_irrelevant)
        graph.add_node("analyze_text_input", self._analyze_text_input)
        graph.add_node("clarification_tool_node", ToolNode(self.tools))
        graph.add_node("summary_user_input", self._summary_user_input)
        
        graph.add_edge(START, "collect_user_input")

        graph.add_conditional_edges(
            "collect_user_input",
            self._route_after_collect_user_input,
            {
                "file_upload": "file_upload",
                "analyze_text_input": "analyze_text_input",
            }
        )

        graph.add_edge("file_upload", "analyze_uploaded_files")

        graph.add_conditional_edges(
            "analyze_uploaded_files",
            self._route_after_analyze_uploaded_files # Since we are using the send objects, we don't need to specify the edges
        )

        # After tool execution, re-analyze uploaded files with user input
        graph.add_edge("clarification_tool_node", "analyze_uploaded_files")

        graph.add_edge("process_template", "summary_user_input")
        graph.add_edge("process_supplement", "summary_user_input")
        graph.add_edge("process_irrelevant", "summary_user_input")

        graph.add_conditional_edges(
            "analyze_text_input",
            self._route_after_analyze_text_input,
            {
                "valid_text_input": "summary_user_input",
                "invalid_text_input": "collect_user_input",
            }
        )

        graph.add_edge("summary_user_input", END)
        return graph



    def create_initial_state(self, user_input: str, session_id: str = "1") -> ProcessUserInputState:
        """This function initializes the state of the process user input agent"""
        return {
            "process_user_input_messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "upload_files_path": [],
            "new_upload_files_path": [],
            "new_upload_files_processed_path": [],
            "uploaded_template_files_path": [],
            "supplement_files_path": {"Ë°®Ê†º": [], "ÊñáÊ°£": []},
            "irrelevant_files_path": [],
            "all_files_irrelevant": False,
            "text_input_validation": None,
            "previous_AI_messages": [AIMessage(content="ËØ∑Êèê‰æõÊõ¥Â§öÂÖ≥‰∫éÁæäÊùë‰∫∫Âè£ÊôÆÊü•ÁöÑ‰ø°ÊÅØ")],
            "session_id": session_id,
        }


    def _collect_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This is the node where we get user's input"""
        user_input = interrupt("Áî®Êà∑Ôºö")
        return {
            "process_user_input_messages": [HumanMessage(content=user_input)],
            "user_input": user_input
        }



    def _route_after_collect_user_input(self, state: ProcessUserInputState) -> str:
        """This node act as a safety check node, it will analyze the user's input and determine if it's a valid input,
        based on the LLM's previous response, at the same time it will route the agent to the correct node"""
        
        # Extract content from the message object
        latest_message = state["process_user_input_messages"][-1]
        message_content = latest_message.content if hasattr(latest_message, 'content') else str(latest_message)
        
        # Check if there are files in the user input
        user_upload_files = detect_and_process_file_paths(message_content)
        if user_upload_files:
            # Files detected - route to file_upload 
            # Note: We cannot modify state in routing functions, so file_upload node will re-detect files
            return "file_upload"
        
        # User didn't upload any new files, we will analyze the text input
        return "analyze_text_input"



    def _file_upload(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will upload user's file to our system"""
        
        # Re-detect files from user input since routing functions cannot modify state
        latest_message = state["process_user_input_messages"][-1]
        message_content = latest_message.content if hasattr(latest_message, 'content') else str(latest_message)
        
        detected_files = detect_and_process_file_paths(message_content)
        data_file = Path("agents/data.json")
        with open(data_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        for file in detected_files:
            file_name = Path(file).name
            if file_name in data["Ë°®Ê†º"] or file_name in data["ÊñáÊ°£"]:
                detected_files.remove(file)
                print(f"‚ö†Ô∏è Êñá‰ª∂ {file} Â∑≤Â≠òÂú®")
        
        if not detected_files:
            print("‚ö†Ô∏è No new files to upload")
            return {
                "new_upload_files_path": [],
                "new_upload_files_processed_path": []
            }
        
        print(f"üîÑ Processing {len(detected_files)} new files")
        
        # Process the files using the correct session_id
        result = retrieve_file_content(detected_files, "files")
        
        print(f"‚úÖ File uploaded: {result}")
        
        # Update state with new files
        return {
            "new_upload_files_path": detected_files,
            "upload_files_path": state["upload_files_path"] + detected_files,
            "new_upload_files_processed_path": result
        }
    


    def _analyze_uploaded_files(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will analyze the user's uploaded files, it need to classify the file into template
        supplement, or irrelevant. If all files are irrelevant, it will flag for text analysis instead."""
        
        import json
        from pathlib import Path
        
        # Initialize classification results
        classification_results = {
            "template": [],
            "supplement": {"Ë°®Ê†º": [], "ÊñáÊ°£": []},
            "irrelevant": []
        }
        
        # Process files one by one for better accuracy
        processed_files = []
        for file_path in state["new_upload_files_processed_path"]:
            try:
                source_path = Path(file_path)
                if not source_path.exists():
                    print(f"‚ùå Êñá‰ª∂‰∏çÂ≠òÂú®: {file_path}")
                    classification_results["irrelevant"].append(file_path)
                    continue
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:2000] if len(file_content) > 2000 else file_content
                
                # Create individual analysis prompt for this file
                system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Ë°®Ê†ºÁîüÊàêÊô∫ËÉΩ‰ΩìÔºåÈúÄË¶ÅÂàÜÊûêÁî®Êà∑‰∏ä‰º†ÁöÑÊñá‰ª∂ÂÜÖÂÆπÂπ∂ËøõË°åÂàÜÁ±ª„ÄÇÂÖ±ÊúâÂõõÁßçÁ±ªÂûãÔºö

                1. **Ê®°ÊùøÁ±ªÂûã (template)**: Á©∫ÁôΩË°®Ê†ºÊ®°ÊùøÔºåÂè™ÊúâË°®Â§¥Ê≤°ÊúâÂÖ∑‰ΩìÊï∞ÊçÆ
                2. **Ë°•ÂÖÖË°®Ê†º (supplement-Ë°®Ê†º)**: Â∑≤Â°´ÂÜôÁöÑÂÆåÊï¥Ë°®Ê†ºÔºåÁî®‰∫éË°•ÂÖÖÊï∞ÊçÆÂ∫ì
                3. **Ë°•ÂÖÖÊñáÊ°£ (supplement-ÊñáÊ°£)**: ÂåÖÂê´ÈáçË¶Å‰ø°ÊÅØÁöÑÊñáÊú¨Êñá‰ª∂ÔºåÂ¶ÇÊ≥ïÂæãÊù°Êñá„ÄÅÊîøÁ≠ñ‰ø°ÊÅØÁ≠â
                4. **Êó†ÂÖ≥Êñá‰ª∂ (irrelevant)**: ‰∏éË°®Ê†ºÂ°´ÂÜôÊó†ÂÖ≥ÁöÑÊñá‰ª∂

                Ê≥®ÊÑèÔºöÊâÄÊúâÊñá‰ª∂Â∑≤ËΩ¨Êç¢‰∏∫txtÊ†ºÂºèÔºåË°®Ê†º‰ª•HTML‰ª£Á†ÅÂΩ¢ÂºèÂëàÁé∞ÔºåËØ∑Ê†πÊçÆÂÜÖÂÆπËÄåÈùûÊñá‰ª∂ÂêçÊàñÂêéÁºÄÂà§Êñ≠„ÄÇ

                Áî®Êà∑ËæìÂÖ•: {state.get("user_input", "")}

                ÂΩìÂâçÂàÜÊûêÊñá‰ª∂:
                Êñá‰ª∂Âêç: {source_path.name}
                Êñá‰ª∂Ë∑ØÂæÑ: {file_path}
                Êñá‰ª∂ÂÜÖÂÆπ:
                {analysis_content}

                ËØ∑‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ãJSONÊ†ºÂºèÂõûÂ§çÔºåÂè™ËøîÂõûËøô‰∏Ä‰∏™Êñá‰ª∂ÁöÑÂàÜÁ±ªÁªìÊûúÔºà‰∏çË¶ÅÊ∑ªÂä†‰ªª‰ΩïÂÖ∂‰ªñÊñáÂ≠óÔºâÔºö
                {{
                    "classification": "template" | "supplement-Ë°®Ê†º" | "supplement-ÊñáÊ°£" | "irrelevant"
                }}"""
                
                # Get LLM analysis for this file
                analysis_response = self.llm_c_with_tools.invoke([SystemMessage(content=system_prompt)])
                
                # Handle tool calls if LLM needs clarification
                if hasattr(analysis_response, 'tool_calls') and analysis_response.tool_calls:
                    print(f"‚ö†Ô∏è LLMÂØπÊñá‰ª∂ {source_path.name} ÈúÄË¶Å‰ΩøÁî®Â∑•ÂÖ∑ÔºåË∑≥ËøáÊ≠§Êñá‰ª∂")
                    classification_results["irrelevant"].append(file_path)
                    continue
                
                # Parse JSON response for this file
                try:
                    # Extract JSON from response
                    response_content = analysis_response.content.strip()
                    # Remove markdown code blocks if present
                    if response_content.startswith('```'):
                        response_content = response_content.split('\n', 1)[1]
                        response_content = response_content.rsplit('\n', 1)[0]
                    
                    file_classification = json.loads(response_content)
                    classification_type = file_classification.get("classification", "irrelevant")
                    
                    # Add to appropriate category
                    if classification_type == "template":
                        classification_results["template"].append(file_path)
                    elif classification_type == "supplement-Ë°®Ê†º":
                        classification_results["supplement"]["Ë°®Ê†º"].append(file_path)
                    elif classification_type == "supplement-ÊñáÊ°£":
                        classification_results["supplement"]["ÊñáÊ°£"].append(file_path)
                    else:  # irrelevant or unknown
                        classification_results["irrelevant"].append(file_path)
                    
                    processed_files.append(source_path.name)
                    print(f"‚úÖ Êñá‰ª∂ {source_path.name} ÂàÜÁ±ª‰∏∫: {classification_type}")
                    
                except json.JSONDecodeError as e:
                    print(f"‚ùå Êñá‰ª∂ {source_path.name} JSONËß£ÊûêÈîôËØØ: {e}")
                    print(f"LLMÂìçÂ∫î: {analysis_response.content}")
                    # Fallback: mark as irrelevant for safety
                    classification_results["irrelevant"].append(file_path)
                
            except Exception as e:
                print(f"‚ùå Â§ÑÁêÜÊñá‰ª∂Âá∫Èîô {file_path}: {e}")
                # Add to irrelevant on error
                classification_results["irrelevant"].append(file_path)
                continue
        
        if not processed_files and not classification_results["irrelevant"]:
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"Ë°®Ê†º": [], "ÊñáÊ°£": []},
                "irrelevant_files_path": [],
                "all_files_irrelevant": True,  # Flag for routing to text analysis
                "process_user_input_messages": [SystemMessage(content="Ê≤°ÊúâÊâæÂà∞ÂèØÂ§ÑÁêÜÁöÑÊñá‰ª∂ÔºåÂ∞ÜÂàÜÊûêÁî®Êà∑ÊñáÊú¨ËæìÂÖ•")]
            }
        
        # Update state with classification results
        uploaded_template_files = classification_results.get("template", [])
        supplement_files = classification_results.get("supplement", {"Ë°®Ê†º": [], "ÊñáÊ°£": []})
        irrelevant_files = classification_results.get("irrelevant", [])
        
        # Check if all files are irrelevant
        all_files_irrelevant = (
            len(uploaded_template_files) == 0 and 
            len(supplement_files.get("Ë°®Ê†º", [])) == 0 and 
            len(supplement_files.get("ÊñáÊ°£", [])) == 0 and
            len(irrelevant_files) == len(state["new_upload_files_processed_path"])
        )
        
        if all_files_irrelevant:
            # All files are irrelevant, flag for text analysis
            analysis_summary = f"""üìã Êñá‰ª∂ÂàÜÊûêÂÆåÊàê - ÊâÄÊúâÊñá‰ª∂Âùá‰∏éË°®Ê†ºÁîüÊàêÊó†ÂÖ≥:
            ‚ùå Êó†ÂÖ≥Êñá‰ª∂: {len(irrelevant_files)} ‰∏™
            
            Êñá‰ª∂ÂàóË°®: {[Path(f).name for f in irrelevant_files]}
            
            üîÑ Â∞ÜËΩ¨‰∏∫ÂàÜÊûêÁî®Êà∑ÊñáÊú¨ËæìÂÖ•ÂÜÖÂÆπ"""
            
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"Ë°®Ê†º": [], "ÊñáÊ°£": []},
                "irrelevant_files_path": irrelevant_files,
                "all_files_irrelevant": True,  # Flag for routing
                "process_user_input_messages": [SystemMessage(content=analysis_summary)]
            }
        else:
            # Some files are relevant, proceed with normal flow
            analysis_summary = f"""üìã Êñá‰ª∂ÂàÜÊûêÂÆåÊàê:
            ‚úÖ Ê®°ÊùøÊñá‰ª∂: {len(uploaded_template_files)} ‰∏™
            ‚úÖ Ë°•ÂÖÖË°®Ê†º: {len(supplement_files.get("Ë°®Ê†º", []))} ‰∏™  
            ‚úÖ Ë°•ÂÖÖÊñáÊ°£: {len(supplement_files.get("ÊñáÊ°£", []))} ‰∏™
            ‚ùå Êó†ÂÖ≥Êñá‰ª∂: {len(irrelevant_files)} ‰∏™

            ÂàÜÁ±ªËØ¶ÊÉÖ:
            Ê®°Êùø: {[Path(f).name for f in uploaded_template_files]}
            Ë°®Ê†º: {[Path(f).name for f in supplement_files.get("Ë°®Ê†º", [])]}
            ÊñáÊ°£: {[Path(f).name for f in supplement_files.get("ÊñáÊ°£", [])]}
            Êó†ÂÖ≥: {[Path(f).name for f in irrelevant_files]}"""
            
            return {
                "uploaded_template_files_path": uploaded_template_files,
                "supplement_files_path": supplement_files,
                "irrelevant_files_path": irrelevant_files,
                "all_files_irrelevant": False,  # Flag for routing
                "process_user_input_messages": [SystemMessage(content=analysis_summary)]
            }
                
    def _route_after_analyze_uploaded_files(self, state: ProcessUserInputState):
        """Route after analyzing uploaded files. Uses Send objects for all routing."""
        
        # Check if LLM request a tool call
        latest_message = state["process_user_input_messages"][-1]
        if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
            return [Send("clarification_tool_node", state)]
        
        # Check if all files are irrelevant - route to text analysis
        if state.get("all_files_irrelevant", False):
            # First clean up irrelevant files, then analyze text
            sends = []
            if state.get("irrelevant_files_path"):
                sends.append(Send("process_irrelevant", state))
            sends.append(Send("analyze_text_input", state))
            return sends
        
        # Some files are relevant - process them in parallel, then continue to text analysis
        sends = []
        if state.get("uploaded_template_files_path"):
            sends.append(Send("process_template", state))
        if state.get("supplement_files_path", {}).get("Ë°®Ê†º") or state.get("supplement_files_path", {}).get("ÊñáÊ°£"):
            sends.append(Send("process_supplement", state))
        if state.get("irrelevant_files_path"):
            sends.append(Send("process_irrelevant", state))

        
        return sends if sends else [Send("analyze_text_input", state)]  # Fallback
    
    def _process_supplement(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the supplement files, it will analyze the supplement files and summarize the content of the files as well as stored the summary in data.json"""
        
        # Load existing data.json
        data_json_path = Path("agents/data.json")
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            data = {"Ë°®Ê†º": {}, "ÊñáÊ°£": {}}
        
        table_files = state["supplement_files_path"]["Ë°®Ê†º"]
        document_files = state["supplement_files_path"]["ÊñáÊ°£"]
        
        # Process table files
        for table_file in table_files:
            try:
                source_path = Path(table_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Ë°®Ê†ºÂàÜÊûê‰∏ìÂÆ∂ÔºåÁé∞Âú®Ëøô‰∏™excelË°®Ê†ºÂ∑≤ÁªèË¢´ËΩ¨Êç¢Êàê‰∫ÜHTMLÊ†ºÂºèÔºå‰Ω†ÁöÑ‰ªªÂä°ÊòØ‰ªîÁªÜÈòÖËØªËøô‰∏™Ë°®Ê†ºÔºåÂàÜÊûêË°®Ê†ºÁöÑÁªìÊûÑÔºåÂπ∂ÊÄªÁªìË°®Ê†ºÁöÑÂÜÖÂÆπÔºåÊâÄÊúâÁöÑË°®Â§¥„ÄÅÂàóÂêç„ÄÅÊï∞ÊçÆÈÉΩË¶ÅÊÄªÁªìÂá∫Êù•„ÄÇ

                Êñá‰ª∂ÂÜÖÂÆπ:
                {file_content}

                ËØ∑ÊåâÁÖß‰ª•‰∏ãÊ†ºÂºèËæìÂá∫ÁªìÊûúÔºö
                {{
                    "Ë°®Ê†ºÁªìÊûÑ": "ÊèèËø∞Ë°®Ê†ºÁöÑÊï¥‰ΩìÁªìÊûÑ",
                    "Ë°®Â§¥‰ø°ÊÅØ": ["ÂàóÂêç1", "ÂàóÂêç2", "ÂàóÂêç3"],
                    "Êï∞ÊçÆÊ¶ÇË¶Å": "Êï∞ÊçÆÁöÑÊÄª‰ΩìÊèèËø∞ÂíåÈáçË¶Å‰ø°ÊÅØ",
                    "Ë°åÊï∞ÁªüËÆ°": "ÊÄªË°åÊï∞",
                    "ÂÖ≥ÈîÆÂ≠óÊÆµ": ["ÈáçË¶ÅÂ≠óÊÆµ1", "ÈáçË¶ÅÂ≠óÊÆµ2"]
                }}"""
                                
                analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
                
                # Store in data.json
                data["Ë°®Ê†º"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(table_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"‚úÖ Ë°®Ê†ºÊñá‰ª∂Â∑≤ÂàÜÊûê: {source_path.name}")
                
            except Exception as e:
                print(f"‚ùå Â§ÑÁêÜË°®Ê†ºÊñá‰ª∂Âá∫Èîô {table_file}: {e}")

        # Process document files
        for document_file in document_files:
            try:
                source_path = Path(document_file)
                file_content = source_path.read_text(encoding='utf-8')
                
                system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™ÊñáÊ°£ÂàÜÊûê‰∏ìÂÆ∂ÔºåÁé∞Âú®Ëøô‰∏™ÊñáÊ°£Â∑≤ÁªèË¢´ËΩ¨Êç¢Êàê‰∫ÜtxtÊ†ºÂºèÔºå‰Ω†ÁöÑ‰ªªÂä°ÊòØ‰ªîÁªÜÈòÖËØªËøô‰∏™ÊñáÊ°£ÔºåÂàÜÊûêÊñáÊ°£ÁöÑÂÜÖÂÆπÔºåÂπ∂ÊÄªÁªìÊñáÊ°£ÁöÑÂÜÖÂÆπ„ÄÇÊñáÊ°£ÂèØËÉΩÂåÖÂê´ÈáçË¶ÅÁöÑ‰ø°ÊÅØÔºå‰æãÂ¶ÇÊ≥ïÂæãÊù°Êñá„ÄÅÊîøÁ≠ñËßÑÂÆöÁ≠âÔºå‰Ω†‰∏çËÉΩÈÅóÊºèËøô‰∫õ‰ø°ÊÅØ„ÄÇ

                Êñá‰ª∂ÂÜÖÂÆπ:
                {file_content}

                ËØ∑ÊåâÁÖß‰ª•‰∏ãÊ†ºÂºèËæìÂá∫ÁªìÊûúÔºö
                {{
                    "ÊñáÊ°£Á±ªÂûã": "Âà§Êñ≠ÊñáÊ°£ÁöÑÁ±ªÂûãÔºàÂ¶ÇÊîøÁ≠ñÊñá‰ª∂„ÄÅÊ≥ïÂæãÊù°Êñá„ÄÅËØ¥ÊòéÊñáÊ°£Á≠âÔºâ",
                    "‰∏ªË¶ÅÂÜÖÂÆπ": "ÊñáÊ°£ÁöÑÊ†∏ÂøÉÂÜÖÂÆπÊ¶ÇË¶Å",
                    "ÈáçË¶ÅÊù°Ê¨æ": ["ÈáçË¶ÅÊù°Ê¨æ1", "ÈáçË¶ÅÊù°Ê¨æ2"],
                    "ÂÖ≥ÈîÆ‰ø°ÊÅØ": ["ÂÖ≥ÈîÆ‰ø°ÊÅØ1", "ÂÖ≥ÈîÆ‰ø°ÊÅØ2"],
                    "Â∫îÁî®Âú∫ÊôØ": "Ëøô‰∫õ‰ø°ÊÅØÂú®Ë°®Ê†ºÂ°´ÂÜô‰∏≠ÁöÑÁî®ÈÄî"
                }}"""
                                
                analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])

                # Update state with analysis response
                state["process_user_input_messages"].append(analysis_response)
                
                # Store in data.json
                data["ÊñáÊ°£"][source_path.name] = {
                    "summary": analysis_response.content,
                    "file_path": str(document_file),
                    "timestamp": datetime.now().isoformat(),
                    "file_size": source_path.stat().st_size
                }
                
                print(f"‚úÖ ÊñáÊ°£Êñá‰ª∂Â∑≤ÂàÜÊûê: {source_path.name}")
                
            except Exception as e:
                print(f"‚ùå Â§ÑÁêÜÊñáÊ°£Êñá‰ª∂Âá∫Èîô {document_file}: {e}")
        
        # Save updated data.json
        try:
            with open(data_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"‚úÖ Â∑≤Êõ¥Êñ∞ data.jsonÔºåË°®Ê†ºÊñá‰ª∂ {len(data['Ë°®Ê†º'])} ‰∏™ÔºåÊñáÊ°£Êñá‰ª∂ {len(data['ÊñáÊ°£'])} ‰∏™")
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠ò data.json Êó∂Âá∫Èîô: {e}")
        
        # Create summary message
        summary_message = f"""üìä Ë°•ÂÖÖÊñá‰ª∂Â§ÑÁêÜÂÆåÊàê:
        ‚úÖ Ë°®Ê†ºÊñá‰ª∂: {len(table_files)} ‰∏™Â∑≤ÂàÜÊûêÂπ∂Â≠òÂÇ®
        ‚úÖ ÊñáÊ°£Êñá‰ª∂: {len(document_files)} ‰∏™Â∑≤ÂàÜÊûêÂπ∂Â≠òÂÇ®
        üìù Êï∞ÊçÆÂ∫ìÂ∑≤Êõ¥Êñ∞ÔºåÊÄªËÆ°Ë°®Ê†º {len(data['Ë°®Ê†º'])} ‰∏™ÔºåÊñáÊ°£ {len(data['ÊñáÊ°£'])} ‰∏™"""
        
        return {
            "process_user_input_messages": [SystemMessage(content=summary_message)]
        }
        
        
    def _process_irrelevant(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the irrelevant files, it will delete the irrelevant files from the conversations folder"""
        
        deleted_files = []
        failed_deletes = []
        
        for file_path in state["irrelevant_files_path"]:
            try:
                file_to_delete = Path(file_path)
                if file_to_delete.exists():
                    os.remove(file_to_delete)
                    deleted_files.append(file_to_delete.name)
                    print(f"üóëÔ∏è Â∑≤Âà†Èô§Êó†ÂÖ≥Êñá‰ª∂: {file_to_delete.name}")
                else:
                    print(f"‚ö†Ô∏è Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåË∑≥ËøáÂà†Èô§: {file_path}")
                    
            except Exception as e:
                failed_deletes.append(Path(file_path).name)
                print(f"‚ùå Âà†Èô§Êñá‰ª∂Êó∂Âá∫Èîô {file_path}: {e}")

        # Create summary message
        summary_message = f"""üóëÔ∏è Êó†ÂÖ≥Êñá‰ª∂Â§ÑÁêÜÂÆåÊàê:
        ‚úÖ ÊàêÂäüÂà†Èô§: {len(deleted_files)} ‰∏™Êñá‰ª∂
        ‚ùå Âà†Èô§Â§±Ë¥•: {len(failed_deletes)} ‰∏™Êñá‰ª∂

        Âà†Èô§ÁöÑÊñá‰ª∂: {', '.join(deleted_files) if deleted_files else 'Êó†'}
        Â§±Ë¥•ÁöÑÊñá‰ª∂: {', '.join(failed_deletes) if failed_deletes else 'Êó†'}"""
        
        return {
            "process_user_input_messages": [SystemMessage(content=summary_message)]
        }
    

    
    def _process_template(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will process the template files, it will analyze the template files and determine if it's a valid template"""
        
        template_files = state["uploaded_template_files_path"]
        
        # If multiple templates, ask user to choose
        if len(template_files) > 1:
            template_names = [Path(f).name for f in template_files]
            question = f"Ê£ÄÊµãÂà∞Â§ö‰∏™Ê®°ÊùøÊñá‰ª∂ÔºåËØ∑ÈÄâÊã©Ë¶Å‰ΩøÁî®ÁöÑÊ®°ÊùøÔºö\n" + \
                      "\n".join([f"{i+1}. {name}" for i, name in enumerate(template_names)]) + \
                      "\nËØ∑ËæìÂÖ•Â∫èÂè∑ÔºàÂ¶ÇÔºö1ÔºâÔºö"
            
            try:
                user_choice = self.request_user_clarification(question, "Á≥ªÁªüÈúÄË¶ÅÁ°ÆÂÆö‰ΩøÁî®Âì™‰∏™Ê®°ÊùøÊñá‰ª∂ËøõË°åÂêéÁª≠Â§ÑÁêÜ")
                
                # Parse user choice
                try:
                    choice_index = int(user_choice.strip()) - 1
                    if 0 <= choice_index < len(template_files):
                        selected_template = template_files[choice_index]
                        # Remove non-selected templates
                        rejected_templates = [f for i, f in enumerate(template_files) if i != choice_index]
                        
                        # Delete rejected template files
                        for rejected_file in rejected_templates:
                            try:
                                Path(rejected_file).unlink()
                                print(f"üóëÔ∏è Â∑≤Âà†Èô§Êú™ÈÄâ‰∏≠ÁöÑÊ®°Êùø: {Path(rejected_file).name}")
                            except Exception as e:
                                print(f"‚ùå Âà†Èô§Ê®°ÊùøÊñá‰ª∂Âá∫Èîô: {e}")
                        
                        # Update state to only include selected template
                        template_files = [selected_template]
                        
                    else:
                        print("‚ùå Êó†ÊïàÁöÑÈÄâÊã©Ôºå‰ΩøÁî®Á¨¨‰∏Ä‰∏™Ê®°Êùø")
                        selected_template = template_files[0]
                        template_files = [selected_template]
                        
                except ValueError:
                    print("‚ùå ËæìÂÖ•Ê†ºÂºèÈîôËØØÔºå‰ΩøÁî®Á¨¨‰∏Ä‰∏™Ê®°Êùø")
                    selected_template = template_files[0]
                    template_files = [selected_template]
                    
            except Exception as e:
                print(f"‚ùå Áî®Êà∑ÈÄâÊã©Âá∫Èîô: {e}")
                selected_template = template_files[0]
                template_files = [selected_template]
        
        # Analyze the selected template for complexity
        template_file = template_files[0]
        
        try:
            source_path = Path(template_file)
            template_content = source_path.read_text(encoding='utf-8')
            
            # Create prompt to determine if template is complex or simple
            system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Ë°®Ê†ºÁªìÊûÑÂàÜÊûê‰∏ìÂÆ∂ÔºåÈúÄË¶ÅÂà§Êñ≠Ëøô‰∏™Ë°®Ê†ºÊ®°ÊùøÊòØÂ§çÊùÇÊ®°ÊùøËøòÊòØÁÆÄÂçïÊ®°Êùø„ÄÇ

            Âà§Êñ≠Ê†áÂáÜÔºö
            - **Â§çÊùÇÊ®°Êùø**: Ë°®Ê†ºÂêåÊó∂ÂåÖÂê´Ë°åË°®Â§¥ÂíåÂàóË°®Â§¥ÔºåÂç≥Êó¢ÊúâË°åÊ†áÈ¢òÂèàÊúâÂàóÊ†áÈ¢òÁöÑ‰∫åÁª¥Ë°®Ê†ºÁªìÊûÑ
            - **ÁÆÄÂçïÊ®°Êùø**: Ë°®Ê†ºÂè™ÂåÖÂê´ÂàóË°®Â§¥ÔºåÊØèË°åÊòØÁã¨Á´ãÁöÑÊï∞ÊçÆËÆ∞ÂΩï

            Ê®°ÊùøÂÜÖÂÆπÔºàHTMLÊ†ºÂºèÔºâÔºö
            {template_content}

            ËØ∑‰ªîÁªÜÂàÜÊûêË°®Ê†ºÁªìÊûÑÔºåÁÑ∂ÂêéÂè™ÂõûÂ§ç‰ª•‰∏ãÈÄâÈ°π‰πã‰∏ÄÔºö
            [Complex] - Â¶ÇÊûúÊòØÂ§çÊùÇÊ®°ÊùøÔºàÂåÖÂê´Ë°åË°®Â§¥ÂíåÂàóË°®Â§¥Ôºâ
            [Simple] - Â¶ÇÊûúÊòØÁÆÄÂçïÊ®°ÊùøÔºàÂè™ÂåÖÂê´ÂàóË°®Â§¥Ôºâ"""
            
            analysis_response = self.llm_c.invoke([SystemMessage(content=system_prompt)])
            
            # Parse response
            response_content = analysis_response.content.strip()
            if "[Complex]" in response_content:
                template_type = "[Complex]"
            elif "[Simple]" in response_content:
                template_type = "[Simple]"
            else:
                # Default to Simple if unclear
                template_type = "[Simple]"
                print("‚ö†Ô∏è Êó†Ê≥ïÁ°ÆÂÆöÊ®°ÊùøÁ±ªÂûãÔºåÈªòËÆ§‰∏∫ÁÆÄÂçïÊ®°Êùø")
            
            # Create analysis summary
            summary_message = f"""üìã Ê®°ÊùøÂàÜÊûêÂÆåÊàê:
            ‚úÖ ÈÄâÂÆöÊ®°Êùø: {Path(template_file).name}
            üîç Ê®°ÊùøÁ±ªÂûã: {template_type}
            üìÅ Ê®°ÊùøË∑ØÂæÑ: {template_file}

            {template_type}"""
            
            return {
                "uploaded_template_files_path": template_files,  # Only selected template
                "process_user_input_messages": [SystemMessage(content=summary_message)]
            }
            
        except Exception as e:
            print(f"‚ùå ÂàÜÊûêÊ®°ÊùøÊó∂Âá∫Èîô: {e}")
            return {
                "uploaded_template_files_path": template_files,
                "process_user_input_messages": [SystemMessage(content=f"Ê®°ÊùøÂàÜÊûêÂá∫Èîô: {e}\nÈªòËÆ§‰∏∫[Simple]")]
            }



    def _route_after_process_template(self, state: ProcessUserInputState) -> str:
        """It has two different routes, if it is [Complex] template we will go to complex template handle node, which for now is a placeholder.
        if it is [Simple] template we simply go to the template_provided node to keep the analysis"""

        latest_message = state["process_user_input_messages"][-1]
        if "[Complex]" in latest_message.content:
            return "complex_template_handle"
        else:
            return "template_provided"
        


    def _analyze_text_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node performs a safety check on user text input when all uploaded files are irrelevant.
        It validates if the user input contains meaningful table/Excel-related content.
        Returns [Valid] or [Invalid] based on the analysis."""
        
        user_input = state["user_input"]
        
        if not user_input or user_input.strip() == "":
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content="‚ùå Áî®Êà∑ËæìÂÖ•‰∏∫Á©∫ÔºåÈ™åËØÅÂ§±Ë¥•")]
            }
        
        # Create validation prompt for text input safety check
        system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™ËæìÂÖ•È™åËØÅ‰∏ìÂÆ∂ÔºåÈúÄË¶ÅÂà§Êñ≠Áî®Êà∑ÁöÑÊñáÊú¨ËæìÂÖ•ÊòØÂê¶‰∏éË°®Ê†ºÁîüÊàê„ÄÅExcelÂ§ÑÁêÜÁõ∏ÂÖ≥ÔºåÂπ∂‰∏îÊòØÂê¶ÂåÖÂê´ÊúâÊÑè‰πâÁöÑÂÜÖÂÆπÔºå‰Ω†ÁöÑÂà§Êñ≠ÈúÄË¶ÅÊ†πÊçÆ‰∏ä‰∏ãÊñáÔºå
        Êàë‰ºöÊèê‰æõ‰∏ä‰∏Ä‰∏™AIÁöÑÂõûÂ§çÔºå‰ª•ÂèäÁî®Êà∑ËæìÂÖ•Ôºå‰Ω†ÈúÄË¶ÅÊ†πÊçÆ‰∏ä‰∏ãÊñáÔºåÂà§Êñ≠Áî®Êà∑ËæìÂÖ•ÊòØÂê¶‰∏éË°®Ê†ºÁîüÊàê„ÄÅExcelÂ§ÑÁêÜÁõ∏ÂÖ≥ÔºåÂπ∂‰∏îÊòØÂê¶ÂåÖÂê´ÊúâÊÑè‰πâÁöÑÂÜÖÂÆπ„ÄÇ
        
        ‰∏ä‰∏Ä‰∏™AIÁöÑÂõûÂ§ç: {state["previous_AI_messages"]}
        Áî®Êà∑ËæìÂÖ•: {user_input}

        È™åËØÅÊ†áÂáÜÔºö
        1. **ÊúâÊïàËæìÂÖ• [Valid]**:
           - ÊòéÁ°ÆÊèêÂà∞ÈúÄË¶ÅÁîüÊàêË°®Ê†º„ÄÅÂ°´ÂÜôË°®Ê†º„ÄÅExcelÁõ∏ÂÖ≥Êìç‰Ωú
           - ÂåÖÂê´ÂÖ∑‰ΩìÁöÑË°®Ê†ºË¶ÅÊ±Ç„ÄÅÊï∞ÊçÆÊèèËø∞„ÄÅÂ≠óÊÆµ‰ø°ÊÅØ
           - ËØ¢ÈóÆË°®Ê†ºÊ®°Êùø„ÄÅË°®Ê†ºÊ†ºÂºèÁõ∏ÂÖ≥ÈóÆÈ¢ò
           - Êèê‰æõ‰∫ÜË°®Ê†ºÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÊàñ‰ø°ÊÅØ

        2. **Êó†ÊïàËæìÂÖ• [Invalid]**:
           - ÂÆåÂÖ®‰∏éË°®Ê†º/ExcelÊó†ÂÖ≥ÁöÑÂÜÖÂÆπ
           - ÂûÉÂúæÊñáÊú¨„ÄÅÈöèÊú∫Â≠óÁ¨¶„ÄÅÊó†ÊÑè‰πâÂÜÖÂÆπ
           - Á©∫ÁôΩÊàñÂè™ÊúâÊ†áÁÇπÁ¨¶Âè∑
           - ÊòéÊòæÁöÑÊµãËØïËæìÂÖ•ÊàñÊó†ÂÖ≥ÈóÆÈ¢ò

        ËØ∑‰ªîÁªÜÂàÜÊûêÁî®Êà∑ËæìÂÖ•ÔºåÁÑ∂ÂêéÂè™ÂõûÂ§ç‰ª•‰∏ãÈÄâÈ°π‰πã‰∏ÄÔºö
        [Valid] - Â¶ÇÊûúËæìÂÖ•‰∏éË°®Ê†ºÁõ∏ÂÖ≥‰∏îÊúâÊÑè‰πâ
        [Invalid] - Â¶ÇÊûúËæìÂÖ•Êó†ÂÖ≥ÊàñÊó†ÊÑè‰πâ"""
        
        try:
            # Get LLM validation
            validation_response = self.llm_s.invoke([SystemMessage(content=system_prompt)])
            
            # Parse response
            response_content = validation_response.content.strip()
            
            if "[Valid]" in response_content:
                validation_result = "[Valid]"
                status_message = "‚úÖ Áî®Êà∑ËæìÂÖ•È™åËØÅÈÄöËøá - ÂÜÖÂÆπ‰∏éË°®Ê†ºÁõ∏ÂÖ≥‰∏îÊúâÊÑè‰πâ"
            elif "[Invalid]" in response_content:
                validation_result = "[Invalid]"
                status_message = "‚ùå Áî®Êà∑ËæìÂÖ•È™åËØÅÂ§±Ë¥• - ÂÜÖÂÆπ‰∏éË°®Ê†ºÊó†ÂÖ≥ÊàñÊó†ÊÑè‰πâ"
            else:
                # Default to Invalid for safety
                validation_result = "[Invalid]"
                status_message = "‚ùå Áî®Êà∑ËæìÂÖ•È™åËØÅÂ§±Ë¥• - Êó†Ê≥ïÁ°ÆÂÆöËæìÂÖ•ÊúâÊïàÊÄßÔºåÈªòËÆ§‰∏∫Êó†Êïà"
                print(f"‚ö†Ô∏è Êó†Ê≥ïËß£ÊûêÈ™åËØÅÁªìÊûúÔºåLLMÂìçÂ∫î: {response_content}")
            
            # Create validation summary
            summary_message = f"""üîç ÊñáÊú¨ËæìÂÖ•ÂÆâÂÖ®Ê£ÄÊü•ÂÆåÊàê:
            
            üìÑ **Áî®Êà∑ËæìÂÖ•**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            ‚úÖ **È™åËØÅÁªìÊûú**: {validation_result}
            üìù **Áä∂ÊÄÅ**: {status_message}"""
            
            return {
                "text_input_validation": validation_result,
                "process_user_input_messages": [SystemMessage(content=summary_message)]
            }
                
        except Exception as e:
            print(f"‚ùå È™åËØÅÊñáÊú¨ËæìÂÖ•Êó∂Âá∫Èîô: {e}")
            
            # Default to Invalid for safety when there's an error
            error_message = f"""‚ùå ÊñáÊú¨ËæìÂÖ•È™åËØÅÂá∫Èîô: {e}
            
            üìÑ **Áî®Êà∑ËæìÂÖ•**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            üîí **ÂÆâÂÖ®Êé™ÊñΩ**: ÈªòËÆ§Ê†áËÆ∞‰∏∫Êó†ÊïàËæìÂÖ•"""
            
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content=error_message)]
            }



    def _route_after_analyze_text_input(self, state: ProcessUserInputState) -> str:
        """Route after text input validation based on [Valid] or [Invalid] result."""
        
        validation_result = state.get("text_input_validation", "[Invalid]")
        
        if validation_result == "[Valid]":
            # Text input is valid and table-related, proceed to summary
            return "valid_text_input"
        else:
            # Text input is invalid, route back to collect user input
            return "invalid_text_input"
        

    
    def _summary_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """Basically this nodes act as a summry nodes, that summarize what the new information has been provided by the user in this round of human in the lopp also it needs to 
        decide which node to route to next
        """
        process_user_input_messages_conent = [item.content for item in state["process_user_input_messages"]]
        system_prompt = f"""‰Ω†ÁöÑ‰ªªÂä°ÊòØË¥üË¥£ÊÄªÁªìÁî®Êà∑Âú®Ëøô‰∏ÄËΩÆÈÉΩÊèê‰æõ‰∫ÜÂì™‰∫õ‰ø°ÊÅØÔºå‰Ω†ÈúÄË¶ÅÊ†πÊçÆÊï¥‰∏™ÂØπËØùËÆ∞ÂΩïÔºåÊÄªÁªìÁî®Êà∑ÈÉΩÊèê‰æõ‰∫ÜÂì™‰∫õ‰ø°ÊÅØÔºåÂπ∂‰∏îÊ†πÊçÆËøô‰∫õ‰ø°ÊÅØÔºåÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•ÁöÑÊµÅÁ®ã
        ËßÑÂàôÂ¶Ç‰∏ãÔºåÂ¶Ç‰ΩïÂá∫Áé∞‰∫ÜÂ§çÊùÇÊ®°ÊùøÔºåËøîÂõû"complex_template"ÔºåÂ¶ÇÊûúÂá∫Áé∞‰∫ÜÁÆÄÂçïÊ®°ÊùøÔºåËøîÂõû"simple_template"ÔºåÂÖ∂‰ΩôÊÉÖÂÜµËØ∑ËøîÂõû"previous_node" 
        ‰Ω†ÁöÑÂõûÂ§çÈúÄË¶ÅÂåÖÂê´ÂØπËøô‰∏ÄËΩÆÁöÑÊÄªÁªìÔºåÂíåËäÇÁÇπË∑ØÁî±‰ø°ÊÅØÔºåÁî±jsonÊù•Ë°®Á§∫

        ÂéÜÂè≤ÂØπËØù:{process_user_input_messages_conent}
        {{
            "summary": "ÊÄªÁªìÁî®Êà∑Âú®Ëøô‰∏ÄËΩÆÈÉΩÊèê‰æõ‰∫ÜÂì™‰∫õ‰ø°ÊÅØ",
            "next_node": "ËäÇÁÇπË∑ØÁî±‰ø°ÊÅØ"
        }}
        
        """
        
        try:
            # Try the LLM call with detailed error handling
            
            messages = [SystemMessage(content=system_prompt)]
            print(f"üîÑ Ê≠£Âú®Ë∞ÉÁî®LLMËøõË°åÊÄªÁªìÔºåÊ∂àÊÅØÊï∞Èáè: {len(messages)}")
            
            response = self.llm_c.invoke(messages)
            print(f"‚úÖ LLMË∞ÉÁî®ÊàêÂäü")
            
            return {"process_user_input_messages": [response]}
            
        except Exception as e:
            print(f"‚ùå LLMË∞ÉÁî®Â§±Ë¥•: {type(e).__name__}: {e}")
            
            # Fallback response when LLM fails
            fallback_response = AIMessage(content="""
            {
                "summary": "Áî±‰∫éÁΩëÁªúËøûÊé•ÈóÆÈ¢òÔºåÊó†Ê≥ïÂÆåÊàêÊô∫ËÉΩÂàÜÊûê„ÄÇÁî®Êà∑Êú¨ËΩÆÊèê‰æõ‰∫ÜËæìÂÖ•‰ø°ÊÅØ„ÄÇ",
                "next_node": "previous_node"
            }
            """)
            
            return {"process_user_input_messages": [fallback_response]}
    

    def run_process_user_input_agent(self, user_input: str, session_id: str = "1") -> None:
        """This function runs the process user input agent"""
        initial_state = self.create_initial_state(user_input, session_id)
        config = {"configurable": {"thread_id": session_id}}
        current_state = initial_state
        
        while True:
            try:
                has_interrupt = False
                for chunk in self.graph.stream(current_state, config = config, stream_mode = "updates"):
                    for node_name, node_output in chunk.items():
                        print(f"\nüìç Node: {node_name}")
                        print("-" * 30)

                        # check if there is an interrupt
                        if "__interrupt__" in chunk:
                            has_interrupt = True
                            interrupt_value = chunk['__interrupt__'][0].value
                            print(f"\nüí¨ Êô∫ËÉΩ‰Ωì: {interrupt_value}")
                            user_response = input("üë§ ËØ∑ËæìÂÖ•ÊÇ®ÁöÑÂõûÂ§ç: ")

                            # set the next input
                            current_state = Command(resume=user_response)
                            break

                        if isinstance(node_output, dict):
                            if "messages" in node_output and node_output["messages"]:
                                latest_message = node_output["messages"][-1]
                                if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
                                    print(f"üí¨ Êô∫ËÉΩ‰ΩìÂõûÂ§ç: {latest_message.content}")

                            for key, value in node_output.items():
                                if key != "messages" and value:
                                    print(f"üìä {key}: {value}")
                        print("-" * 30)
                
                if not has_interrupt:
                    break

            
            except Exception as e:
                print(f"‚ùå Â§ÑÁêÜÁî®Êà∑ËæìÂÖ•Êó∂Âá∫Èîô: {e}")
                break



if __name__ == "__main__":
    agent = ProcessUserInputAgent()
    # save_graph_visualization(agent.graph, "process_user_input_graph.png")
    agent.run_process_user_input_agent("")

