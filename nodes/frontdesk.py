from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
import uuid
import json

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰å‰å°æ¥å¾…å‘˜çŠ¶æ€
class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    table_structure: dict
    table_info: dict
    additonal_requirements: dict
    gather_complete: bool
    has_template: bool
    user_input: str

class FrontDeskAgent:
    """
    åŸºäºLangGraphçš„AIä»£ç†ç³»ç»Ÿï¼Œç”¨äºåˆ¤æ–­ç”¨æˆ·æ˜¯å¦ç»™å‡ºäº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ï¼Œå¹¶å¸®åŠ©ç”¨æˆ·æ±‡æ€»è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿
    """

    def __init__(self, model_name: str = "gpt-4o", checkpoint_path: str = "checkpoints.db"):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
        self.memory = MemorySaver()
        self.tools = []
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»ºç”Ÿæˆè¡¨æ ¼çš„LangGraphçŠ¶æ€å›¾"""

        workflow = StateGraph(FrontdeskState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_template", self._check_template_node)
        workflow.add_node("gather_requirements", self._gather_requirements_node)
        workflow.add_node("collect_input", self._gather_user_input)
        workflow.add_node("store_information", self._store_information_node)

        # å…¥å£èŠ‚ç‚¹
        workflow.set_entry_point("check_template")

        # è¿æ¥èŠ‚ç‚¹
        workflow.add_conditional_edges(
            "check_template",
            self._route_after_template_check,
            {
                "has_template": "store_information",
                "no_template": "gather_requirements"
            }
        )

        workflow.add_conditional_edges(
            "gather_requirements",
            self._route_after_requirements,
            {
                "complete": "store_information",
                "continue": "collect_input"
            }
        )

        workflow.add_edge("collect_input", "gather_requirements")
        workflow.add_edge("store_information", END)
        
        return workflow.compile(checkpointer = self.memory)
        
    def _check_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿"""

        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼æ¨¡æ¿è¯†åˆ«ä¸“å®¶ï¼Œè´Ÿè´£å‡†ç¡®åˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç»æä¾›äº†å®Œæ•´çš„è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ã€‚

        **åˆ¤æ–­æ ‡å‡†ï¼š**
        ç”¨æˆ·æä¾›äº†è¡¨æ ¼æ¨¡æ¿å½“ä¸”ä»…å½“æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š

        1. **ç»“æ„åŒ–æè¿°**ï¼šç”¨æˆ·æ¸…æ™°ã€è¯¦ç»†åœ°æè¿°äº†è¡¨æ ¼çš„å®Œæ•´ç»“æ„ï¼ŒåŒ…æ‹¬ï¼š
           - æ˜ç¡®çš„è¡¨å¤´åç§°å’Œå±‚çº§å…³ç³»
           - æ¯ä¸ªå­—æ®µçš„å…·ä½“å«ä¹‰å’Œæ•°æ®ç±»å‹
           - è¡¨æ ¼çš„æ•´ä½“å¸ƒå±€å’Œç»„ç»‡æ–¹å¼
           
        2. **æ–‡ä»¶æ¨¡æ¿**ï¼šç”¨æˆ·æä¾›äº†åŒ…å«è¡¨æ ¼ç»“æ„çš„æ–‡ä»¶ï¼Œå¦‚ï¼š
           - Excelæ–‡ä»¶(.xlsx, .xls)
           - CSVæ¨¡æ¿æ–‡ä»¶
           - PDFæ–‡æ¡£ä¸­çš„è¡¨æ ¼æ ·å¼
           - å›¾ç‰‡ä¸­çš„è¡¨æ ¼æˆªå›¾
           
        3. **å…·ä½“ç¤ºä¾‹**ï¼šç”¨æˆ·ç»™å‡ºäº†è¡¨æ ¼çš„å…·ä½“ç¤ºä¾‹ï¼ŒåŒ…å«ï¼š
           - å®Œæ•´çš„è¡¨å¤´ç»“æ„
           - ç¤ºä¾‹æ•°æ®è¡Œ
           - æ ¼å¼è¦æ±‚å’Œè§„èŒƒ

        **ä¸ç¬¦åˆæ¡ä»¶çš„æƒ…å†µï¼š**
        - ä»…æè¿°è¡¨æ ¼ç”¨é€”æˆ–ç›®çš„
        - åªæåˆ°éœ€è¦å“ªäº›ä¿¡æ¯ç±»åˆ«ï¼Œä½†æœªå…·ä½“åŒ–è¡¨å¤´
        - æ¨¡ç³Šçš„éœ€æ±‚æè¿°
        - è¯¢é—®å¦‚ä½•åˆ¶ä½œè¡¨æ ¼

        **è¾“å‡ºè¦æ±‚ï¼š**
        - å¦‚æœç”¨æˆ·æä¾›äº†ç¬¦åˆä¸Šè¿°æ ‡å‡†çš„å®Œæ•´è¡¨æ ¼æ¨¡æ¿ï¼Œè¯·å›ç­” [YES]
        - å¦‚æœç”¨æˆ·æœªæä¾›å®Œæ•´æ¨¡æ¿æˆ–æè¿°ä¸å¤Ÿå…·ä½“ï¼Œè¯·å›ç­” [NO]
        - å¦‚æœæœ‰ä»»ä½•ä¸ç¡®å®šçš„åœ°æ–¹ï¼Œå€¾å‘äºå›ç­” [NO]

        **åˆ†æè¿‡ç¨‹ï¼š**
        è¯·ä»”ç»†åˆ†æç”¨æˆ·è¾“å…¥ï¼Œè€ƒè™‘æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ç»“æ„åŒ–ä¿¡æ¯æ¥ç›´æ¥ç”Ÿæˆè¡¨æ ¼ã€‚
        """
        system_message = SystemMessage(content=system_prompt)

        latest_message = [system_message] + [state["messages"][-1]] if state["messages"] else [system_message]

        response = self.llm.invoke(latest_message)

        has_template = "[YES]" in response.content.upper()

        return {
            "has_template": has_template,
            "messages": [AIMessage(content = f"æ¨¡æ¿è¯†åˆ«ç»“æœï¼š{"å·²æä¾›å®Œæ•´æ¨¡æ¿" if has_template else "æœªæä¾›å®Œæ•´æ¨¡æ¿ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¶é›†ä¿¡æ¯"}")]
        }

    def _route_after_template_check(self, state: FrontdeskState) -> str:
        """æ ¹æ®æ¨¡æ¿æ£€æŸ¥ç»“æœè·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return "has_template" if state["has_template"] else "no_template"

    def _gather_requirements_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·å¯¹è¯ç¡®å®šç”Ÿæˆè¡¨æ ¼çš„å†…å®¹ï¼Œè¦æ±‚ç­‰"""

        # If gather_complete is already True, don't override it
        if state.get("gather_complete", False):
            return {
                "messages": state["messages"],
                "gather_complete": True
            }

        system_prompt_text = """ä½ ä½œä¸ºä¸€ä¸ªèµ„æ·±çš„excelè¡¨æ ¼è®¾è®¡ä¸“å®¶ï¼Œç°åœ¨éœ€è¦é€šè¿‡å’Œç”¨æˆ·å¯¹è¯çš„æ–¹å¼äº†è§£ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶é€šè¿‡å‘æ•£å››ç»´
         ä¸€æ­¥ä¸€æ­¥å¸®ç”¨æˆ·è®¾è®¡å‡ºexcelè¡¨æ ¼ï¼Œä½ éœ€è¦å¼„æ¸…æ¥šä»¥ä¸‹é—®é¢˜

         -è¿™ä¸ªè¡¨æ ¼æ˜¯ç”¨æ¥å¹²ä»€ä¹ˆçš„
         -éœ€è¦æ”¶é›†å“ªäº›ä¿¡æ¯
         -è¡¨æ ¼éƒ½æ¶‰åŠåˆ°å“ªäº›è¡¨å¤´ï¼Œæ˜¯å¦å­˜åœ¨å¤šçº§è¡¨å¤´
         -éœ€è¦ç”¨åˆ°å“ªäº›æ•°æ®

         è¯·ä¸€æ¬¡åªé—®1-2ä¸ªé—®é¢˜ï¼Œè®©å¯¹è¯è‡ªç„¶è¿›è¡Œ

        ä½ ä¹Ÿå¯ä»¥ç»™å‡ºç”¨æˆ·ä¸€äº›å»ºè®®å¹¶è¯¢é—®ç”¨æˆ·æ˜¯å¦é‡‡çº³ã€‚
        å½“ä½ è®¤ä¸ºä¿¡æ¯æ”¶é›†å®Œæ•´æ—¶ï¼Œè¯·åœ¨å›å¤æœ€ååŠ ä¸Š [COMPLETE] æ ‡è®°ï¼Œå¹¶æ€»ç»“è¡¨æ ¼ä¿¡æ¯ã€‚
        """

        messages = state["messages"]

        # åˆ¤æ–­æ˜¯å¦å·²æä¾›ç³»ç»Ÿæç¤ºè¯
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content = system_prompt_text)] + messages

        # Add the current user input if available
        if state.get("user_input"):
            messages.append(HumanMessage(content = state["user_input"]))

        response = self.llm.invoke(messages)

        gather_complete = "[COMPLETE]" in response.content

        return {
            "messages": [response],
            "gather_complete": gather_complete    
        }

    def _route_after_requirements(self, state: FrontdeskState) -> str:
        """æ ¹æ®éœ€æ±‚æ”¶é›†å®ŒæˆçŠ¶æ€è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return "complete" if state["gather_complete"] else "continue"

    def _gather_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """ç”¨æˆ·å’Œagentå¯¹è¯ç¡®è®¤ä¿¡æ¯ï¼Œæˆ–æä¾›é¢å¤–ä¿¡æ¯ç”¨äºæ™ºèƒ½ä½“æ”¶é›†è¡¨æ ¼ä¿¡æ¯"""

        try:
            user_input = input("ç”¨æˆ·ï¼š")
            return {
                "user_input": user_input
            }
        except EOFError:
            # Handle non-interactive environments
            print("âš ï¸  éäº¤äº’å¼ç¯å¢ƒï¼Œæ— æ³•è·å–ç”¨æˆ·è¾“å…¥")
            return {
                "user_input": "",
                "gather_complete": True  # Force completion to avoid infinite loop
            }
    
    def _route_after_gather(self, state: FrontdeskState) -> str:
        """æ ¹æ®"gather_complete"çš„å€¼è¿”å›ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""

        gather_complete = state["gather_complete"]

        if gather_complete:
            return "ready"

        return "collect_input"
    
    def _store_information_node(self, state: FrontdeskState) -> FrontdeskState:
        """å°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»“æ„åŒ–å‚¨å­˜"""

        # Check if we have enough conversation context
        conversation_length = len([msg for msg in state["messages"] if isinstance(msg, (HumanMessage, AIMessage))])
        
        if conversation_length < 2:
            # æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·åˆå§‹è¾“å…¥åˆ›ç«‹åŸºç¡€è¡¨æ ¼
            initial_input = state["user_input"] if state.get("user_input") else "æœªçŸ¥éœ€æ±‚"
            
            basic_template = {
                "table_info": {
                    "purpose": f"åŸºäºç”¨æˆ·è¾“å…¥åˆ›å»ºçš„è¡¨æ ¼ï¼š{initial_input}",
                    "description": "ç”¨æˆ·æä¾›çš„åŸºæœ¬éœ€æ±‚ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„",
                    "data_sources": ["ç”¨æˆ·è¾“å…¥"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "headers": [
                        {
                            "name": "å¾…å®šå­—æ®µ1",
                            "description": "éœ€è¦è¿›ä¸€æ­¥ç¡®å®šçš„è¡¨å¤´",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹æ•°æ®"
                        }
                    ],
                    "multi_level_headers": {
                        "level_1": []
                    }
                },
                "additional_requirements": {
                    "formatting": ["å¾…ç¡®å®š"],
                    "validation_rules": ["å¾…ç¡®å®š"],
                    "special_features": ["å¾…ç¡®å®š"]
                }
            }
            
            print("âš ï¸ å¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œç”ŸæˆåŸºç¡€æ¨¡æ¿")
            return {
                **state,
                "table_info": basic_template["table_info"],
                "table_structure": basic_template["table_structure"],
                "additional_requirements": basic_template["additional_requirements"],
                "gather_complete": True
            }

        system_prompt ="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®å¯¹è¯å†å²è®°å½•ï¼Œæˆ–ç”¨æˆ·æä¾›çš„è¡¨æ ¼æ¨¡æ¿ï¼Œ
        æå–å¹¶ç»“æ„åŒ–è¡¨æ ¼ç›¸å…³ä¿¡æ¯ã€‚

        **é‡è¦æé†’ï¼š**
        å¦‚æœå¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªåˆç†çš„åŸºç¡€ç»“æ„ï¼Œä¸è¦æ‹’ç»ç”Ÿæˆã€‚

        **ä»»åŠ¡è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–è¡¨æ ¼çš„ç”¨é€”ã€å†…å®¹ã€æ•°æ®éœ€æ±‚å’Œç»“æ„ä¿¡æ¯
        2. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
        3. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ•°æ®ç»“æ„è¾“å‡º

        **è¾“å‡ºæ ¼å¼ï¼š**
        è¯·ç›´æ¥è¾“å‡ºJSONå†…å®¹ï¼Œä¸è¦ä½¿ç”¨markdownä»£ç å—åŒ…è£…ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
        {
        "table_info": {
            "purpose": "è¡¨æ ¼çš„å…·ä½“ç”¨é€”å’Œç›®æ ‡",
            "description": "è¡¨æ ¼å†…å®¹çš„è¯¦ç»†æè¿°",
            "data_sources": ["æ•°æ®æ¥æº1", "æ•°æ®æ¥æº2"],
            "target_users": ["ç›®æ ‡ç”¨æˆ·ç¾¤ä½“"],
            "frequency": "ä½¿ç”¨é¢‘ç‡ï¼ˆå¦‚ï¼šæ¯æ—¥/æ¯å‘¨/æ¯æœˆï¼‰"
        },
        "table_structure": {
            "has_multi_level": false,
            "headers": [
            {
                "name": "è¡¨å¤´åç§°",
                "description": "è¡¨å¤´è¯´æ˜",
                "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                "required": true,
                "example": "ç¤ºä¾‹æ•°æ®"
            }
            ],
            "multi_level_headers": {
            "level_1": [
                {
                "name": "ä¸€çº§è¡¨å¤´åç§°",
                "description": "ä¸€çº§è¡¨å¤´è¯´æ˜",
                "children": [
                    {
                    "name": "äºŒçº§è¡¨å¤´åç§°",
                    "description": "äºŒçº§è¡¨å¤´è¯´æ˜",
                    "data_type": "æ•°æ®ç±»å‹",
                    "required": true,
                    "example": "ç¤ºä¾‹æ•°æ®"
                    }
                ]
                }
            ]
            }
        },
        "additional_requirements": {
            "formatting": ["æ ¼å¼è¦æ±‚"],
            "validation_rules": ["æ•°æ®éªŒè¯è§„åˆ™"],
            "special_features": ["ç‰¹æ®ŠåŠŸèƒ½éœ€æ±‚"]
        }
        }
        """
        print("æ­£åœ¨ç”Ÿæˆè¡¨æ ¼æ¨¡æ¿......")
        system_message = SystemMessage(content=system_prompt)
        filtered_messages = [
            msg for msg in state["messages"]
            if not isinstance(msg, SystemMessage)
        ]
        messages = [system_message] + filtered_messages
        response = self.llm.invoke(messages)

        try:
            # Clean the response content to handle markdown-wrapped JSON
            response_content = response.content.strip()
            
            # Remove markdown code block markers if present
            if response_content.startswith('```json'):
                response_content = response_content[7:]  # Remove ```json
            if response_content.startswith('```'):
                response_content = response_content[3:]   # Remove ```
            if response_content.endswith('```'):
                response_content = response_content[:-3]  # Remove trailing ```
            
            response_content = response_content.strip()
            
            # Parse the JSON response
            structured_output = json.loads(response_content)
            
            # Extract components
            table_info = structured_output["table_info"]
            table_structure = structured_output["table_structure"]
            additional_requirements = structured_output["additional_requirements"]
            
            print("âœ… JSONè§£ææˆåŠŸï¼Œè¡¨æ ¼æ¨¡æ¿ç”Ÿæˆå®Œæˆ")
            # Return updated state
            return {
                **state,
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "gather_complete": True
            }
        
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response.content}")
            print(f"æ¸…ç†åå“åº”: {response_content if 'response_content' in locals() else 'N/A'}")
            
            # Fallback: create a basic template from the conversation
            print("ğŸ”„ ä½¿ç”¨å¯¹è¯å†…å®¹ç”ŸæˆåŸºç¡€æ¨¡æ¿")
            fallback_template = {
                "table_info": {
                    "purpose": "æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆçš„è¡¨æ ¼",
                    "description": "åŸºäºç”¨æˆ·éœ€æ±‚çš„åŸºç¡€è¡¨æ ¼ç»“æ„",
                    "data_sources": ["ç”¨æˆ·å¯¹è¯"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "headers": [
                        {
                            "name": "åŸºç¡€å­—æ®µ",
                            "description": "æ ¹æ®å¯¹è¯æ¨æ–­çš„å­—æ®µ",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹"
                        }
                    ],
                    "multi_level_headers": {
                        "level_1": []
                    }
                },
                "additional_requirements": {
                    "formatting": ["æ ‡å‡†æ ¼å¼"],
                    "validation_rules": ["åŸºæœ¬éªŒè¯"],
                    "special_features": ["æ— ç‰¹æ®Šè¦æ±‚"]
                }
            }
            
            return {
                **state,
                "table_info": fallback_template["table_info"],
                "table_structure": fallback_template["table_structure"],
                "additional_requirements": fallback_template["additional_requirements"],
                "gather_complete": True
            }
        except KeyError as e:
            print(f"âŒ JSONç»“æ„é”™è¯¯: {e}")
            print(f"å¯ç”¨é”®: {list(structured_output.keys()) if 'structured_output' in locals() else 'N/A'}")
            
            return {
                **state,
                "gather_complete": False
            }

    def _create_initial_state(self, user_input: str, session_id: str = "default") -> FrontdeskState:
        """åˆ›å»ºLanggraphæœ€åˆçŠ¶æ€"""
        return {
            "messages": [HumanMessage(content=user_input)],
            "session_id": session_id,
            "table_structure": {},
            "table_info": {},
            "gather_complete": False,
            "has_template": False,
            "user_input": user_input,
            "additonal_requirements": {}
        }
    
    def run_front_desk_agent(self, user_input: str, session_id = "1") -> None: # session_idé»˜è®¤ä¸º1
        """æ‰§è¡Œå‰å°æ™ºèƒ½ä½“"""
        initial_state = self._create_initial_state(user_input, session_id)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ¤– Processing user input: {user_input}")
        print("=" * 50)

        for chunk in self.graph.stream(initial_state, config=config, stream_mode="updates"):
                for node_name, node_output in chunk.items():
                    print(f"\nğŸ“ Node: {node_name}")
                    print("-" * 30)
                    
                    if isinstance(node_output, dict):
                        if "messages" in node_output and node_output["messages"]:
                            latest_message = node_output["messages"][-1]
                            if hasattr(latest_message, 'content'):
                                print(f"ğŸ’¬ Response: {latest_message.content}")
                        
                        for key, value in node_output.items():
                            if key != "messages" and value:
                                print(f"ğŸ“Š {key}: {value}")
                    
                    print("-" * 30)

                
if __name__ == "__main__":

    #åˆ›å»ºæ™ºèƒ½ä½“
    frontdeskagent = FrontDeskAgent()

    # save_graph_visualization(frontdeskagent.graph)

    user_input = input("è¯·è¾“å…¥ä½ æƒ³ç”Ÿæˆçš„è¡¨æ ¼ï¼š")
    frontdeskagent.run_front_desk_agent(user_input)




        






        
