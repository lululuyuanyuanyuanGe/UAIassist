from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
import uuid
import json

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰å‰å°æ¥å¾…å‘˜çŠ¶æ€
class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    table_structure: dict
    table_info: dict
    additonal_requirements: dict
    gather_complete: bool
    has_template: bool
    user_input: str

class FrontDeskAgent:
    """
    åŸºäºLangGraphçš„AIä»£ç†ç³»ç»Ÿï¼Œç”¨äºåˆ¤æ–­ç”¨æˆ·æ˜¯å¦ç»™å‡ºäº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ï¼Œå¹¶å¸®åŠ©ç”¨æˆ·æ±‡æ€»è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿
    """

    def __init__(self, model_name: str = "gpt-4o", checkpoint_path: str = "checkpoints.db"):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
        self.memory = MemorySaver()
        self.tools = []
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»ºç”Ÿæˆè¡¨æ ¼çš„LangGraphçŠ¶æ€å›¾"""

        workflow = StateGraph(FrontdeskState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_template", self._check_template_node)
        workflow.add_node("gather_requirements", self._gather_requirements_node)
        workflow.add_node("collect_input", self._gather_user_input)
        workflow.add_node("store_information", self._store_information_node)

        # å…¥å£èŠ‚ç‚¹
        workflow.set_entry_point("check_template")

        # è¿æ¥èŠ‚ç‚¹
        workflow.add_conditional_edges(
            "check_template",
            self._route_after_template_check,
            {
                "has_template": "store_information",
                "no_template": "gather_requirements"
            }
        )

        workflow.add_conditional_edges(
            "gather_requirements",
            self._route_after_requirements,
            {
                "complete": "store_information",
                "continue": "collect_input"
            }
        )

        workflow.add_edge("collect_input", "gather_requirements")
        workflow.add_edge("store_information", END)
        
        return workflow.compile(checkpointer = self.memory)
        
    def _check_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿"""

        system_prompt = """
        ä½ ä½œä¸ºä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œç¬¬ä¸€æ­¥ä½ éœ€è¦åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æä¾›äº†è¡¨æ ¼çš„æ¨¡æ¿ï¼Œ
        åˆ¤æ–­è§„åˆ™å¦‚ä¸‹ï¼š
        1.ç”¨æˆ·æ¸…æ™°çš„æè¿°å‡ºäº†è¡¨æ ¼çš„ç»“æ„ï¼Œæ¯ä¸€çº§è¡¨å¤´æ ‡é¢˜
        2.ç”¨æˆ·æä¾›äº†è¡¨æ ¼å„å¼çš„æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯excelæ–‡ä»¶ï¼Œpdfç­‰é‡Œé¢æ¸…æ™°çš„å®šä¹‰äº†è¡¨æ ¼ç»“æ„
        å¦‚æœç”¨æˆ·æä¾›äº†è¡¨æ ¼æ¨¡æ¿åˆ™å›ç­”[YES]ï¼Œåä¹‹å›ç­”[NO]ï¼Œå¦‚æœä½ æœ‰ä»»ä½•ä¸æ¸…æ¥šçš„åœ°æ–¹ä¹Ÿéœ€è¦å›ç­”[NO]
        """
        system_message = SystemMessage(content=system_prompt)

        latest_message = [system_message] + [state["messages"][-1]] if state["messages"] else [system_message]

        response = self.llm.invoke(latest_message)

        has_template = "[YES]" in response.content.upper()

        return {
            "has_template": has_template,
            "messages": [AIMessage(content = f"æ˜¯å¦æä¾›æ¨¡æ¿ï¼š{"æ˜¯" if has_template else "å¦"}")]
        }

    def _route_after_template_check(self, state: FrontdeskState) -> str:
        """æ ¹æ®æ¨¡æ¿æ£€æŸ¥ç»“æœè·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return "has_template" if state["has_template"] else "no_template"

    def _gather_requirements_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·å¯¹è¯ç¡®å®šç”Ÿæˆè¡¨æ ¼çš„å†…å®¹ï¼Œè¦æ±‚ç­‰"""

        # If gather_complete is already True, don't override it
        if state.get("gather_complete", False):
            return {
                "messages": state["messages"],
                "gather_complete": True
            }

        system_prompt_text = """ä½ ä½œä¸ºä¸€ä¸ªèµ„æ·±çš„excelè¡¨æ ¼è®¾è®¡ä¸“å®¶ï¼Œç°åœ¨éœ€è¦é€šè¿‡å’Œç”¨æˆ·å¯¹è¯çš„æ–¹å¼äº†è§£ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶é€šè¿‡å‘æ•£å››ç»´
         ä¸€æ­¥ä¸€æ­¥å¸®ç”¨æˆ·è®¾è®¡å‡ºexcelè¡¨æ ¼ï¼Œä½ éœ€è¦å¼„æ¸…æ¥šä»¥ä¸‹é—®é¢˜

         -è¿™ä¸ªè¡¨æ ¼æ˜¯ç”¨æ¥å¹²ä»€ä¹ˆçš„
         -éœ€è¦æ”¶é›†å“ªäº›ä¿¡æ¯
         -è¡¨æ ¼éƒ½æ¶‰åŠåˆ°å“ªäº›è¡¨å¤´ï¼Œæ˜¯å¦å­˜åœ¨å¤šçº§è¡¨å¤´
         -éœ€è¦ç”¨åˆ°å“ªäº›æ•°æ®

         è¯·ä¸€æ¬¡åªé—®1-2ä¸ªé—®é¢˜ï¼Œè®©å¯¹è¯è‡ªç„¶è¿›è¡Œ

        ä½ ä¹Ÿå¯ä»¥ç»™å‡ºç”¨æˆ·ä¸€äº›å»ºè®®å¹¶è¯¢é—®ç”¨æˆ·æ˜¯å¦é‡‡çº³ã€‚
        å½“ä½ è®¤ä¸ºä¿¡æ¯æ”¶é›†å®Œæ•´æ—¶ï¼Œè¯·åœ¨å›å¤æœ€ååŠ ä¸Š [COMPLETE] æ ‡è®°ï¼Œå¹¶æ€»ç»“è¡¨æ ¼ä¿¡æ¯ã€‚
        """

        messages = state["messages"]

        # åˆ¤æ–­æ˜¯å¦å·²æä¾›ç³»ç»Ÿæç¤ºè¯
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content = system_prompt_text)] + messages

        # Add the current user input if available
        if state.get("user_input"):
            messages.append(HumanMessage(content = state["user_input"]))

        response = self.llm.invoke(messages)

        gather_complete = "[COMPLETE]" in response.content

        return {
            "messages": [response],
            "gather_complete": gather_complete    
        }

    def _route_after_requirements(self, state: FrontdeskState) -> str:
        """æ ¹æ®éœ€æ±‚æ”¶é›†å®ŒæˆçŠ¶æ€è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return "complete" if state["gather_complete"] else "continue"

    def _gather_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """ç”¨æˆ·å’Œagentå¯¹è¯ç¡®è®¤ä¿¡æ¯ï¼Œæˆ–æä¾›é¢å¤–ä¿¡æ¯ç”¨äºæ™ºèƒ½ä½“æ”¶é›†è¡¨æ ¼ä¿¡æ¯"""

        try:
            user_input = input("ç”¨æˆ·ï¼š")
            return {
                "user_input": user_input
            }
        except EOFError:
            # Handle non-interactive environments
            print("âš ï¸  éäº¤äº’å¼ç¯å¢ƒï¼Œæ— æ³•è·å–ç”¨æˆ·è¾“å…¥")
            return {
                "user_input": "",
                "gather_complete": True  # Force completion to avoid infinite loop
            }
    
    def _route_after_gather(self, state: FrontdeskState) -> str:
        """æ ¹æ®"gather_complete"çš„å€¼è¿”å›ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""

        gather_complete = state["gather_complete"]

        if gather_complete:
            return "ready"

        return "collect_input"
    
    def _store_information_node(self, state: FrontdeskState) -> FrontdeskState:
        """å°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»“æ„åŒ–å‚¨å­˜"""

        system_prompt ="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®å¯¹è¯å†å²è®°å½•ï¼Œæˆ–ç”¨æˆ·æä¾›çš„è¡¨æ ¼æ¨¡æ¿ï¼Œ
        æå–å¹¶ç»“æ„åŒ–è¡¨æ ¼ç›¸å…³ä¿¡æ¯ã€‚

        **ä»»åŠ¡è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–è¡¨æ ¼çš„ç”¨é€”ã€å†…å®¹ã€æ•°æ®éœ€æ±‚å’Œç»“æ„ä¿¡æ¯
        2. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
        3. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ•°æ®ç»“æ„è¾“å‡º

        **è¾“å‡ºæ ¼å¼ï¼š**
        ```json
        {
        "table_info": {
            "purpose": "è¡¨æ ¼çš„å…·ä½“ç”¨é€”å’Œç›®æ ‡",
            "description": "è¡¨æ ¼å†…å®¹çš„è¯¦ç»†æè¿°",
            "data_sources": ["æ•°æ®æ¥æº1", "æ•°æ®æ¥æº2"],
            "target_users": ["ç›®æ ‡ç”¨æˆ·ç¾¤ä½“"],
            "frequency": "ä½¿ç”¨é¢‘ç‡ï¼ˆå¦‚ï¼šæ¯æ—¥/æ¯å‘¨/æ¯æœˆï¼‰"
        },
        "table_structure": {
            "has_multi_level": false,
            "headers": [
            {
                "name": "è¡¨å¤´åç§°",
                "description": "è¡¨å¤´è¯´æ˜",
                "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                "required": true,
                "example": "ç¤ºä¾‹æ•°æ®"
            }
            ],
            "multi_level_headers": {
            "level_1": [
                {
                "name": "ä¸€çº§è¡¨å¤´åç§°",
                "description": "ä¸€çº§è¡¨å¤´è¯´æ˜",
                "children": [
                    {
                    "name": "äºŒçº§è¡¨å¤´åç§°",
                    "description": "äºŒçº§è¡¨å¤´è¯´æ˜",
                    "data_type": "æ•°æ®ç±»å‹",
                    "required": true,
                    "example": "ç¤ºä¾‹æ•°æ®"
                    }
                ]
                }
            ]
            }
        },
        "additional_requirements": {
            "formatting": ["æ ¼å¼è¦æ±‚"],
            "validation_rules": ["æ•°æ®éªŒè¯è§„åˆ™"],
            "special_features": ["ç‰¹æ®ŠåŠŸèƒ½éœ€æ±‚"]
        }
        }
        """

        system_message = SystemMessage(content=system_prompt)
        messages = [system_message] + state["messages"]
        response = self.llm.invoke(messages)

        try:
            # Parse the JSON response
            structured_output = json.loads(response.content)
            
            # Extract components
            table_info = structured_output["table_info"]
            table_structure = structured_output["table_structure"]
            additional_requirements = structured_output["additional_requirements"]
            
            # Return updated state
            return {
                **state,
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "gather_complete": True
            }
        
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON response: {e}")
            print(f"Raw response: {response.content}")
            
            # Return state with error indication
            return {
                **state,
                "gather_complete": False
            }
        except KeyError as e:
            print(f"Missing key in JSON response: {e}")
            print(f"Available keys: {list(structured_output.keys()) if 'structured_output' in locals() else 'N/A'}")
            
            return {
                **state,
                "gather_complete": False
            }

    def _create_initial_state(self, user_input: str, session_id: str = "default") -> FrontdeskState:
        """åˆ›å»ºLanggraphæœ€åˆçŠ¶æ€"""
        return {
            "messages": [HumanMessage(content=user_input)],
            "session_id": session_id,
            "table_structure": {},
            "table_info": {},
            "gather_complete": False,
            "has_template": False,
            "user_input": user_input,
            "additonal_requirements": {}
        }
    
    def run_front_desk_agent(self, user_input: str, session_id = "1") -> None: # session_idé»˜è®¤ä¸º1
        """æ‰§è¡Œå‰å°æ™ºèƒ½ä½“"""
        initial_state = self._create_initial_state(user_input, session_id)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ¤– Processing user input: {user_input}")
        print("=" * 50)

        for chunk in self.graph.stream(initial_state, config=config, stream_mode="updates"):
                for node_name, node_output in chunk.items():
                    print(f"\nğŸ“ Node: {node_name}")
                    print("-" * 30)
                    
                    if isinstance(node_output, dict):
                        if "messages" in node_output and node_output["messages"]:
                            latest_message = node_output["messages"][-1]
                            if hasattr(latest_message, 'content'):
                                print(f"ğŸ’¬ Response: {latest_message.content}")
                        
                        for key, value in node_output.items():
                            if key != "messages" and value:
                                print(f"ğŸ“Š {key}: {value}")
                    
                    print("-" * 30)

                
if __name__ == "__main__":

    #åˆ›å»ºæ™ºèƒ½ä½“
    frontdeskagent = FrontDeskAgent()

    user_input = input("è¯·è¾“å…¥ä½ æƒ³ç”Ÿæˆçš„è¡¨æ ¼ï¼š")
    frontdeskagent.run_front_desk_agent(user_input)




        






        
