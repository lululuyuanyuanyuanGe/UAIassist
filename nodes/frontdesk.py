from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utilities.visualize_graph import save_graph_visualization
from utilities.message_process import build_BaseMessage_type, create_assistant_with_files, filter_out_system_messages, detect_and_process_file_paths
import uuid
import json
import os
# Create an interactive chatbox using gradio
import gradio as gr
from dotenv import load_dotenv
import re

load_dotenv()

# ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
from openai import OpenAI
client = OpenAI(
    api_key = os.environ.get("OPENAI_API_KEY")
)

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, Interrupt, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

@tool
def upload_file_to_LLM():
    """ç”¨äºå°†ç”¨æˆ·è¾“å…¥çš„æ–‡ä»¶ä¸Šä¼ ç»™å¤§æ¨¡å‹"""
    pass

# å®šä¹‰å‰å°æ¥å¾…å‘˜çŠ¶æ€
class FrontdeskState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    table_structure: dict
    table_info: dict
    additional_requirements: dict
    gather_complete: bool
    has_template: bool
    complete_confirm: bool
    uploaded_files: list  # Add support for tracking uploaded files
    previous_node: str  # Track the previous node before file upload

class FrontDeskAgent:
    """
    åŸºäºLangGraphçš„AIä»£ç†ç³»ç»Ÿï¼Œç”¨äºåˆ¤æ–­ç”¨æˆ·æ˜¯å¦ç»™å‡ºäº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ï¼Œå¹¶å¸®åŠ©ç”¨æˆ·æ±‡æ€»è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿
    æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æ¡£ã€å›¾ç‰‡ç­‰ï¼‰
    """

    def __init__(self, model_name: str = "gpt-4o", checkpoint_path: str = "checkpoints.db"):
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
        self.tools = [upload_file_to_LLM]
        self.llm_with_tool = self.llm.bind_tools(self.tools)
        self.memory = MemorySaver()
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»ºç”Ÿæˆè¡¨æ ¼çš„LangGraphçŠ¶æ€å›¾"""

        workflow = StateGraph(FrontdeskState)

        # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
        file_upload_node = ToolNode(self.tools)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_template", self._check_template_node)
        workflow.add_node("confirm_template", self._confirm_template_node)
        workflow.add_node("gather_requirements", self._gather_requirements_node)
        workflow.add_node("store_information", self._store_information_node)
        workflow.add_node("collect_input", self._gather_user_input)
        workflow.add_node("collect_template_supplement", self._gather_user_template_supplement)
        workflow.add_node("file_upload_tool", file_upload_node)


        # å…¥å£èŠ‚ç‚¹
        workflow.set_entry_point("check_template")

        # è¿æ¥èŠ‚ç‚¹
        # æ£€æµ‹èŠ‚ç‚¹åˆ¤æ–­ç”¨æˆ·æä¾›äº†æ–‡ä»¶éœ€è¦ä¸Šä¼ 
        workflow.add_conditional_edges(
            "check_template",
            self._route_after_template_check,
            {
                "has_template": "confirm_template",
                "has_file_upload": "file_upload_tool",
                "no_template": "gather_requirements"
            }
        )

        # å½“æ¨¡æ¿æä¾›æ—¶å’Œç”¨æˆ·ç¡®è®¤
        workflow.add_conditional_edges(
            "confirm_template",
            self._route_after_template_confirm,
            {
                "complete_confirm": "store_information",
                "incomplete_confirm": "collect_template_supplement"
            }
        )

        # collect_template_supplementæ—¶ç”¨æˆ·å¯èƒ½ä¸Šä¼ æ–‡ä»¶
        workflow.add_conditional_edges(
            "collect_template_supplement",
            self._route_after_collect_template_supplement,
            {
                "continue_confirm": "confirm_template",
                "upload_file": "file_upload_tool"
            }
        )

        # collect_inputæ—¶ç”¨æˆ·å¯èƒ½ä¸Šä¼ æ–‡ä»¶
        workflow.add_conditional_edges(
            "collect_input",
            self._route_after_collect_input,
            {
                "continue_gather": "gather_requirements",
                "upload_file": "file_upload_tool"
            }
        )
        
        # å½“æ¨¡æ¿æœªæä¾›æ—¶
        workflow.add_conditional_edges(
            "gather_requirements",
            self._route_after_gather_requirements,
            {
                "complete": "store_information",
                "continue": "collect_input"
            }
        )

        # æ–‡ä»¶ä¸Šä¼ å·¥å…·å¤„ç†å®Œåçš„è·¯ç”±
        workflow.add_conditional_edges(
            "file_upload_tool",
            self._route_after_file_upload,
            {
                "check_template": "check_template",
                "confirm_template": "confirm_template", 
                "gather_requirements": "gather_requirements",
                "collect_template_supplement": "collect_template_supplement",
                "collect_input": "collect_input"
            }
        )

        workflow.add_edge("store_information", END)
        
        return workflow.compile(checkpointer = self.memory)

    def _check_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""

        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼æ¨¡æ¿è¯†åˆ«ä¸“å®¶ï¼Œè´Ÿè´£å‡†ç¡®åˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç»æä¾›äº†å®Œæ•´çš„è¡¨æ ¼ç”Ÿæˆæ¨¡æ¿ã€‚
        ä½ éœ€è¦åˆ†æç”¨æˆ·çš„æ–‡æœ¬æè¿°ä»¥åŠä»–ä»¬ä¸Šä¼ çš„ä»»ä½•æ–‡ä»¶ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€æ–‡æ¡£ç­‰ï¼‰ã€‚

        **åˆ¤æ–­æ ‡å‡†ï¼š**
        ç”¨æˆ·æä¾›äº†è¡¨æ ¼æ¨¡æ¿å½“ä¸”ä»…å½“æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š

        1. **ç»“æ„åŒ–æè¿°**ï¼šç”¨æˆ·æ¸…æ™°ã€è¯¦ç»†åœ°æè¿°äº†è¡¨æ ¼çš„å®Œæ•´ç»“æ„ï¼ŒåŒ…æ‹¬ï¼š
           - æ˜ç¡®çš„è¡¨å¤´åç§°å’Œå±‚çº§å…³ç³»
           - æ¯ä¸ªå­—æ®µçš„å…·ä½“å«ä¹‰å’Œæ•°æ®ç±»å‹
           - è¡¨æ ¼çš„æ•´ä½“å¸ƒå±€å’Œç»„ç»‡æ–¹å¼
           
        2. **æ–‡ä»¶æ¨¡æ¿**ï¼šç”¨æˆ·æä¾›äº†åŒ…å«è¡¨æ ¼ç»“æ„çš„æ–‡ä»¶ï¼Œå¦‚ï¼š
           - Excelæ–‡ä»¶(.xlsx, .xls) - åŒ…å«å…·ä½“çš„è¡¨å¤´å’Œæ•°æ®ç»“æ„
           - CSVæ¨¡æ¿æ–‡ä»¶ - æœ‰æ˜ç¡®çš„åˆ—åå’Œæ ¼å¼
           - PDFæ–‡æ¡£ä¸­çš„è¡¨æ ¼æ ·å¼ - æ˜¾ç¤ºå®Œæ•´çš„è¡¨æ ¼å¸ƒå±€
           - å›¾ç‰‡ä¸­çš„è¡¨æ ¼æˆªå›¾ - èƒ½æ¸…æ™°çœ‹åˆ°è¡¨å¤´å’Œç»“æ„
           
        3. **å…·ä½“ç¤ºä¾‹**ï¼šç”¨æˆ·ç»™å‡ºäº†è¡¨æ ¼çš„å…·ä½“ç¤ºä¾‹ï¼ŒåŒ…å«ï¼š
           - å®Œæ•´çš„è¡¨å¤´ç»“æ„
           - ç¤ºä¾‹æ•°æ®è¡Œ
           - æ ¼å¼è¦æ±‚å’Œè§„èŒƒ

        **ç‰¹åˆ«æ³¨æ„æ–‡ä»¶ç±»å‹ï¼š**
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†Excelæ–‡ä»¶(.xlsx, .xls)ï¼Œè¯·ä»”ç»†åˆ†æå…¶ä¸­çš„è¡¨å¤´ç»“æ„å’Œæ•°æ®æ ¼å¼
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡æ–‡ä»¶ï¼Œåˆ†æå›¾ç‰‡ä¸­æ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡æ¡£æ–‡ä»¶ï¼Œè€ƒè™‘å…¶å¯èƒ½åŒ…å«çš„è¡¨æ ¼æ¨¡æ¿ä¿¡æ¯

        **ä¸ç¬¦åˆæ¡ä»¶çš„æƒ…å†µï¼š**
        - ä»…æè¿°è¡¨æ ¼ç”¨é€”æˆ–ç›®çš„
        - åªæåˆ°éœ€è¦å“ªäº›ä¿¡æ¯ç±»åˆ«ï¼Œä½†æœªå…·ä½“åŒ–è¡¨å¤´
        - æ¨¡ç³Šçš„éœ€æ±‚æè¿°
        - è¯¢é—®å¦‚ä½•åˆ¶ä½œè¡¨æ ¼
        - ä¸Šä¼ çš„æ–‡ä»¶ä¸è¡¨æ ¼è®¾è®¡æ— å…³

        **è¾“å‡ºè¦æ±‚ï¼š**
        - å¦‚æœç”¨æˆ·æä¾›äº†ç¬¦åˆä¸Šè¿°æ ‡å‡†çš„å®Œæ•´è¡¨æ ¼æ¨¡æ¿ï¼Œè¯·å›ç­” [YES]
        - å¦‚æœç”¨æˆ·æœªæä¾›å®Œæ•´æ¨¡æ¿æˆ–æè¿°ä¸å¤Ÿå…·ä½“ï¼Œè¯·å›ç­” [NO]
        - å¦‚æœæœ‰ä»»ä½•ä¸ç¡®å®šçš„åœ°æ–¹ï¼Œå€¾å‘äºå›ç­” [NO]

        **åˆ†æè¿‡ç¨‹ï¼š**
        è¯·ä»”ç»†åˆ†æç”¨æˆ·è¾“å…¥å’Œä¸Šä¼ çš„æ–‡ä»¶ï¼Œè€ƒè™‘æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ç»“æ„åŒ–ä¿¡æ¯æ¥ç›´æ¥ç”Ÿæˆè¡¨æ ¼ã€‚
        å¦‚æœç”¨æˆ·ä¸Šä¼ äº†Excelæ–‡ä»¶ï¼Œè¯·ä½¿ç”¨pandasç­‰å·¥å…·åˆ†ææ–‡ä»¶ç»“æ„ï¼ŒæŸ¥çœ‹è¡¨å¤´ã€æ•°æ®ç±»å‹ã€è¡Œæ•°ç­‰ä¿¡æ¯ã€‚

        **æ³¨æ„äº‹é¡¹**
        å¦‚æœä½ è®¤ä¸ºç”¨æˆ·å½“å‰çš„ä¿¡æ¯ä¸å¤Ÿå®Œæ•´ï¼Œæˆ–è€…ä½ éœ€è¦ä¸€äº›è¡¥å……ä¹Ÿè¦å›ç­” [NO]
        """
        
        # è·å–ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
        user_message = state["messages"][-1] if state["messages"] else HumanMessage(content="")
        file_paths = state.get("uploaded_files", [])

        # æ£€æŸ¥æ˜¯å¦ä¸Šä¼ äº†æ–‡ä»¶
        if file_paths:
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨Assistants APIåˆ†æ {len(file_paths)} ä¸ªæ–‡ä»¶...")
            try:
                # ä½¿ç”¨æ–°çš„Assistants APIæ–¹æ³•
                result = create_assistant_with_files(
                    client=client,
                    file_paths=file_paths,
                    user_input=user_message.content,
                    system_prompt=system_prompt
                )
                
                response_content = result["response"]
                print("âœ… Assistants APIæ–‡ä»¶åˆ†æå®Œæˆ")
                
                # å°†åˆ†æç»“æœè½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼
                analysis_message = AIMessage(content=response_content)
                state["messages"].append(analysis_message)
                
            except Exception as e:
                print(f"âŒ Assistants APIåˆ†æå¤±è´¥: {e}")
                print("ğŸ”„ å›é€€åˆ°æ–‡æœ¬åˆ†ææ¨¡å¼")
                # å›é€€åˆ°æ–‡æœ¬åˆ†æ
                messages = [SystemMessage(content=system_prompt), user_message]
                response = self.llm.invoke(messages)
                response_content = response.content
        else:
            # æ„å»ºæ­£ç¡®çš„æ¶ˆæ¯åˆ—è¡¨
            messages = [SystemMessage(content=system_prompt), user_message]
            response = self.llm.invoke(messages)
            response_content = response.content

        has_template = "[YES]" in response_content.upper()
        
        return {
            "has_template": has_template,
            "messages": [AIMessage(content=response_content)]
        }

    def _route_after_template_check(self, state: FrontdeskState) -> str:
        """ç”¨æˆ·æä¾›å¤–éƒ¨æ–‡ä»¶æ—¶è¿”å›å·¥å…·èŠ‚ç‚¹è·¯ç”±ï¼Œæ²¡æœ‰å¤–éƒ¨æ–‡ä»¶åˆ™æ­£å¸¸åˆ¤æ–­"""
        if state.get("uploaded_files"):
            # Set previous node before going to file upload
            state["previous_node"] = "check_template"
            return "has_file_upload"
        return "has_template" if state["has_template"] else "no_template"
    
    def _route_after_collect_template_supplement(self, state: FrontdeskState) -> str:
        """æ¨¡æ¿è¡¥å……æ”¶é›†åçš„è·¯ç”±å†³ç­– - æ£€æµ‹ç”¨æˆ·æ˜¯å¦æä¾›äº†æ–°æ–‡ä»¶"""
        # æ£€æµ‹æœ€æ–°ç”¨æˆ·æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„
        if state.get("messages"):
            latest_message = state["messages"][-1]
            if isinstance(latest_message, HumanMessage):
                # æ£€æµ‹å¹¶å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„
                detected_files = detect_and_process_file_paths(latest_message.content)
                if detected_files:
                    # æ›´æ–°çŠ¶æ€ä¸­çš„ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
                    current_files = state.get("uploaded_files", [])
                    state["uploaded_files"] = current_files + detected_files
                    # Set previous node before going to file upload
                    state["previous_node"] = "collect_template_supplement"
                    return "upload_file"
        
        return "continue_confirm"

    def _route_after_collect_input(self, state: FrontdeskState) -> str:
        """ç”¨æˆ·è¾“å…¥æ”¶é›†åçš„è·¯ç”±å†³ç­– - æ£€æµ‹ç”¨æˆ·æ˜¯å¦æä¾›äº†æ–°æ–‡ä»¶"""
        # æ£€æµ‹æœ€æ–°ç”¨æˆ·æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„
        if state.get("messages"):
            latest_message = state["messages"][-1]
            if isinstance(latest_message, HumanMessage):
                # æ£€æµ‹å¹¶å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„
                detected_files = detect_and_process_file_paths(latest_message.content)
                if detected_files:
                    # æ›´æ–°çŠ¶æ€ä¸­çš„ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
                    current_files = state.get("uploaded_files", [])
                    state["uploaded_files"] = current_files + detected_files
                    # Set previous node before going to file upload
                    state["previous_node"] = "collect_input"
                    return "upload_file"
        
        return "continue_gather"

    def _confirm_template_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·ç¡®è®¤æ¨¡æ¿ç»†èŠ‚"""

        # If complete_confirm is already True, don't override it
        if state.get("complete_confirm", False):
            return {
                "messages": state["messages"],
                "complete_confirm": True
            }

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼æ¨¡æ¿å®¡æ ¸ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸»åŠ¨ä¸ç”¨æˆ·ç¡®è®¤å’Œå®Œå–„è¡¨æ ¼æ¨¡æ¿çš„è¯¦ç»†ä¿¡æ¯ã€‚

        **ä½ éœ€è¦æŒ‰é¡ºåºç¡®è®¤ä»¥ä¸‹ä¿¡æ¯ï¼š**
        1. **è¡¨æ ¼çš„ç”¨é€”å’Œç›®æ ‡**ï¼šç¡®è®¤è¡¨æ ¼çš„å…·ä½“ç”¨é€”ï¼Œç”¨æ¥åšä»€ä¹ˆï¼Ÿè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
        2. **éœ€è¦æ”¶é›†çš„å…·ä½“ä¿¡æ¯ç±»å‹**ï¼šç¡®è®¤æ‰€æœ‰æ•°æ®å­—æ®µï¼Œæ˜¯å¦æœ‰é—æ¼çš„é‡è¦å­—æ®µï¼Ÿ
        3. **è¡¨æ ¼ç»“æ„è®¾è®¡**ï¼šç¡®è®¤æ˜¯å¦éœ€è¦å¤šçº§è¡¨å¤´ï¼Ÿå¦‚ä½•åˆ†ç»„ï¼Ÿå±‚çº§å…³ç³»æ˜¯å¦åˆç†ï¼Ÿ
        4. **ç‰¹æ®Šè¦æ±‚**ï¼šç¡®è®¤æ ¼å¼ã€éªŒè¯è§„åˆ™ã€ç‰¹æ®ŠåŠŸèƒ½ç­‰

        **æ£€æŸ¥é‡ç‚¹ï¼š**
        - **è¡¨å¤´å®Œæ•´æ€§**ï¼šæ£€æŸ¥è¡¨å¤´æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Œæ˜¯å¦æœ‰æ­§ä¹‰æˆ–æ¨¡ç³Šçš„è¡¨è¿°
        - **æ•°æ®ç±»å‹æ˜ç¡®æ€§**ï¼šç¡®è®¤æ¯ä¸ªå­—æ®µçš„æ•°æ®ç±»å‹æ˜¯å¦æ˜ç¡®ï¼ˆæ–‡æœ¬ã€æ•°å­—ã€æ—¥æœŸç­‰ï¼‰
        - **å¿…å¡«å­—æ®µæ ‡è¯†**ï¼šç¡®è®¤å“ªäº›å­—æ®µæ˜¯å¿…å¡«çš„ï¼Œå“ªäº›æ˜¯å¯é€‰çš„
        - **æ•°æ®æ ¼å¼è§„èŒƒ**ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹å®šçš„æ•°æ®æ ¼å¼è¦æ±‚
        - **è¡¨æ ¼ç»“æ„é€»è¾‘**ï¼šéªŒè¯è¡¨æ ¼çš„å±‚çº§ç»“æ„æ˜¯å¦åˆç†
        - **ä¸šåŠ¡é€»è¾‘ä¸€è‡´æ€§**ï¼šç¡®ä¿è¡¨æ ¼è®¾è®¡ç¬¦åˆå®é™…ä¸šåŠ¡éœ€æ±‚

        **å¯¹è¯ç­–ç•¥ï¼š**
        - ä¸»åŠ¨è¯¢é—®ï¼Œä¸è¦è¢«åŠ¨ç­‰å¾…
        - ä¸€æ¬¡ç¡®è®¤1-2ä¸ªå…·ä½“é—®é¢˜ï¼Œé¿å…è®©ç”¨æˆ·æ„Ÿåˆ°å›°æ‰°
        - å¦‚æœå‘ç°ä»»ä½•ä¸ç¡®å®šæˆ–ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œè¯·å…·ä½“æŒ‡å‡ºå¹¶è¯¢é—®ç”¨æˆ·
        - æ ¹æ®ç”¨æˆ·å›ç­”ç»™å‡ºå»ºè®®å’Œé€‰é¡¹
        - å¦‚æœç”¨æˆ·å›ç­”æ¨¡ç³Šï¼Œè¿½é—®å…·ä½“ç»†èŠ‚
        - å½“ç¡®è®¤æ‰€æœ‰ä¿¡æ¯éƒ½æ¸…æ™°å®Œæ•´æ—¶ï¼Œä¸»åŠ¨æ€»ç»“å¹¶æ ‡è®° [COMPLETE]

        **åˆ¤æ–­å®Œæˆæ ‡å‡†ï¼š**
        å½“ä½ ç¡®è®¤äº†è¡¨æ ¼ç”¨é€”ã€æ‰€æœ‰å­—æ®µè¯¦æƒ…ã€ç»“æ„ç»„ç»‡æ–¹å¼ã€ç‰¹æ®Šè¦æ±‚åï¼Œåº”è¯¥ä¸»åŠ¨æ€»ç»“ä¿¡æ¯å¹¶åœ¨å›å¤æœ«å°¾åŠ ä¸Š [COMPLETE] æ ‡è®°ã€‚

        **ç¤ºä¾‹ç¡®è®¤æ ¼å¼ï¼š**
        "å¥½çš„ï¼Œæˆ‘å·²ç»ä»”ç»†å®¡æ ¸äº†æ‚¨çš„è¡¨æ ¼æ¨¡æ¿ï¼Œç°åœ¨è®©æˆ‘æ€»ç»“ç¡®è®¤çš„ä¿¡æ¯ï¼š
        - è¡¨æ ¼ç”¨é€”ï¼š[ç”¨é€”è¯´æ˜]
        - ä¸»è¦å­—æ®µï¼š[å­—æ®µåˆ—è¡¨]
        - ç»“æ„è®¾è®¡ï¼š[æè¿°è¡¨å¤´ç»„ç»‡]
        - ç‰¹æ®Šè¦æ±‚ï¼š[è¦æ±‚è¯´æ˜]
        æ‰€æœ‰ä¿¡æ¯éƒ½å·²ç¡®è®¤æ¸…æ¥šï¼Œç°åœ¨å¯ä»¥å¼€å§‹ç”Ÿæˆè¡¨æ ¼äº†ã€‚[COMPLETE]"

        **ç¤ºä¾‹è¡¥å……è¯¢é—®æ ¼å¼ï¼š**
        "æˆ‘æ³¨æ„åˆ°æ‚¨çš„æ¨¡æ¿ä¸­æœ‰å‡ ä¸ªåœ°æ–¹éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤ï¼š
        1. [å…·ä½“é—®é¢˜1]
        2. [å…·ä½“é—®é¢˜2]
        è¯·æ‚¨æä¾›æ›´å¤šç»†èŠ‚ï¼Œä»¥ä¾¿æˆ‘ä¸ºæ‚¨ç”Ÿæˆæ›´å‡†ç¡®çš„è¡¨æ ¼ã€‚"

        å½“æ¨¡æ¿ç¡®è®¤å®Œæˆåè¯·åœ¨å›å¤ç»“å°¾åŠ å…¥[COMPLETE]
        """

        messages = state["messages"].copy()

        # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt)] + messages

        response = self.llm.invoke(messages)
        complete_confirm = "[COMPLETE]" in response.content.upper()

        return{
            "complete_confirm": complete_confirm,
            "messages": [response]
        }
    
    # confirm template node's conditional check
    def _route_after_template_confirm(self, state: FrontdeskState) -> str:
        """æ ¹æ®æ˜¯å¦å®Œæˆæ ¼å¼æ ¡éªŒè·¯ç”±åˆ°ç›¸åº”èŠ‚ç‚¹"""
        return "complete_confirm" if state["complete_confirm"] else "incomplete_confirm"
    
    def _gather_user_template_supplement(self, state: FrontdeskState) -> FrontdeskState:
        """æ”¶é›†ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œæ¥ç¡®è®¤æ¨¡æ¿"""
        user_response = interrupt("è¯·ä¸ºæ¨¡æ¿æä¾›è¡¥å……ä¿¡æ¯: ")
        return {
            "messages": [HumanMessage(content=user_response)]
        }

    def _gather_requirements_node(self, state: FrontdeskState) -> FrontdeskState:
        """å’Œç”¨æˆ·å¯¹è¯ç¡®å®šç”Ÿæˆè¡¨æ ¼çš„å†…å®¹ï¼Œè¦æ±‚ç­‰ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥åˆ†æ"""

        # If gather_complete is already True, don't override it
        if state.get("gather_complete", False):
            return {
                "messages": state["messages"],
                "gather_complete": True
            }

        system_prompt_text = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„excelè¡¨æ ¼è®¾è®¡ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·å®Œæˆè¡¨æ ¼è®¾è®¡ã€‚
        ä½ å¯ä»¥åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€æ–‡æ¡£ã€Excelæ–‡ä»¶ç­‰ï¼‰æ¥æ›´å¥½åœ°ç†è§£ä»–ä»¬çš„éœ€æ±‚ã€‚

        **ä½ éœ€è¦æŒ‰é¡ºåºæ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š**
        1. è¡¨æ ¼çš„ç”¨é€”å’Œç›®æ ‡ï¼ˆç”¨æ¥åšä»€ä¹ˆï¼Ÿè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼‰
        2. éœ€è¦æ”¶é›†çš„å…·ä½“ä¿¡æ¯ç±»å‹ï¼ˆå“ªäº›æ•°æ®å­—æ®µï¼Ÿï¼‰ï¼Œå¯ä»¥å‘æ•£æ€ç»´é€‚å½“è¿½é—®ç”¨æˆ·è¡¥å……é¢å¤–æ•°æ®
        3. è¡¨æ ¼ç»“æ„è®¾è®¡ï¼ˆæ˜¯å¦éœ€è¦å¤šçº§è¡¨å¤´ï¼Ÿå¦‚ä½•åˆ†ç»„ï¼Ÿï¼‰
        4. ç‰¹æ®Šè¦æ±‚ï¼ˆæ ¼å¼ã€éªŒè¯è§„åˆ™ã€ç‰¹æ®ŠåŠŸèƒ½ç­‰ï¼‰

        **å¤šæ¨¡æ€åˆ†æèƒ½åŠ›ï¼š**
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡ï¼Œå°è¯•åˆ†æå›¾ç‰‡ä¸­çš„è¡¨æ ¼ç»“æ„æˆ–ç›¸å…³ä¿¡æ¯
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡æ¡£ï¼Œè€ƒè™‘æ–‡æ¡£ä¸­å¯èƒ½åŒ…å«çš„è¡¨æ ¼éœ€æ±‚æˆ–æ¨¡æ¿
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†Excel/CSVæ–‡ä»¶ï¼Œåˆ†æå…¶ç»“æ„ä½œä¸ºå‚è€ƒ

        **å¯¹è¯ç­–ç•¥ï¼š**
        - ä¸»åŠ¨è¯¢é—®ï¼Œä¸è¦è¢«åŠ¨ç­‰å¾…
        - ä¸€æ¬¡é—®1æˆ–2ä¸ªå…·ä½“é—®é¢˜
        - æ ¹æ®ç”¨æˆ·å›ç­”å’Œä¸Šä¼ çš„æ–‡ä»¶ç»™å‡ºå»ºè®®å’Œé€‰é¡¹
        - å¦‚æœç”¨æˆ·å›ç­”æ¨¡ç³Šï¼Œè¿½é—®å…·ä½“ç»†èŠ‚
        - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†ç›¸å…³æ–‡ä»¶ï¼Œä¸»åŠ¨æåŠå¹¶è¯¢é—®æ˜¯å¦åŸºäºæ–‡ä»¶å†…å®¹è®¾è®¡
        - å½“æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯è®¾è®¡å®Œæ•´è¡¨æ ¼æ—¶ï¼Œä¸»åŠ¨æ€»ç»“å¹¶æ ‡è®° [COMPLETE]

        **åˆ¤æ–­å®Œæˆæ ‡å‡†ï¼š**
        å½“ä½ æ˜ç¡®äº†è¡¨æ ¼ç”¨é€”ã€ä¸»è¦å­—æ®µã€ç»“æ„ç»„ç»‡æ–¹å¼åï¼Œåº”è¯¥ä¸»åŠ¨æ€»ç»“ä¿¡æ¯å¹¶åœ¨å›å¤æœ«å°¾åŠ ä¸Š [COMPLETE] æ ‡è®°ã€‚

        **ç¤ºä¾‹å®Œæˆæ€»ç»“æ ¼å¼ï¼š**
        "å¥½çš„ï¼Œæ ¹æ®æˆ‘ä»¬çš„è®¨è®ºå’Œæ‚¨æä¾›çš„æ–‡ä»¶ï¼Œæˆ‘å·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥è®¾è®¡è¿™ä¸ªè¡¨æ ¼ï¼š
        - ç”¨é€”ï¼š[æ€»ç»“ç”¨é€”]
        - ä¸»è¦å­—æ®µï¼š[åˆ—å‡ºå­—æ®µ]
        - ç»“æ„ï¼š[æè¿°è¡¨å¤´ç»„ç»‡]
        ç°åœ¨æˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†çš„è¡¨æ ¼ç»“æ„äº†ã€‚[COMPLETE]"
        """

        messages = state["messages"].copy()

        # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=system_prompt_text)] + messages

        response = self.llm.invoke(messages)
        gather_complete = "[COMPLETE]" in response.content

        return {
            "messages": [response],
            "gather_complete": gather_complete    
        }

    def _route_after_gather_requirements(self, state: FrontdeskState) -> str:
        """æ ¹æ®éœ€æ±‚æ”¶é›†å®ŒæˆçŠ¶æ€è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return "complete" if state["gather_complete"] else "continue"

    def _gather_user_input(self, state: FrontdeskState) -> FrontdeskState:
        """ç”¨æˆ·å’Œagentå¯¹è¯ç¡®è®¤ä¿¡æ¯ï¼Œæˆ–æä¾›é¢å¤–ä¿¡æ¯ç”¨äºæ™ºèƒ½ä½“æ”¶é›†è¡¨æ ¼ä¿¡æ¯"""
        user_response = interrupt("è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
        return {
            "messages": [HumanMessage(content=user_response)]
        }

    
    def _store_information_node(self, state: FrontdeskState) -> FrontdeskState:
        """å°†æ”¶é›†åˆ°çš„ä¿¡æ¯ç»“æ„åŒ–å‚¨å­˜"""

        # Check if we have enough conversation context
        conversation_length = len([msg for msg in state["messages"] if isinstance(msg, (HumanMessage, AIMessage))])
        
        if conversation_length < 2:
            # æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·åˆå§‹è¾“å…¥åˆ›ç«‹åŸºç¡€è¡¨æ ¼
            initial_input = state["messages"][0].content if state["messages"] else "æœªçŸ¥éœ€æ±‚"
            
            basic_template = {
                "table_info": {
                    "purpose": f"åŸºäºç”¨æˆ·è¾“å…¥åˆ›å»ºçš„è¡¨æ ¼ï¼š{initial_input}",
                    "description": "ç”¨æˆ·æä¾›çš„åŸºæœ¬éœ€æ±‚ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„",
                    "data_sources": ["ç”¨æˆ·è¾“å…¥"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "multi_level_headers": [
                        {
                            "name": "å¾…å®šå­—æ®µ1",
                            "description": "éœ€è¦è¿›ä¸€æ­¥ç¡®å®šçš„è¡¨å¤´",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹æ•°æ®"
                        }
                    ]
                },
                "additional_requirements": {
                    "formatting": ["å¾…ç¡®å®š"],
                    "validation_rules": ["å¾…ç¡®å®š"],
                    "special_features": ["å¾…ç¡®å®š"]
                }
            }
            
            print("âš ï¸ å¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œç”ŸæˆåŸºç¡€æ¨¡æ¿")
            return {
                **state,
                "table_info": basic_template["table_info"],
                "table_structure": basic_template["table_structure"],
                "additional_requirements": basic_template["additional_requirements"],
                "gather_complete": True
            }

        system_prompt ="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®å¯¹è¯å†å²è®°å½•ï¼Œæˆ–ç”¨æˆ·æä¾›çš„è¡¨æ ¼æ¨¡æ¿ï¼Œ
        æå–å¹¶ç»“æ„åŒ–è¡¨æ ¼ç›¸å…³ä¿¡æ¯ã€‚

        **é‡è¦æé†’ï¼š**
        å¦‚æœå¯¹è¯ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªåˆç†çš„åŸºç¡€ç»“æ„ï¼Œä¸è¦æ‹’ç»ç”Ÿæˆã€‚

        **ä»»åŠ¡è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–è¡¨æ ¼çš„ç”¨é€”ã€å†…å®¹ã€æ•°æ®éœ€æ±‚å’Œç»“æ„ä¿¡æ¯
        2. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
        3. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ•°æ®ç»“æ„è¾“å‡º

        **è¡¨æ ¼ç»“æ„è¯´æ˜ï¼š**
        - å¯¹äºå¤šçº§è¡¨å¤´ï¼Œä½¿ç”¨åµŒå¥—çš„æ•°ç»„å’Œå­—å…¸ç»“æ„
        - æ ‡é¢˜è¡¨å¤´ï¼ˆæœ‰å­çº§çš„ï¼‰åªåŒ…å« name å’Œ children å­—æ®µ
        - æ•°æ®è¡¨å¤´ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰åŒ…å« name, description, data_type, required, example å­—æ®µ
        - æ”¯æŒä»»æ„å±‚çº§çš„åµŒå¥—ç»“æ„

        **è¾“å‡ºæ ¼å¼ï¼š**
        è¯·ç›´æ¥è¾“å‡ºJSONå†…å®¹ï¼Œä¸è¦ä½¿ç”¨markdownä»£ç å—åŒ…è£…ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
        {
        "table_info": {
            "purpose": "è¡¨æ ¼çš„å…·ä½“ç”¨é€”å’Œç›®æ ‡",
            "description": "è¡¨æ ¼å†…å®¹çš„è¯¦ç»†æè¿°",
            "data_sources": ["æ•°æ®æ¥æº1", "æ•°æ®æ¥æº2"],
            "target_users": ["ç›®æ ‡ç”¨æˆ·ç¾¤ä½“"],
            "frequency": "ä½¿ç”¨é¢‘ç‡ï¼ˆå¦‚ï¼šæ¯æ—¥/æ¯å‘¨/æ¯æœˆï¼‰"
        },
        "table_structure": {
            "has_multi_level": true,
            "multi_level_headers": [
            {
                "name": "ç¬¬ä¸€çº§æ ‡é¢˜è¡¨å¤´åç§°",
                "children": [
                {
                    "name": "ç¬¬äºŒçº§æ ‡é¢˜è¡¨å¤´åç§°",
                    "children": [
                    {
                        "name": "æ•°æ®å­—æ®µåç§°",
                        "description": "æ•°æ®å­—æ®µè¯´æ˜",
                        "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                        "required": true,
                        "example": "ç¤ºä¾‹æ•°æ®"
                    }
                    ]
                },
                {
                    "name": "ç›´æ¥æ•°æ®å­—æ®µåç§°",
                    "description": "æ•°æ®å­—æ®µè¯´æ˜",
                    "data_type": "æ•°æ®ç±»å‹ï¼ˆtext/number/date/booleanï¼‰",
                    "required": false,
                    "example": "ç¤ºä¾‹æ•°æ®"
                }
                ]
            }
            ]
        },
        "additional_requirements": {
            "formatting": ["æ ¼å¼è¦æ±‚"],
            "validation_rules": ["æ•°æ®éªŒè¯è§„åˆ™"],
            "special_features": ["ç‰¹æ®ŠåŠŸèƒ½éœ€æ±‚"]
        }
        }

        **ç»“æ„ç¤ºä¾‹è¯´æ˜ï¼š**
        - å¦‚æœè¡¨å¤´æ˜¯æ ‡é¢˜æ€§è´¨ï¼ˆæœ‰å­è¡¨å¤´ï¼‰ï¼Œåªéœ€è¦ "name" å’Œ "children"
        - å¦‚æœè¡¨å¤´æ˜¯æ•°æ®å­—æ®µï¼ˆå¶å­èŠ‚ç‚¹ï¼‰ï¼Œéœ€è¦å®Œæ•´çš„å­—æ®µä¿¡æ¯
        - children æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥åŒ…å«æ›´å¤šçš„æ ‡é¢˜è¡¨å¤´æˆ–æ•°æ®å­—æ®µ
        - æ”¯æŒ2çº§ã€3çº§æˆ–æ›´å¤šçº§çš„åµŒå¥—ç»“æ„
        """
        print("æ­£åœ¨ç”Ÿæˆè¡¨æ ¼æ¨¡æ¿......")
        system_message = SystemMessage(content=system_prompt)
        filtered_messages = filter_out_system_messages(state["messages"])
        messages = [system_message] + filtered_messages
        response = self.llm.invoke(messages)

        try:
            # Clean the response content to handle markdown-wrapped JSON
            response_content = response.content.strip()
            
            # ç§»é™¤markdownè¾“å‡º
            if response_content.startswith('```json'):
                response_content = response_content[7:]  # Remove ```json
            if response_content.startswith('```'):
                response_content = response_content[3:]   # Remove ```
            if response_content.endswith('```'):
                response_content = response_content[:-3]  # Remove trailing ```
            
            response_content = response_content.strip()
            
            # Parse the JSON response
            structured_output = json.loads(response_content)
            
            # Extract components
            table_info = structured_output["table_info"]
            table_structure = structured_output["table_structure"]
            additional_requirements = structured_output["additional_requirements"]

            # åˆ›å»ºå®Œæ•´çš„æ•°æ®
            complete_data = {
                "session_id": state.get("session_id", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "conversation_messages": [
                    {
                        "type": msg.__class__.__name__,
                        "content": msg.content
                    } for msg in state["messages"] if hasattr(msg, 'content')
                ]
            }

            print("âœ… JSONè§£ææˆåŠŸï¼Œè¡¨æ ¼æ¨¡æ¿ç”Ÿæˆå®Œæˆ")

            # åˆ›å»ºæ–‡ä»¶å¤¹ç”¨äºå­˜å‚¨ç”Ÿæˆçš„è¡¨æ ¼æ¨¡æ¿
            output_dir = "table_template"
            os.makedirs(output_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶åç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_id = state.get("session_id", "default")
            filename = f"table_template_{session_id}_{timestamp}.json"

            # å°†è¡¨æ ¼æ¨¡æ¿å‚¨å­˜åˆ°è¿™ä¸ªJSONæ–‡ä»¶
            file_path = os.path.join(output_dir, filename)
            with open(file_path, 'w', encoding = 'utf-8') as f:
                json.dump(complete_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… è¡¨æ ¼æ¨¡æ¿å·²ä¿å­˜åˆ°: {filename}")
        
            # Return updated state
            return {
                **state,
                "table_info": table_info,
                "table_structure": table_structure,
                "additional_requirements": additional_requirements,
                "gather_complete": True
            }
        
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response.content}")
            print(f"æ¸…ç†åå“åº”: {response_content if 'response_content' in locals() else 'N/A'}")
            
            # Fallback: create a basic template from the conversation
            print("ğŸ”„ ä½¿ç”¨å¯¹è¯å†…å®¹ç”ŸæˆåŸºç¡€æ¨¡æ¿")
            fallback_template = {
                "table_info": {
                    "purpose": "æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆçš„è¡¨æ ¼",
                    "description": "åŸºäºç”¨æˆ·éœ€æ±‚çš„åŸºç¡€è¡¨æ ¼ç»“æ„",
                    "data_sources": ["ç”¨æˆ·å¯¹è¯"],
                    "target_users": ["ç”¨æˆ·"],
                    "frequency": "å¾…ç¡®å®š"
                },
                "table_structure": {
                    "has_multi_level": False,
                    "multi_level_headers": [
                        {
                            "name": "åŸºç¡€å­—æ®µ",
                            "description": "æ ¹æ®å¯¹è¯æ¨æ–­çš„å­—æ®µ",
                            "data_type": "text",
                            "required": True,
                            "example": "ç¤ºä¾‹"
                        }
                    ]
                },
                "additional_requirements": {
                    "formatting": ["æ ‡å‡†æ ¼å¼"],
                    "validation_rules": ["åŸºæœ¬éªŒè¯"],
                    "special_features": ["æ— ç‰¹æ®Šè¦æ±‚"]
                }
            }
            
            return {
                **state,
                "table_info": fallback_template["table_info"],
                "table_structure": fallback_template["table_structure"],
                "additional_requirements": fallback_template["additional_requirements"],
                "gather_complete": True
            }
        except KeyError as e:
            print(f"âŒ JSONç»“æ„é”™è¯¯: {e}")
            print(f"å¯ç”¨é”®: {list(structured_output.keys()) if 'structured_output' in locals() else 'N/A'}")
            
            return {
                **state,
                "gather_complete": False
            }

    def _create_initial_state(self, user_input: str, session_id: str = "default") -> FrontdeskState:
        """åˆ›å»ºLanggraphæœ€åˆçŠ¶æ€ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œè‡ªåŠ¨æ–‡ä»¶è·¯å¾„æ£€æµ‹"""
        
        # æ£€æµ‹å¹¶å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„
        detected_files = detect_and_process_file_paths(user_input)
        
        return {
            "messages": [HumanMessage(content=user_input)],
            "session_id": session_id,
            "table_structure": {},
            "table_info": {},
            "additional_requirements": {},
            "gather_complete": False,
            "has_template": False,
            "complete_confirm": False,
            "uploaded_files": detected_files,  # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ–‡ä»¶è·¯å¾„
            "previous_node": "check_template"  # åˆå§‹çŠ¶æ€ä¸‹ï¼Œå¦‚æœæœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œåº”è¯¥å›åˆ°check_template
        }
    
    def run_front_desk_agent(self, user_input: str, session_id = "1") -> None: # session_idé»˜è®¤ä¸º1
        """æ‰§è¡Œå‰å°æ™ºèƒ½ä½“"""
        initial_state = self._create_initial_state(user_input, session_id)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ¤– æ­£åœ¨å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
        print("=" * 50)

        current_input = initial_state
        
        while True:
            try:
                has_interrupt = False
                for chunk in self.graph.stream(current_input, config = config, stream_mode = "updates"):
                    for node_name, node_output in chunk.items():
                        print(f"\nğŸ“ Node: {node_name}")
                        print("-" * 30)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰interrupt
                        if '__interrupt__' in chunk:
                            has_interrupt = True
                            interrupt_value = chunk['__interrupt__'][0].value
                            print(f"\nğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                            user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                            
                            # è®¾ç½®ä¸‹ä¸€æ¬¡å¾ªç¯çš„è¾“å…¥
                            current_input = Command(resume=user_response)
                            break
                        
                        if isinstance(node_output, dict):
                            if "messages" in node_output and node_output["messages"]:
                                latest_message = node_output["messages"][-1]
                                if hasattr(latest_message, 'content') and not isinstance(latest_message, HumanMessage):
                                    print(f"ğŸ’¬ æ™ºèƒ½ä½“å›å¤: {latest_message.content}")
                            
                            for key, value in node_output.items():
                                if key != "messages" and value:
                                    print(f"ğŸ“Š {key}: {value}")
                        print("-" * 30)
                
                # å¦‚æœæ²¡æœ‰interruptï¼Œè¯´æ˜æµç¨‹å®Œæˆ
                if not has_interrupt:
                    break
                    
            except Exception as e:
                print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
                raise e
        
        print("\nâœ… è¡¨æ ¼æ¨¡æ¿ç”Ÿæˆå®Œæˆï¼")

    def _route_after_file_upload(self, state: FrontdeskState) -> str:
        """æ–‡ä»¶ä¸Šä¼ å·¥å…·å¤„ç†å®Œæˆåçš„è·¯ç”±å†³ç­– - è¿”å›åˆ°ä¹‹å‰çš„èŠ‚ç‚¹"""
        # è¿”å›åˆ°æ–‡ä»¶ä¸Šä¼ å‰çš„èŠ‚ç‚¹
        previous_node = state.get("previous_node", "check_template")
        
        print(f"ğŸ“ æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œè¿”å›åˆ°èŠ‚ç‚¹: {previous_node}")
        
        # æ ¹æ®ä¹‹å‰çš„èŠ‚ç‚¹è¿”å›ç›¸åº”çš„è·¯ç”±å€¼
        node_routing_map = {
            "check_template": "check_template",
            "collect_template_supplement": "collect_template_supplement", 
            "collect_input": "collect_input",
            "confirm_template": "confirm_template",
            "gather_requirements": "gather_requirements"
        }
        
        return node_routing_map.get(previous_node, "check_template")

if __name__ == "__main__":

    #åˆ›å»ºæ™ºèƒ½ä½“
    frontdeskagent = FrontDeskAgent()

    save_graph_visualization(frontdeskagent.graph)

    # user_input = input("ğŸ¤– ä½ å¥½æˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½å¡«è¡¨åŠ©æ‰‹ï¼Œè¯·å‘Šè¯‰æˆ‘ä½ æƒ³å¡«ä»€ä¹ˆè¡¨æ ¼: \n")
    # frontdeskagent.run_front_desk_agent(user_input)